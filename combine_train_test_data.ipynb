{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b45c4e6",
   "metadata": {},
   "source": [
    "# Combine Train and Test Data for Sinhala ASR\n",
    "\n",
    "This notebook combines train.csv and test.csv files, extracts file paths and sentences, and saves the combined dataset to a new CSV file for Sinhala Automatic Speech Recognition (ASR) research.\n",
    "\n",
    "## Overview\n",
    "- Load train and test CSV files\n",
    "- Extract file paths and sentences\n",
    "- Combine datasets with source identification\n",
    "- Save to new CSV file\n",
    "- Display dataset statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26709509",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import pandas for data manipulation and processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4dda1876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5634be1f",
   "metadata": {},
   "source": [
    "## 2. Read CSV Files\n",
    "\n",
    "Load train.csv and test.csv files using pandas, display their shapes and basic information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f56202da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train.csv...\n",
      "Train data shape: (132574, 6)\n",
      "Train data columns: ['Unnamed: 0', 'filename', 'x', 'sentence', 'full', 'file']\n",
      "First few rows of train data:\n",
      "Train data shape: (132574, 6)\n",
      "Train data columns: ['Unnamed: 0', 'filename', 'x', 'sentence', 'full', 'file']\n",
      "First few rows of train data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>filename</th>\n",
       "      <th>x</th>\n",
       "      <th>sentence</th>\n",
       "      <th>full</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69861</td>\n",
       "      <td>70d725d61b</td>\n",
       "      <td>6e92f</td>\n",
       "      <td>‡∑Å‡∑ä‚Äç‡∂ª‡∑è‡∑Ä‡∂ö ‡∂†‡∂ª‡∑í‡∂≠ ‡∂±‡∑í‡∂Ø‡∑É‡∑î‡∂±‡∑ä ‡∂ö‡∑ú‡∂ß ‡∂¥‡∑ê‡∑Ñ‡∑ê‡∂Ø‡∑í‡∂Ω‡∑í ‡∂ö‡∂ª‡∂±‡∑ä‡∂±.</td>\n",
       "      <td>70d725d61b6e92f</td>\n",
       "      <td>asr_sinhala/data/70/70d725d61b.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77001</td>\n",
       "      <td>7f2e316d14</td>\n",
       "      <td>3b15d</td>\n",
       "      <td>‡∑Ä‡∑ô‡∂±‡∑ä‡∂± ‡∂¥‡∑î‡∑Ö‡∑î‡∑Ä‡∂±‡∑í</td>\n",
       "      <td>7f2e316d143b15d</td>\n",
       "      <td>asr_sinhala/data/7f/7f2e316d14.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>108545</td>\n",
       "      <td>bc470065df</td>\n",
       "      <td>4b5d0</td>\n",
       "      <td>‡∂ë‡∂∫ ‡∂≠‡∂∏‡∂∫‡∑í ‡∂î‡∑Ä‡∑î‡∂±‡∑ä‡∂ß</td>\n",
       "      <td>bc470065df4b5d0</td>\n",
       "      <td>asr_sinhala/data/bc/bc470065df.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47802</td>\n",
       "      <td>4b48bbffc5</td>\n",
       "      <td>8e991</td>\n",
       "      <td>‡∑É‡∂¥‡∑ä‡∂≠ ‡∂Ü‡∂ª‡∑ä‡∂∫ ‡∂∞‡∂±‡∂∫‡∑ô‡∑Ñ‡∑í ‡∂ë‡∂ö‡∑ä ‡∂ö‡∑ú‡∂ß‡∑É‡∂ö‡∑í.</td>\n",
       "      <td>4b48bbffc58e991</td>\n",
       "      <td>asr_sinhala/data/4b/4b48bbffc5.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28441</td>\n",
       "      <td>2f2951d11c</td>\n",
       "      <td>936a6</td>\n",
       "      <td>‡∂∏‡∑î‡∂Ø‡∂Ω‡∑è‡∂Ω‡∑í ‡∂±‡∑í‡∑Ä‡∑É‡∑ö ‡∂Ø‡∑ú‡∂ª ‡∑Ä‡∑í‡∑Ä‡∑ò‡∂≠ ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏‡∑ô‡∂±‡∑ä ‡∂¥‡∑É‡∑î ‡∂ë‡∂∫ ‡∑Ä‡∑É‡∑è ‡∂±‡∑ú...</td>\n",
       "      <td>2f2951d11c936a6</td>\n",
       "      <td>asr_sinhala/data/2f/2f2951d11c.flac</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    filename      x  \\\n",
       "0       69861  70d725d61b  6e92f   \n",
       "1       77001  7f2e316d14  3b15d   \n",
       "2      108545  bc470065df  4b5d0   \n",
       "3       47802  4b48bbffc5  8e991   \n",
       "4       28441  2f2951d11c  936a6   \n",
       "\n",
       "                                            sentence             full  \\\n",
       "0           ‡∑Å‡∑ä‚Äç‡∂ª‡∑è‡∑Ä‡∂ö ‡∂†‡∂ª‡∑í‡∂≠ ‡∂±‡∑í‡∂Ø‡∑É‡∑î‡∂±‡∑ä ‡∂ö‡∑ú‡∂ß ‡∂¥‡∑ê‡∑Ñ‡∑ê‡∂Ø‡∑í‡∂Ω‡∑í ‡∂ö‡∂ª‡∂±‡∑ä‡∂±.  70d725d61b6e92f   \n",
       "1                                      ‡∑Ä‡∑ô‡∂±‡∑ä‡∂± ‡∂¥‡∑î‡∑Ö‡∑î‡∑Ä‡∂±‡∑í  7f2e316d143b15d   \n",
       "2                                     ‡∂ë‡∂∫ ‡∂≠‡∂∏‡∂∫‡∑í ‡∂î‡∑Ä‡∑î‡∂±‡∑ä‡∂ß  bc470065df4b5d0   \n",
       "3                       ‡∑É‡∂¥‡∑ä‡∂≠ ‡∂Ü‡∂ª‡∑ä‡∂∫ ‡∂∞‡∂±‡∂∫‡∑ô‡∑Ñ‡∑í ‡∂ë‡∂ö‡∑ä ‡∂ö‡∑ú‡∂ß‡∑É‡∂ö‡∑í.  4b48bbffc58e991   \n",
       "4  ‡∂∏‡∑î‡∂Ø‡∂Ω‡∑è‡∂Ω‡∑í ‡∂±‡∑í‡∑Ä‡∑É‡∑ö ‡∂Ø‡∑ú‡∂ª ‡∑Ä‡∑í‡∑Ä‡∑ò‡∂≠ ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏‡∑ô‡∂±‡∑ä ‡∂¥‡∑É‡∑î ‡∂ë‡∂∫ ‡∑Ä‡∑É‡∑è ‡∂±‡∑ú...  2f2951d11c936a6   \n",
       "\n",
       "                                  file  \n",
       "0  asr_sinhala/data/70/70d725d61b.flac  \n",
       "1  asr_sinhala/data/7f/7f2e316d14.flac  \n",
       "2  asr_sinhala/data/bc/bc470065df.flac  \n",
       "3  asr_sinhala/data/4b/4b48bbffc5.flac  \n",
       "4  asr_sinhala/data/2f/2f2951d11c.flac  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read train CSV file\n",
    "print(\"Reading train.csv...\")\n",
    "train_df = pd.read_csv('train.csv')\n",
    "print(f\"Train data shape: {train_df.shape}\")\n",
    "print(f\"Train data columns: {list(train_df.columns)}\")\n",
    "print(f\"First few rows of train data:\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac580b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading test.csv...\n",
      "Test data shape: (23396, 6)\n",
      "Test data columns: ['Unnamed: 0', 'filename', 'x', 'sentence', 'full', 'file']\n",
      "First few rows of test data:\n",
      "Test data shape: (23396, 6)\n",
      "Test data columns: ['Unnamed: 0', 'filename', 'x', 'sentence', 'full', 'file']\n",
      "First few rows of test data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>filename</th>\n",
       "      <th>x</th>\n",
       "      <th>sentence</th>\n",
       "      <th>full</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>147490</td>\n",
       "      <td>f44918e4dd</td>\n",
       "      <td>cb1fe</td>\n",
       "      <td>‡∂¥‡∑í‡∂ª‡∑í‡∂∏‡∑í‡∂±‡∑ä‡∂ß ‡∂Ø‡∑ì‡∂Ω ‡∂≠‡∑í‡∂∫‡∑ô‡∂±‡∑Ä‡∑è</td>\n",
       "      <td>f44918e4ddcb1fe</td>\n",
       "      <td>asr_sinhala/data/f4/f44918e4dd.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>149997</td>\n",
       "      <td>f7d50f4b2c</td>\n",
       "      <td>59778</td>\n",
       "      <td>‡∂ë‡∂∏‡∑ô‡∂±‡∑ä‡∂∏ ‡∂∏‡∑è‡∂ª‡∑ä‡∂ú‡∂∫ ‡∂â‡∂Ø‡∑í‡∂ö‡∂ª‡∂± ‡∑É‡∑ä‡∂Æ‡∑è‡∂±‡∂∫‡∂ß</td>\n",
       "      <td>f7d50f4b2c59778</td>\n",
       "      <td>asr_sinhala/data/f7/f7d50f4b2c.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12622</td>\n",
       "      <td>12d4081d0c</td>\n",
       "      <td>199c3</td>\n",
       "      <td>‡∂ë‡∂≠‡∂ö‡∑ú‡∂ß ‡∑É‡∑í‡∂ª‡∑í‡∂ú‡∑î‡∂≠‡∑ä‡∂≠ ‡∑Ñ‡∂∫‡∑í‡∂∫‡∑ô‡∂±‡∑ä ‡∂ö‡∑ë ‡∂ú‡∑Ñ‡∂Ω‡∑è</td>\n",
       "      <td>12d4081d0c199c3</td>\n",
       "      <td>asr_sinhala/data/12/12d4081d0c.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>124199</td>\n",
       "      <td>d2a53f5f0a</td>\n",
       "      <td>e1e01</td>\n",
       "      <td>‡∂ë‡∂≠‡∂± ‡∂ú‡∑ú‡∂©‡∂ú‡∑ê‡∑Ñ‡∑ô‡∂±‡∑ä‡∂±‡∑ö ‡∑É‡∑î‡∂±‡∑ä‡∂∂‡∑î‡∂±‡∑ä ‡∂ú‡∑ú‡∂©‡∂ö‡∑ä ‡∂¥‡∂∏‡∂´‡∂∫‡∑í.</td>\n",
       "      <td>d2a53f5f0ae1e01</td>\n",
       "      <td>asr_sinhala/data/d2/d2a53f5f0a.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>139037</td>\n",
       "      <td>e841b4dfe0</td>\n",
       "      <td>8f885</td>\n",
       "      <td>‡∂ë‡∂ö ‡∑Ä‡∑í‡∑Å‡∑ä‡∑Ä ‡∂∞‡∂ª‡∑ä‡∂∏‡∂∫</td>\n",
       "      <td>e841b4dfe08f885</td>\n",
       "      <td>asr_sinhala/data/e8/e841b4dfe0.flac</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    filename      x                               sentence  \\\n",
       "0      147490  f44918e4dd  cb1fe                  ‡∂¥‡∑í‡∂ª‡∑í‡∂∏‡∑í‡∂±‡∑ä‡∂ß ‡∂Ø‡∑ì‡∂Ω ‡∂≠‡∑í‡∂∫‡∑ô‡∂±‡∑Ä‡∑è   \n",
       "1      149997  f7d50f4b2c  59778           ‡∂ë‡∂∏‡∑ô‡∂±‡∑ä‡∂∏ ‡∂∏‡∑è‡∂ª‡∑ä‡∂ú‡∂∫ ‡∂â‡∂Ø‡∑í‡∂ö‡∂ª‡∂± ‡∑É‡∑ä‡∂Æ‡∑è‡∂±‡∂∫‡∂ß   \n",
       "2       12622  12d4081d0c  199c3        ‡∂ë‡∂≠‡∂ö‡∑ú‡∂ß ‡∑É‡∑í‡∂ª‡∑í‡∂ú‡∑î‡∂≠‡∑ä‡∂≠ ‡∑Ñ‡∂∫‡∑í‡∂∫‡∑ô‡∂±‡∑ä ‡∂ö‡∑ë ‡∂ú‡∑Ñ‡∂Ω‡∑è   \n",
       "3      124199  d2a53f5f0a  e1e01  ‡∂ë‡∂≠‡∂± ‡∂ú‡∑ú‡∂©‡∂ú‡∑ê‡∑Ñ‡∑ô‡∂±‡∑ä‡∂±‡∑ö ‡∑É‡∑î‡∂±‡∑ä‡∂∂‡∑î‡∂±‡∑ä ‡∂ú‡∑ú‡∂©‡∂ö‡∑ä ‡∂¥‡∂∏‡∂´‡∂∫‡∑í.   \n",
       "4      139037  e841b4dfe0  8f885                         ‡∂ë‡∂ö ‡∑Ä‡∑í‡∑Å‡∑ä‡∑Ä ‡∂∞‡∂ª‡∑ä‡∂∏‡∂∫   \n",
       "\n",
       "              full                                 file  \n",
       "0  f44918e4ddcb1fe  asr_sinhala/data/f4/f44918e4dd.flac  \n",
       "1  f7d50f4b2c59778  asr_sinhala/data/f7/f7d50f4b2c.flac  \n",
       "2  12d4081d0c199c3  asr_sinhala/data/12/12d4081d0c.flac  \n",
       "3  d2a53f5f0ae1e01  asr_sinhala/data/d2/d2a53f5f0a.flac  \n",
       "4  e841b4dfe08f885  asr_sinhala/data/e8/e841b4dfe0.flac  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read test CSV file\n",
    "print(\"Reading test.csv...\")\n",
    "test_df = pd.read_csv('test.csv')\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"Test data columns: {list(test_df.columns)}\")\n",
    "print(f\"First few rows of test data:\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f717d426",
   "metadata": {},
   "source": [
    "## 3. Extract Required Columns\n",
    "\n",
    "Extract 'file' and 'sentence' columns from both train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5aab5946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train extracted shape: (132574, 2)\n",
      "Test extracted shape: (23396, 2)\n",
      "\n",
      "Sample from train_extracted:\n",
      "                                  file  \\\n",
      "0  asr_sinhala/data/70/70d725d61b.flac   \n",
      "1  asr_sinhala/data/7f/7f2e316d14.flac   \n",
      "2  asr_sinhala/data/bc/bc470065df.flac   \n",
      "\n",
      "                                   sentence  \n",
      "0  ‡∑Å‡∑ä‚Äç‡∂ª‡∑è‡∑Ä‡∂ö ‡∂†‡∂ª‡∑í‡∂≠ ‡∂±‡∑í‡∂Ø‡∑É‡∑î‡∂±‡∑ä ‡∂ö‡∑ú‡∂ß ‡∂¥‡∑ê‡∑Ñ‡∑ê‡∂Ø‡∑í‡∂Ω‡∑í ‡∂ö‡∂ª‡∂±‡∑ä‡∂±.  \n",
      "1                             ‡∑Ä‡∑ô‡∂±‡∑ä‡∂± ‡∂¥‡∑î‡∑Ö‡∑î‡∑Ä‡∂±‡∑í  \n",
      "2                            ‡∂ë‡∂∫ ‡∂≠‡∂∏‡∂∫‡∑í ‡∂î‡∑Ä‡∑î‡∂±‡∑ä‡∂ß  \n",
      "\n",
      "Sample from test_extracted:\n",
      "                                  file                         sentence\n",
      "0  asr_sinhala/data/f4/f44918e4dd.flac            ‡∂¥‡∑í‡∂ª‡∑í‡∂∏‡∑í‡∂±‡∑ä‡∂ß ‡∂Ø‡∑ì‡∂Ω ‡∂≠‡∑í‡∂∫‡∑ô‡∂±‡∑Ä‡∑è\n",
      "1  asr_sinhala/data/f7/f7d50f4b2c.flac     ‡∂ë‡∂∏‡∑ô‡∂±‡∑ä‡∂∏ ‡∂∏‡∑è‡∂ª‡∑ä‡∂ú‡∂∫ ‡∂â‡∂Ø‡∑í‡∂ö‡∂ª‡∂± ‡∑É‡∑ä‡∂Æ‡∑è‡∂±‡∂∫‡∂ß\n",
      "2  asr_sinhala/data/12/12d4081d0c.flac  ‡∂ë‡∂≠‡∂ö‡∑ú‡∂ß ‡∑É‡∑í‡∂ª‡∑í‡∂ú‡∑î‡∂≠‡∑ä‡∂≠ ‡∑Ñ‡∂∫‡∑í‡∂∫‡∑ô‡∂±‡∑ä ‡∂ö‡∑ë ‡∂ú‡∑Ñ‡∂Ω‡∑è\n"
     ]
    }
   ],
   "source": [
    "# Extract file path and sentence columns from both datasets\n",
    "train_extracted = train_df[['file', 'sentence']].copy()\n",
    "test_extracted = test_df[['file', 'sentence']].copy()\n",
    "\n",
    "print(f\"Train extracted shape: {train_extracted.shape}\")\n",
    "print(f\"Test extracted shape: {test_extracted.shape}\")\n",
    "print(f\"\\nSample from train_extracted:\")\n",
    "print(train_extracted.head(3))\n",
    "print(f\"\\nSample from test_extracted:\")\n",
    "print(test_extracted.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c796be3c",
   "metadata": {},
   "source": [
    "## 4. Add Source Identification\n",
    "\n",
    "Add a 'source' column to identify whether each record came from train or test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "198529c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data with source column:\n",
      "                                  file  \\\n",
      "0  asr_sinhala/data/70/70d725d61b.flac   \n",
      "1  asr_sinhala/data/7f/7f2e316d14.flac   \n",
      "2  asr_sinhala/data/bc/bc470065df.flac   \n",
      "\n",
      "                                   sentence source  \n",
      "0  ‡∑Å‡∑ä‚Äç‡∂ª‡∑è‡∑Ä‡∂ö ‡∂†‡∂ª‡∑í‡∂≠ ‡∂±‡∑í‡∂Ø‡∑É‡∑î‡∂±‡∑ä ‡∂ö‡∑ú‡∂ß ‡∂¥‡∑ê‡∑Ñ‡∑ê‡∂Ø‡∑í‡∂Ω‡∑í ‡∂ö‡∂ª‡∂±‡∑ä‡∂±.  train  \n",
      "1                             ‡∑Ä‡∑ô‡∂±‡∑ä‡∂± ‡∂¥‡∑î‡∑Ö‡∑î‡∑Ä‡∂±‡∑í  train  \n",
      "2                            ‡∂ë‡∂∫ ‡∂≠‡∂∏‡∂∫‡∑í ‡∂î‡∑Ä‡∑î‡∂±‡∑ä‡∂ß  train  \n",
      "\n",
      "Test data with source column:\n",
      "                                  file                         sentence source\n",
      "0  asr_sinhala/data/f4/f44918e4dd.flac            ‡∂¥‡∑í‡∂ª‡∑í‡∂∏‡∑í‡∂±‡∑ä‡∂ß ‡∂Ø‡∑ì‡∂Ω ‡∂≠‡∑í‡∂∫‡∑ô‡∂±‡∑Ä‡∑è   test\n",
      "1  asr_sinhala/data/f7/f7d50f4b2c.flac     ‡∂ë‡∂∏‡∑ô‡∂±‡∑ä‡∂∏ ‡∂∏‡∑è‡∂ª‡∑ä‡∂ú‡∂∫ ‡∂â‡∂Ø‡∑í‡∂ö‡∂ª‡∂± ‡∑É‡∑ä‡∂Æ‡∑è‡∂±‡∂∫‡∂ß   test\n",
      "2  asr_sinhala/data/12/12d4081d0c.flac  ‡∂ë‡∂≠‡∂ö‡∑ú‡∂ß ‡∑É‡∑í‡∂ª‡∑í‡∂ú‡∑î‡∂≠‡∑ä‡∂≠ ‡∑Ñ‡∂∫‡∑í‡∂∫‡∑ô‡∂±‡∑ä ‡∂ö‡∑ë ‡∂ú‡∑Ñ‡∂Ω‡∑è   test\n",
      "\n",
      "Train records: 132574\n",
      "Test records: 23396\n"
     ]
    }
   ],
   "source": [
    "# Add a source column to identify which dataset each row came from\n",
    "train_extracted['source'] = 'train'\n",
    "test_extracted['source'] = 'test'\n",
    "\n",
    "print(\"Train data with source column:\")\n",
    "print(train_extracted.head(3))\n",
    "print(f\"\\nTest data with source column:\")\n",
    "print(test_extracted.head(3))\n",
    "\n",
    "print(f\"\\nTrain records: {len(train_extracted)}\")\n",
    "print(f\"Test records: {len(test_extracted)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66799b66",
   "metadata": {},
   "source": [
    "## 5. Combine Datasets\n",
    "\n",
    "Use pandas concat to merge the train and test datasets into a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "340e14ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data shape: (155970, 3)\n",
      "Total records: 155970\n",
      "Columns: ['file', 'sentence', 'source']\n",
      "\n",
      "Data distribution by source:\n",
      "source\n",
      "train    132574\n",
      "test      23396\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Combine both datasets\n",
    "combined_df = pd.concat([train_extracted, test_extracted], ignore_index=True)\n",
    "\n",
    "print(f\"Combined data shape: {combined_df.shape}\")\n",
    "print(f\"Total records: {len(combined_df)}\")\n",
    "print(f\"Columns: {list(combined_df.columns)}\")\n",
    "\n",
    "# Verify the combination\n",
    "print(f\"\\nData distribution by source:\")\n",
    "print(combined_df['source'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eea6510",
   "metadata": {},
   "source": [
    "## 6. Save Combined Data\n",
    "\n",
    "Export the combined dataset to a new CSV file named 'combined_file_path_sentence.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbfa2f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data saved to: combined_file_path_sentence.csv\n",
      "File size: 19.25 MB\n",
      "‚úÖ File saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save to new CSV file\n",
    "output_file = 'combined_file_path_sentence.csv'\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "print(f\"Combined data saved to: {output_file}\")\n",
    "\n",
    "# Verify the file was created and check its size\n",
    "import os\n",
    "if os.path.exists(output_file):\n",
    "    file_size = os.path.getsize(output_file) / (1024 * 1024)  # MB\n",
    "    print(f\"File size: {file_size:.2f} MB\")\n",
    "    print(\"‚úÖ File saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2296ae40",
   "metadata": {},
   "source": [
    "## 7. Display Data Overview\n",
    "\n",
    "Show the first few rows of the combined dataset and print basic information about record counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9548df20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of combined data:\n",
      "                                  file  \\\n",
      "0  asr_sinhala/data/70/70d725d61b.flac   \n",
      "1  asr_sinhala/data/7f/7f2e316d14.flac   \n",
      "2  asr_sinhala/data/bc/bc470065df.flac   \n",
      "3  asr_sinhala/data/4b/4b48bbffc5.flac   \n",
      "4  asr_sinhala/data/2f/2f2951d11c.flac   \n",
      "\n",
      "                                            sentence source  \n",
      "0           ‡∑Å‡∑ä‚Äç‡∂ª‡∑è‡∑Ä‡∂ö ‡∂†‡∂ª‡∑í‡∂≠ ‡∂±‡∑í‡∂Ø‡∑É‡∑î‡∂±‡∑ä ‡∂ö‡∑ú‡∂ß ‡∂¥‡∑ê‡∑Ñ‡∑ê‡∂Ø‡∑í‡∂Ω‡∑í ‡∂ö‡∂ª‡∂±‡∑ä‡∂±.  train  \n",
      "1                                      ‡∑Ä‡∑ô‡∂±‡∑ä‡∂± ‡∂¥‡∑î‡∑Ö‡∑î‡∑Ä‡∂±‡∑í  train  \n",
      "2                                     ‡∂ë‡∂∫ ‡∂≠‡∂∏‡∂∫‡∑í ‡∂î‡∑Ä‡∑î‡∂±‡∑ä‡∂ß  train  \n",
      "3                       ‡∑É‡∂¥‡∑ä‡∂≠ ‡∂Ü‡∂ª‡∑ä‡∂∫ ‡∂∞‡∂±‡∂∫‡∑ô‡∑Ñ‡∑í ‡∂ë‡∂ö‡∑ä ‡∂ö‡∑ú‡∂ß‡∑É‡∂ö‡∑í.  train  \n",
      "4  ‡∂∏‡∑î‡∂Ø‡∂Ω‡∑è‡∂Ω‡∑í ‡∂±‡∑í‡∑Ä‡∑É‡∑ö ‡∂Ø‡∑ú‡∂ª ‡∑Ä‡∑í‡∑Ä‡∑ò‡∂≠ ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏‡∑ô‡∂±‡∑ä ‡∂¥‡∑É‡∑î ‡∂ë‡∂∫ ‡∑Ä‡∑É‡∑è ‡∂±‡∑ú...  train  \n",
      "\n",
      "Last 5 rows of combined data:\n",
      "                                       file                    sentence source\n",
      "155965  asr_sinhala/data/7f/7f65556b1e.flac      ‡∂ë‡∂Ø‡∑è ‡∂Ø‡∑Ä‡∑É‡∑ô‡∂±‡∑ä ‡∑Ä‡∑ê‡∂©‡∑í ‡∂ö‡∑ú‡∂ß‡∑É‡∂ö‡∑ä   test\n",
      "155966  asr_sinhala/data/dc/dcd0c96642.flac             little boy bomb   test\n",
      "155967  asr_sinhala/data/e2/e21b29cf06.flac  ‡∂Ö‡∂±‡∑ô‡∂ö‡∑î‡∂≠‡∑ä ‡∑Ä‡∑ê‡∂©‡∑í‡∑Ñ‡∑í‡∂ß‡∑í‡∂∫‡∂±‡∑ä‡∂ß ‡∂¥‡∂∏‡∂´‡∂∫‡∑í   test\n",
      "155968  asr_sinhala/data/12/1217bd55fd.flac       ‡∂Ü‡∂ú‡∂∏‡∑í‡∂ö ‡∑É‡∂∑‡∑è‡∑Ä ‡∂ö‡∑ô‡∂ª‡∑ô‡∑Ñ‡∑í ‡∂á‡∂≠‡∑í   test\n",
      "155969  asr_sinhala/data/f4/f496206b58.flac      ‡∂≠‡∂∏ ‡∂¥‡∑î‡∂≠‡∂±‡∑î‡∑Ä‡∂±‡∑ä‡∂ß ‡∑Ä‡∂≥‡∑í‡∂± ‡∂Ω‡∂Ø‡∑ì.   test\n",
      "\n",
      "Dataset info:\n",
      "- Shape: (155970, 3)\n",
      "- Columns: ['file', 'sentence', 'source']\n",
      "- Memory usage: 40.28 MB\n"
     ]
    }
   ],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows of combined data:\")\n",
    "print(combined_df.head())\n",
    "\n",
    "print(f\"\\nLast 5 rows of combined data:\")\n",
    "print(combined_df.tail())\n",
    "\n",
    "print(f\"\\nDataset info:\")\n",
    "print(f\"- Shape: {combined_df.shape}\")\n",
    "print(f\"- Columns: {list(combined_df.columns)}\")\n",
    "print(f\"- Memory usage: {combined_df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368db1fa",
   "metadata": {},
   "source": [
    "## 8. Generate Dataset Statistics\n",
    "\n",
    "Calculate and display dataset summary statistics including total sentences, unique file paths, and average sentence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4be86e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä DATASET SUMMARY STATISTICS\n",
      "==================================================\n",
      "üìÅ Total sentences: 155,970\n",
      "üéØ Unique file paths: 155,970\n",
      "\n",
      "üìù SENTENCE LENGTH ANALYSIS:\n",
      "   ‚Ä¢ Average: 34.1 characters\n",
      "   ‚Ä¢ Median: 24.0 characters\n",
      "   ‚Ä¢ Min: 2 characters\n",
      "   ‚Ä¢ Max: 156305 characters\n",
      "   ‚Ä¢ Std Dev: 799.2 characters\n",
      "\n",
      "üìä DATA SOURCE DISTRIBUTION:\n",
      "   ‚Ä¢ Train: 132,574 records (85.0%)\n",
      "   ‚Ä¢ Test: 23,396 records (15.0%)\n",
      "\n",
      "üîç DATA QUALITY:\n",
      "   ‚Ä¢ Missing file paths: 0\n",
      "   ‚Ä¢ Missing sentences: 0\n",
      "\n",
      "üìÇ FILE PATH ANALYSIS:\n",
      "üéØ Unique file paths: 155,970\n",
      "\n",
      "üìù SENTENCE LENGTH ANALYSIS:\n",
      "   ‚Ä¢ Average: 34.1 characters\n",
      "   ‚Ä¢ Median: 24.0 characters\n",
      "   ‚Ä¢ Min: 2 characters\n",
      "   ‚Ä¢ Max: 156305 characters\n",
      "   ‚Ä¢ Std Dev: 799.2 characters\n",
      "\n",
      "üìä DATA SOURCE DISTRIBUTION:\n",
      "   ‚Ä¢ Train: 132,574 records (85.0%)\n",
      "   ‚Ä¢ Test: 23,396 records (15.0%)\n",
      "\n",
      "üîç DATA QUALITY:\n",
      "   ‚Ä¢ Missing file paths: 0\n",
      "   ‚Ä¢ Missing sentences: 0\n",
      "\n",
      "üìÇ FILE PATH ANALYSIS:\n",
      "   ‚Ä¢ Unique directories: 238\n",
      "\n",
      "‚úÖ Dataset processing completed successfully!\n",
      "   ‚Ä¢ Unique directories: 238\n",
      "\n",
      "‚úÖ Dataset processing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Calculate comprehensive dataset statistics\n",
    "print(\"üìä DATASET SUMMARY STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Basic counts\n",
    "print(f\"üìÅ Total sentences: {len(combined_df):,}\")\n",
    "print(f\"üéØ Unique file paths: {combined_df['file'].nunique():,}\")\n",
    "\n",
    "# Text analysis\n",
    "sentence_lengths = combined_df['sentence'].str.len()\n",
    "print(f\"\\nüìù SENTENCE LENGTH ANALYSIS:\")\n",
    "print(f\"   ‚Ä¢ Average: {sentence_lengths.mean():.1f} characters\")\n",
    "print(f\"   ‚Ä¢ Median: {sentence_lengths.median():.1f} characters\")\n",
    "print(f\"   ‚Ä¢ Min: {sentence_lengths.min()} characters\")\n",
    "print(f\"   ‚Ä¢ Max: {sentence_lengths.max()} characters\")\n",
    "print(f\"   ‚Ä¢ Std Dev: {sentence_lengths.std():.1f} characters\")\n",
    "\n",
    "# Source distribution\n",
    "print(f\"\\nüìä DATA SOURCE DISTRIBUTION:\")\n",
    "source_counts = combined_df['source'].value_counts()\n",
    "for source, count in source_counts.items():\n",
    "    percentage = (count / len(combined_df)) * 100\n",
    "    print(f\"   ‚Ä¢ {source.capitalize()}: {count:,} records ({percentage:.1f}%)\")\n",
    "\n",
    "# Missing values check\n",
    "print(f\"\\nüîç DATA QUALITY:\")\n",
    "missing_files = combined_df['file'].isnull().sum()\n",
    "missing_sentences = combined_df['sentence'].isnull().sum()\n",
    "print(f\"   ‚Ä¢ Missing file paths: {missing_files}\")\n",
    "print(f\"   ‚Ä¢ Missing sentences: {missing_sentences}\")\n",
    "\n",
    "# File path analysis\n",
    "print(f\"\\nüìÇ FILE PATH ANALYSIS:\")\n",
    "unique_directories = combined_df['file'].str.extract(r'data/([a-f0-9]{2})/')[0].nunique()\n",
    "print(f\"   ‚Ä¢ Unique directories: {unique_directories}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset processing completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b82cdc",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Sinhala ASR Training\n",
    "\n",
    "This section preprocesses the combined dataset to prepare it for Sinhala Automatic Speech Recognition (ASR) model training. The preprocessing includes text normalization, cleaning, tokenization, and audio file validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493869f5",
   "metadata": {},
   "source": [
    "## 9. Text Normalization and Cleaning\n",
    "\n",
    "Clean and normalize the Sinhala text data by removing unwanted characters, normalizing whitespace, and handling special cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5083a763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Applying minimal cleaning to preserve Sinhala text integrity...\n",
      "\n",
      "üìù Text Cleaning Results:\n",
      "Before and after cleaning examples:\n",
      "1. Text preserved: '‡∑Å‡∑ä‚Äç‡∂ª‡∑è‡∑Ä‡∂ö ‡∂†‡∂ª‡∑í‡∂≠ ‡∂±‡∑í‡∂Ø‡∑É‡∑î‡∂±‡∑ä ‡∂ö‡∑ú‡∂ß ‡∂¥‡∑ê‡∑Ñ‡∑ê‡∂Ø‡∑í‡∂Ω‡∑í ‡∂ö‡∂ª‡∂±‡∑ä‡∂±.'\n",
      "\n",
      "2. Text preserved: '‡∑Ä‡∑ô‡∂±‡∑ä‡∂± ‡∂¥‡∑î‡∑Ö‡∑î‡∑Ä‡∂±‡∑í'\n",
      "\n",
      "3. Text preserved: '‡∂ë‡∂∫ ‡∂≠‡∂∏‡∂∫‡∑í ‡∂î‡∑Ä‡∑î‡∂±‡∑ä‡∂ß'\n",
      "\n",
      "4. Text preserved: '‡∑É‡∂¥‡∑ä‡∂≠ ‡∂Ü‡∂ª‡∑ä‡∂∫ ‡∂∞‡∂±‡∂∫‡∑ô‡∑Ñ‡∑í ‡∂ë‡∂ö‡∑ä ‡∂ö‡∑ú‡∂ß‡∑É‡∂ö‡∑í.'\n",
      "\n",
      "5. Text preserved: '‡∂∏‡∑î‡∂Ø‡∂Ω‡∑è‡∂Ω‡∑í ‡∂±‡∑í‡∑Ä‡∑É‡∑ö ‡∂Ø‡∑ú‡∂ª ‡∑Ä‡∑í‡∑Ä‡∑ò‡∂≠ ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏‡∑ô‡∂±‡∑ä ‡∂¥‡∑É‡∑î ‡∂ë‡∂∫ ‡∑Ä‡∑É‡∑è ‡∂±‡∑ú‡∂Ø‡∂∏‡∑è ‡∂ú‡∑ô‡∂≠‡∑î‡∑Ö‡∂ß ‡∂ú‡∑ú‡∑É‡∑ä‚Äå ‡∂≠‡∑í‡∂∂‡∑ö.'\n",
      "\n",
      "‚ö†Ô∏è  Empty sentences after cleaning: 0\n",
      "\n",
      "üìä Cleaning Impact:\n",
      "   ‚Ä¢ Texts modified: 14 out of 155,970 (0.01%)\n",
      "   ‚Ä¢ Texts preserved: 155,956 (99.99%)\n",
      "\n",
      "üìä Length Comparison:\n",
      "   ‚Ä¢ Original avg length: 34.1 characters\n",
      "   ‚Ä¢ Cleaned avg length: 34.1 characters\n",
      "   ‚Ä¢ Length difference: -0.0 characters\n",
      "   ‚Ä¢ Min length: 2 characters\n",
      "   ‚Ä¢ Max length: 156305 characters\n",
      "\n",
      "üìù Text Cleaning Results:\n",
      "Before and after cleaning examples:\n",
      "1. Text preserved: '‡∑Å‡∑ä‚Äç‡∂ª‡∑è‡∑Ä‡∂ö ‡∂†‡∂ª‡∑í‡∂≠ ‡∂±‡∑í‡∂Ø‡∑É‡∑î‡∂±‡∑ä ‡∂ö‡∑ú‡∂ß ‡∂¥‡∑ê‡∑Ñ‡∑ê‡∂Ø‡∑í‡∂Ω‡∑í ‡∂ö‡∂ª‡∂±‡∑ä‡∂±.'\n",
      "\n",
      "2. Text preserved: '‡∑Ä‡∑ô‡∂±‡∑ä‡∂± ‡∂¥‡∑î‡∑Ö‡∑î‡∑Ä‡∂±‡∑í'\n",
      "\n",
      "3. Text preserved: '‡∂ë‡∂∫ ‡∂≠‡∂∏‡∂∫‡∑í ‡∂î‡∑Ä‡∑î‡∂±‡∑ä‡∂ß'\n",
      "\n",
      "4. Text preserved: '‡∑É‡∂¥‡∑ä‡∂≠ ‡∂Ü‡∂ª‡∑ä‡∂∫ ‡∂∞‡∂±‡∂∫‡∑ô‡∑Ñ‡∑í ‡∂ë‡∂ö‡∑ä ‡∂ö‡∑ú‡∂ß‡∑É‡∂ö‡∑í.'\n",
      "\n",
      "5. Text preserved: '‡∂∏‡∑î‡∂Ø‡∂Ω‡∑è‡∂Ω‡∑í ‡∂±‡∑í‡∑Ä‡∑É‡∑ö ‡∂Ø‡∑ú‡∂ª ‡∑Ä‡∑í‡∑Ä‡∑ò‡∂≠ ‡∂ö‡∑í‡∂ª‡∑ì‡∂∏‡∑ô‡∂±‡∑ä ‡∂¥‡∑É‡∑î ‡∂ë‡∂∫ ‡∑Ä‡∑É‡∑è ‡∂±‡∑ú‡∂Ø‡∂∏‡∑è ‡∂ú‡∑ô‡∂≠‡∑î‡∑Ö‡∂ß ‡∂ú‡∑ú‡∑É‡∑ä‚Äå ‡∂≠‡∑í‡∂∂‡∑ö.'\n",
      "\n",
      "‚ö†Ô∏è  Empty sentences after cleaning: 0\n",
      "\n",
      "üìä Cleaning Impact:\n",
      "   ‚Ä¢ Texts modified: 14 out of 155,970 (0.01%)\n",
      "   ‚Ä¢ Texts preserved: 155,956 (99.99%)\n",
      "\n",
      "üìä Length Comparison:\n",
      "   ‚Ä¢ Original avg length: 34.1 characters\n",
      "   ‚Ä¢ Cleaned avg length: 34.1 characters\n",
      "   ‚Ä¢ Length difference: -0.0 characters\n",
      "   ‚Ä¢ Min length: 2 characters\n",
      "   ‚Ä¢ Max length: 156305 characters\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def clean_sinhala_text(text):\n",
    "    \"\"\"\n",
    "    Minimal cleaning for Sinhala text to preserve original structure\n",
    "    Only removes truly problematic characters while preserving Sinhala integrity\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string if not already\n",
    "    text = str(text)\n",
    "    \n",
    "    # Only normalize Unicode to NFC (canonical composition) - preserve ZWJ and other important chars\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    \n",
    "    # Only normalize excessive whitespace (3+ spaces to single space)\n",
    "    text = re.sub(r' {3,}', ' ', text)\n",
    "    \n",
    "    # Remove leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Only remove obviously problematic characters (control chars except common ones)\n",
    "    # Preserve all Sinhala chars, ZWJ (\\u200D), ZWNJ (\\u200C), and normal punctuation\n",
    "    # Remove only control characters that are not needed for text rendering\n",
    "    text = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]', '', text)\n",
    "    \n",
    "    # Remove some rare Unicode categories that are clearly not text\n",
    "    # But preserve Sinhala, Latin, punctuation, symbols, joiners, etc.\n",
    "    cleaned_chars = []\n",
    "    for char in text:\n",
    "        category = unicodedata.category(char)\n",
    "        # Remove only format characters that are not joiners\n",
    "        if category == 'Cf' and char not in ['\\u200C', '\\u200D']:  # Preserve ZWNJ and ZWJ\n",
    "            continue\n",
    "        cleaned_chars.append(char)\n",
    "    \n",
    "    text = ''.join(cleaned_chars)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Apply minimal cleaning to the sentences\n",
    "print(\"üßπ Applying minimal cleaning to preserve Sinhala text integrity...\")\n",
    "combined_df['sentence_cleaned'] = combined_df['sentence'].apply(clean_sinhala_text)\n",
    "\n",
    "# Compare before and after cleaning\n",
    "print(\"\\nüìù Text Cleaning Results:\")\n",
    "print(\"Before and after cleaning examples:\")\n",
    "for i in range(5):\n",
    "    if i < len(combined_df):\n",
    "        original = combined_df.iloc[i]['sentence']\n",
    "        cleaned = combined_df.iloc[i]['sentence_cleaned']\n",
    "        if original != cleaned:\n",
    "            print(f\"{i+1}. Original: '{original}'\")\n",
    "            print(f\"   Cleaned:  '{cleaned}'\")\n",
    "            print(f\"   Changed:  {'Yes' if original != cleaned else 'No'}\")\n",
    "        else:\n",
    "            print(f\"{i+1}. Text preserved: '{cleaned}'\")\n",
    "        print()\n",
    "\n",
    "# Check for empty sentences after cleaning\n",
    "empty_after_cleaning = combined_df['sentence_cleaned'].str.len() == 0\n",
    "print(f\"‚ö†Ô∏è  Empty sentences after cleaning: {empty_after_cleaning.sum()}\")\n",
    "\n",
    "if empty_after_cleaning.sum() > 0:\n",
    "    print(\"Examples of sentences that became empty:\")\n",
    "    empty_examples = combined_df[empty_after_cleaning]['sentence'].head(5)\n",
    "    for idx, sentence in empty_examples.items():\n",
    "        print(f\"  - '{sentence}'\")\n",
    "\n",
    "# Check how many texts were actually changed\n",
    "texts_changed = (combined_df['sentence'] != combined_df['sentence_cleaned']).sum()\n",
    "print(f\"\\nüìä Cleaning Impact:\")\n",
    "print(f\"   ‚Ä¢ Texts modified: {texts_changed:,} out of {len(combined_df):,} ({texts_changed/len(combined_df)*100:.2f}%)\")\n",
    "print(f\"   ‚Ä¢ Texts preserved: {len(combined_df) - texts_changed:,} ({(len(combined_df) - texts_changed)/len(combined_df)*100:.2f}%)\")\n",
    "\n",
    "# Update sentence length statistics after cleaning\n",
    "cleaned_lengths = combined_df['sentence_cleaned'].str.len()\n",
    "original_lengths = combined_df['sentence'].str.len()\n",
    "print(f\"\\nüìä Length Comparison:\")\n",
    "print(f\"   ‚Ä¢ Original avg length: {original_lengths.mean():.1f} characters\")\n",
    "print(f\"   ‚Ä¢ Cleaned avg length: {cleaned_lengths.mean():.1f} characters\")\n",
    "print(f\"   ‚Ä¢ Length difference: {(cleaned_lengths.mean() - original_lengths.mean()):.1f} characters\")\n",
    "print(f\"   ‚Ä¢ Min length: {cleaned_lengths.min()} characters\")\n",
    "print(f\"   ‚Ä¢ Max length: {cleaned_lengths.max()} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd96dfbb",
   "metadata": {},
   "source": [
    "## 10. Audio File Validation\n",
    "\n",
    "Validate that audio files exist and can be accessed for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba7a7901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Validating audio files...\n",
      "üìÅ Audio File Validation Results (sample of 100):\n",
      "   ‚Ä¢ Files found: 100\n",
      "   ‚Ä¢ Files missing: 0\n",
      "üìÅ Audio File Validation Results (sample of 100):\n",
      "   ‚Ä¢ Files found: 100\n",
      "   ‚Ä¢ Files missing: 0\n",
      "\n",
      "üìä File Extensions Distribution:\n",
      "   ‚Ä¢ .flac: 155,970 files\n",
      "\n",
      "üìä File Extensions Distribution:\n",
      "   ‚Ä¢ .flac: 155,970 files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def validate_audio_files(df, sample_size=1000):\n",
    "    \"\"\"\n",
    "    Validate existence of audio files\n",
    "    \"\"\"\n",
    "    print(\"üîç Validating audio files...\")\n",
    "    \n",
    "    # Check if files exist (sample for performance)\n",
    "    sample_df = df.sample(n=min(sample_size, len(df)), random_state=42)\n",
    "    \n",
    "    existing_files = []\n",
    "    missing_files = []\n",
    "    \n",
    "    for idx, row in sample_df.iterrows():\n",
    "        file_path = row['file']\n",
    "        if os.path.exists(file_path):\n",
    "            existing_files.append(file_path)\n",
    "        else:\n",
    "            missing_files.append(file_path)\n",
    "    \n",
    "    return existing_files, missing_files\n",
    "\n",
    "# Validate a sample of audio files\n",
    "existing, missing = validate_audio_files(combined_df, sample_size=100)\n",
    "\n",
    "print(f\"üìÅ Audio File Validation Results (sample of 100):\")\n",
    "print(f\"   ‚Ä¢ Files found: {len(existing)}\")\n",
    "print(f\"   ‚Ä¢ Files missing: {len(missing)}\")\n",
    "\n",
    "if missing:\n",
    "    print(f\"\\n‚ö†Ô∏è  Missing files examples:\")\n",
    "    for i, file_path in enumerate(missing[:5]):\n",
    "        print(f\"   {i+1}. {file_path}\")\n",
    "\n",
    "# Check unique file extensions\n",
    "file_extensions = combined_df['file'].str.extract(r'\\.([a-zA-Z0-9]+)$')[0].value_counts()\n",
    "print(f\"\\nüìä File Extensions Distribution:\")\n",
    "for ext, count in file_extensions.items():\n",
    "    print(f\"   ‚Ä¢ .{ext}: {count:,} files\")\n",
    "\n",
    "# Create a column for absolute file paths (if needed)\n",
    "combined_df['file_absolute'] = combined_df['file'].apply(lambda x: os.path.abspath(x) if os.path.exists(x) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704862fa",
   "metadata": {},
   "source": [
    "## 11. Data Filtering and Quality Control\n",
    "\n",
    "Filter out problematic samples and ensure data quality for ASR training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f20009a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Applying quality filters...\n",
      "üìä Filtering Results:\n",
      "   ‚Ä¢ Initial records: 155,970\n",
      "   ‚Ä¢ Empty sentences removed: 0\n",
      "   ‚Ä¢ Length filtered: 33\n",
      "   ‚Ä¢ Non-Sinhala removed: 5,751\n",
      "   ‚Ä¢ Duplicates removed: 59,992\n",
      "   ‚Ä¢ Final records: 90,194\n",
      "   ‚Ä¢ Retention rate: 57.8%\n",
      "\n",
      "üìà Post-filtering Statistics:\n",
      "   ‚Ä¢ Average length: 26.7 characters\n",
      "   ‚Ä¢ Median length: 25.0 characters\n",
      "   ‚Ä¢ Length range: 2 - 132 characters\n",
      "\n",
      "üìä Source Distribution After Filtering:\n",
      "   ‚Ä¢ Train: 82,166 records (91.1%)\n",
      "   ‚Ä¢ Test: 8,028 records (8.9%)\n",
      "üìä Filtering Results:\n",
      "   ‚Ä¢ Initial records: 155,970\n",
      "   ‚Ä¢ Empty sentences removed: 0\n",
      "   ‚Ä¢ Length filtered: 33\n",
      "   ‚Ä¢ Non-Sinhala removed: 5,751\n",
      "   ‚Ä¢ Duplicates removed: 59,992\n",
      "   ‚Ä¢ Final records: 90,194\n",
      "   ‚Ä¢ Retention rate: 57.8%\n",
      "\n",
      "üìà Post-filtering Statistics:\n",
      "   ‚Ä¢ Average length: 26.7 characters\n",
      "   ‚Ä¢ Median length: 25.0 characters\n",
      "   ‚Ä¢ Length range: 2 - 132 characters\n",
      "\n",
      "üìä Source Distribution After Filtering:\n",
      "   ‚Ä¢ Train: 82,166 records (91.1%)\n",
      "   ‚Ä¢ Test: 8,028 records (8.9%)\n"
     ]
    }
   ],
   "source": [
    "def filter_data_for_asr(df, min_length=2, max_length=500):\n",
    "    \"\"\"\n",
    "    Filter data based on quality criteria for ASR training\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Applying quality filters...\")\n",
    "    \n",
    "    initial_count = len(df)\n",
    "    \n",
    "    # Filter 1: Remove empty sentences\n",
    "    df_filtered = df[df['sentence_cleaned'].str.len() > 0].copy()\n",
    "    empty_removed = initial_count - len(df_filtered)\n",
    "    \n",
    "    # Filter 2: Remove sentences that are too short or too long\n",
    "    df_filtered = df_filtered[\n",
    "        (df_filtered['sentence_cleaned'].str.len() >= min_length) & \n",
    "        (df_filtered['sentence_cleaned'].str.len() <= max_length)\n",
    "    ].copy()\n",
    "    length_filtered = len(df) - len(df_filtered) - empty_removed\n",
    "    \n",
    "    # Filter 3: Remove sentences with mostly non-Sinhala characters\n",
    "    def is_mostly_sinhala(text):\n",
    "        if not text:\n",
    "            return False\n",
    "        sinhala_chars = len(re.findall(r'[\\u0D80-\\u0DFF]', text))\n",
    "        total_chars = len(re.findall(r'[^\\s]', text))  # Non-whitespace chars\n",
    "        return total_chars > 0 and (sinhala_chars / total_chars) >= 0.5\n",
    "    \n",
    "    df_filtered = df_filtered[df_filtered['sentence_cleaned'].apply(is_mostly_sinhala)].copy()\n",
    "    non_sinhala_removed = len(df) - len(df_filtered) - empty_removed - length_filtered\n",
    "    \n",
    "    # Filter 4: Remove duplicates based on cleaned sentences\n",
    "    df_filtered = df_filtered.drop_duplicates(subset=['sentence_cleaned'], keep='first').copy()\n",
    "    duplicates_removed = len(df) - len(df_filtered) - empty_removed - length_filtered - non_sinhala_removed\n",
    "    \n",
    "    print(f\"üìä Filtering Results:\")\n",
    "    print(f\"   ‚Ä¢ Initial records: {initial_count:,}\")\n",
    "    print(f\"   ‚Ä¢ Empty sentences removed: {empty_removed:,}\")\n",
    "    print(f\"   ‚Ä¢ Length filtered: {length_filtered:,}\")\n",
    "    print(f\"   ‚Ä¢ Non-Sinhala removed: {non_sinhala_removed:,}\")\n",
    "    print(f\"   ‚Ä¢ Duplicates removed: {duplicates_removed:,}\")\n",
    "    print(f\"   ‚Ä¢ Final records: {len(df_filtered):,}\")\n",
    "    print(f\"   ‚Ä¢ Retention rate: {len(df_filtered)/initial_count*100:.1f}%\")\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "# Apply filtering\n",
    "filtered_df = filter_data_for_asr(combined_df)\n",
    "\n",
    "# Update statistics after filtering\n",
    "print(f\"\\nüìà Post-filtering Statistics:\")\n",
    "filtered_lengths = filtered_df['sentence_cleaned'].str.len()\n",
    "print(f\"   ‚Ä¢ Average length: {filtered_lengths.mean():.1f} characters\")\n",
    "print(f\"   ‚Ä¢ Median length: {filtered_lengths.median():.1f} characters\")\n",
    "print(f\"   ‚Ä¢ Length range: {filtered_lengths.min()} - {filtered_lengths.max()} characters\")\n",
    "\n",
    "# Show distribution by source after filtering\n",
    "print(f\"\\nüìä Source Distribution After Filtering:\")\n",
    "source_dist = filtered_df['source'].value_counts()\n",
    "for source, count in source_dist.items():\n",
    "    percentage = (count / len(filtered_df)) * 100\n",
    "    print(f\"   ‚Ä¢ {source.capitalize()}: {count:,} records ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a3d514",
   "metadata": {},
   "source": [
    "## 12. Character-Level Analysis\n",
    "\n",
    "Analyze the character distribution for ASR vocabulary preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "939c0094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî§ Analyzing character distribution...\n",
      "üìä Character Distribution Summary:\n",
      "   ‚Ä¢ Total unique characters: 107\n",
      "   ‚Ä¢ Sinhala characters: 76\n",
      "   ‚Ä¢ Latin characters: 0\n",
      "   ‚Ä¢ Punctuation: 8\n",
      "   ‚Ä¢ Other characters: 22\n",
      "\n",
      "üá±üá∞ Top 20 Sinhala Characters:\n",
      "    1. '‡∑ä' : 179,293\n",
      "    2. '‡∂±' : 164,429\n",
      "    3. '‡∑í' : 146,687\n",
      "    4. '‡∑Ä' : 116,263\n",
      "    5. '‡∂ö' : 107,634\n",
      "    6. '‡∂∫' : 103,131\n",
      "    7. '‡∂∏' : 98,461\n",
      "    8. '‡∂≠' : 94,662\n",
      "    9. '‡∑è' : 94,047\n",
      "   10. '‡∂ª' : 79,568\n",
      "   11. '‡∑î' : 75,894\n",
      "   12. '‡∑É' : 64,996\n",
      "   13. '‡∂Ø' : 61,699\n",
      "   14. '‡∑ô' : 55,223\n",
      "   15. '‡∂¥' : 51,924\n",
      "   16. '‡∂Ω' : 51,695\n",
      "   17. '‡∑ö' : 49,952\n",
      "   18. '‡∑Ñ' : 47,500\n",
      "   19. '‡∂ß' : 45,513\n",
      "   20. '‡∂ú' : 39,015\n",
      "\n",
      "üìù Punctuation Distribution:\n",
      "   ‚Ä¢ '.' : 23,771\n",
      "   ‚Ä¢ '?' : 1,882\n",
      "   ‚Ä¢ ',' : 1,323\n",
      "   ‚Ä¢ '!' : 263\n",
      "   ‚Ä¢ ''' : 142\n",
      "   ‚Ä¢ '\"' : 100\n",
      "   ‚Ä¢ ';' : 2\n",
      "   ‚Ä¢ ':' : 1\n",
      "\n",
      "üìö ASR Vocabulary:\n",
      "   ‚Ä¢ Vocabulary size: 101\n",
      "   ‚Ä¢ Characters included: ['<pad>', '<unk>', '<sos>', '<eos>', ' ', '‡∑ä', '‡∂±', '‡∑í', '‡∑Ä', '‡∂ö', '‡∂∫', '‡∂∏', '‡∂≠', '‡∑è', '‡∂ª', '‡∑î', '‡∑É', '‡∂Ø', '‡∑ô', '‡∂¥']...\n",
      "\n",
      "üíæ Vocabulary created with 101 characters\n",
      "üìä Character Distribution Summary:\n",
      "   ‚Ä¢ Total unique characters: 107\n",
      "   ‚Ä¢ Sinhala characters: 76\n",
      "   ‚Ä¢ Latin characters: 0\n",
      "   ‚Ä¢ Punctuation: 8\n",
      "   ‚Ä¢ Other characters: 22\n",
      "\n",
      "üá±üá∞ Top 20 Sinhala Characters:\n",
      "    1. '‡∑ä' : 179,293\n",
      "    2. '‡∂±' : 164,429\n",
      "    3. '‡∑í' : 146,687\n",
      "    4. '‡∑Ä' : 116,263\n",
      "    5. '‡∂ö' : 107,634\n",
      "    6. '‡∂∫' : 103,131\n",
      "    7. '‡∂∏' : 98,461\n",
      "    8. '‡∂≠' : 94,662\n",
      "    9. '‡∑è' : 94,047\n",
      "   10. '‡∂ª' : 79,568\n",
      "   11. '‡∑î' : 75,894\n",
      "   12. '‡∑É' : 64,996\n",
      "   13. '‡∂Ø' : 61,699\n",
      "   14. '‡∑ô' : 55,223\n",
      "   15. '‡∂¥' : 51,924\n",
      "   16. '‡∂Ω' : 51,695\n",
      "   17. '‡∑ö' : 49,952\n",
      "   18. '‡∑Ñ' : 47,500\n",
      "   19. '‡∂ß' : 45,513\n",
      "   20. '‡∂ú' : 39,015\n",
      "\n",
      "üìù Punctuation Distribution:\n",
      "   ‚Ä¢ '.' : 23,771\n",
      "   ‚Ä¢ '?' : 1,882\n",
      "   ‚Ä¢ ',' : 1,323\n",
      "   ‚Ä¢ '!' : 263\n",
      "   ‚Ä¢ ''' : 142\n",
      "   ‚Ä¢ '\"' : 100\n",
      "   ‚Ä¢ ';' : 2\n",
      "   ‚Ä¢ ':' : 1\n",
      "\n",
      "üìö ASR Vocabulary:\n",
      "   ‚Ä¢ Vocabulary size: 101\n",
      "   ‚Ä¢ Characters included: ['<pad>', '<unk>', '<sos>', '<eos>', ' ', '‡∑ä', '‡∂±', '‡∑í', '‡∑Ä', '‡∂ö', '‡∂∫', '‡∂∏', '‡∂≠', '‡∑è', '‡∂ª', '‡∑î', '‡∑É', '‡∂Ø', '‡∑ô', '‡∂¥']...\n",
      "\n",
      "üíæ Vocabulary created with 101 characters\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def analyze_characters(df):\n",
    "    \"\"\"\n",
    "    Analyze character distribution in the dataset\n",
    "    \"\"\"\n",
    "    print(\"üî§ Analyzing character distribution...\")\n",
    "    \n",
    "    # Combine all text\n",
    "    all_text = ' '.join(df['sentence_cleaned'].tolist())\n",
    "    \n",
    "    # Count characters\n",
    "    char_counts = Counter(all_text)\n",
    "    \n",
    "    # Separate different character types\n",
    "    sinhala_chars = {}\n",
    "    punctuation_chars = {}\n",
    "    latin_chars = {}\n",
    "    other_chars = {}\n",
    "    \n",
    "    for char, count in char_counts.items():\n",
    "        if '\\u0D80' <= char <= '\\u0DFF':  # Sinhala Unicode range\n",
    "            sinhala_chars[char] = count\n",
    "        elif char.isalpha() and ord(char) < 128:  # Basic Latin\n",
    "            latin_chars[char] = count\n",
    "        elif char in '.,!?;:\"\\'-()[]{}':  # Common punctuation\n",
    "            punctuation_chars[char] = count\n",
    "        elif char != ' ':  # Skip spaces\n",
    "            other_chars[char] = count\n",
    "    \n",
    "    return sinhala_chars, latin_chars, punctuation_chars, other_chars, char_counts\n",
    "\n",
    "# Analyze character distribution\n",
    "sinhala_chars, latin_chars, punct_chars, other_chars, all_chars = analyze_characters(filtered_df)\n",
    "\n",
    "print(f\"üìä Character Distribution Summary:\")\n",
    "print(f\"   ‚Ä¢ Total unique characters: {len(all_chars):,}\")\n",
    "print(f\"   ‚Ä¢ Sinhala characters: {len(sinhala_chars):,}\")\n",
    "print(f\"   ‚Ä¢ Latin characters: {len(latin_chars):,}\")\n",
    "print(f\"   ‚Ä¢ Punctuation: {len(punct_chars):,}\")\n",
    "print(f\"   ‚Ä¢ Other characters: {len(other_chars):,}\")\n",
    "\n",
    "# Show top Sinhala characters\n",
    "print(f\"\\nüá±üá∞ Top 20 Sinhala Characters:\")\n",
    "top_sinhala = sorted(sinhala_chars.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "for i, (char, count) in enumerate(top_sinhala, 1):\n",
    "    print(f\"   {i:2d}. '{char}' : {count:,}\")\n",
    "\n",
    "# Show punctuation distribution\n",
    "if punct_chars:\n",
    "    print(f\"\\nüìù Punctuation Distribution:\")\n",
    "    for char, count in sorted(punct_chars.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"   ‚Ä¢ '{char}' : {count:,}\")\n",
    "\n",
    "# Create vocabulary for ASR\n",
    "def create_vocab(char_counts, min_frequency=10):\n",
    "    \"\"\"Create vocabulary list for ASR training\"\"\"\n",
    "    vocab = []\n",
    "    \n",
    "    # Add special tokens\n",
    "    special_tokens = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "    vocab.extend(special_tokens)\n",
    "    \n",
    "    # Add space\n",
    "    vocab.append(' ')\n",
    "    \n",
    "    # Add characters with frequency >= min_frequency\n",
    "    for char, count in sorted(char_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        if char != ' ' and count >= min_frequency:\n",
    "            vocab.append(char)\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "# Create vocabulary\n",
    "vocab = create_vocab(all_chars, min_frequency=5)\n",
    "print(f\"\\nüìö ASR Vocabulary:\")\n",
    "print(f\"   ‚Ä¢ Vocabulary size: {len(vocab)}\")\n",
    "print(f\"   ‚Ä¢ Characters included: {vocab[:20]}...\")  # Show first 20\n",
    "\n",
    "# Save vocabulary\n",
    "vocab_df = pd.DataFrame({'character': vocab, 'index': range(len(vocab))})\n",
    "print(f\"\\nüíæ Vocabulary created with {len(vocab)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2f41b4",
   "metadata": {},
   "source": [
    "## 13. Create Training and Test Splits (80/20)\n",
    "\n",
    "Split the preprocessed data into 80% training and 20% testing sets for ASR model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "66ba62d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Creating train/test splits (80/20)...\n",
      "üìä Data Split Summary:\n",
      "   ‚Ä¢ Training set: 72,155 samples (80.0%)\n",
      "   ‚Ä¢ Test set: 18,039 samples (20.0%)\n",
      "\n",
      "üìà Source Distribution by Split:\n",
      "\n",
      "Training:\n",
      "   ‚Ä¢ Train: 65,733 (91.1%)\n",
      "   ‚Ä¢ Test: 6,422 (8.9%)\n",
      "\n",
      "Test:\n",
      "   ‚Ä¢ Train: 16,433 (91.1%)\n",
      "   ‚Ä¢ Test: 1,606 (8.9%)\n",
      "\n",
      "üìè Average Sentence Length by Split:\n",
      "   ‚Ä¢ Training: 26.7 characters\n",
      "   ‚Ä¢ Test: 26.7 characters\n",
      "\n",
      "üí° Note: You can create a validation subset from training data during model training if needed.\n",
      "   Recommended: Use 10-15% of training data for validation during training.\n",
      "üìä Data Split Summary:\n",
      "   ‚Ä¢ Training set: 72,155 samples (80.0%)\n",
      "   ‚Ä¢ Test set: 18,039 samples (20.0%)\n",
      "\n",
      "üìà Source Distribution by Split:\n",
      "\n",
      "Training:\n",
      "   ‚Ä¢ Train: 65,733 (91.1%)\n",
      "   ‚Ä¢ Test: 6,422 (8.9%)\n",
      "\n",
      "Test:\n",
      "   ‚Ä¢ Train: 16,433 (91.1%)\n",
      "   ‚Ä¢ Test: 1,606 (8.9%)\n",
      "\n",
      "üìè Average Sentence Length by Split:\n",
      "   ‚Ä¢ Training: 26.7 characters\n",
      "   ‚Ä¢ Test: 26.7 characters\n",
      "\n",
      "üí° Note: You can create a validation subset from training data during model training if needed.\n",
      "   Recommended: Use 10-15% of training data for validation during training.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_asr_splits(df, test_size=0.20, random_state=42):\n",
    "    \"\"\"\n",
    "    Create train/test splits for ASR training (80/20)\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Creating train/test splits (80/20)...\")\n",
    "    \n",
    "    # Split into train (80%) and test (20%)\n",
    "    train_df, test_df = train_test_split(\n",
    "        df, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state,\n",
    "        stratify=df['source']  # Maintain source distribution\n",
    "    )\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "# Create splits\n",
    "train_data, test_data = create_asr_splits(filtered_df)\n",
    "\n",
    "print(f\"üìä Data Split Summary:\")\n",
    "print(f\"   ‚Ä¢ Training set: {len(train_data):,} samples ({len(train_data)/len(filtered_df)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Test set: {len(test_data):,} samples ({len(test_data)/len(filtered_df)*100:.1f}%)\")\n",
    "\n",
    "# Check source distribution in each split\n",
    "print(f\"\\nüìà Source Distribution by Split:\")\n",
    "for split_name, split_data in [(\"Training\", train_data), (\"Test\", test_data)]:\n",
    "    source_counts = split_data['source'].value_counts()\n",
    "    print(f\"\\n{split_name}:\")\n",
    "    for source, count in source_counts.items():\n",
    "        percentage = (count / len(split_data)) * 100\n",
    "        print(f\"   ‚Ä¢ {source.capitalize()}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "# Check sentence length distribution across splits\n",
    "print(f\"\\nüìè Average Sentence Length by Split:\")\n",
    "for split_name, split_data in [(\"Training\", train_data), (\"Test\", test_data)]:\n",
    "    avg_length = split_data['sentence_cleaned'].str.len().mean()\n",
    "    print(f\"   ‚Ä¢ {split_name}: {avg_length:.1f} characters\")\n",
    "\n",
    "# Create a validation subset from training data if needed for model development\n",
    "print(f\"\\nüí° Note: You can create a validation subset from training data during model training if needed.\")\n",
    "print(f\"   Recommended: Use 10-15% of training data for validation during training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7b95d6",
   "metadata": {},
   "source": [
    "## 14. Export Preprocessed Data\n",
    "\n",
    "Save the preprocessed and split data in formats suitable for ASR training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9202a581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Exporting preprocessed data to 'processed_asr_data' directory...\n",
      "‚úÖ Export completed! Files created:\n",
      "   üìÑ CSV Files:\n",
      "      ‚Ä¢ train_data.csv (72,155 samples - 80%)\n",
      "      ‚Ä¢ test_data.csv (18,039 samples - 20%)\n",
      "      ‚Ä¢ vocabulary.csv (101 characters)\n",
      "   üìÑ Manifest Files (JSONL format):\n",
      "      ‚Ä¢ train_manifest.jsonl\n",
      "      ‚Ä¢ test_manifest.jsonl\n",
      "   üìÑ Metadata:\n",
      "      ‚Ä¢ metadata.json\n",
      "      üìÅ metadata.json: 0.00 MB\n",
      "      üìÅ test_data.csv: 4.92 MB\n",
      "      üìÅ test_manifest.jsonl: 3.07 MB\n",
      "      üìÅ train_data.csv: 19.67 MB\n",
      "      üìÅ train_manifest.jsonl: 12.27 MB\n",
      "      üìÅ vocabulary.csv: 0.00 MB\n",
      "\n",
      "üìä Total exported data size: 39.94 MB\n",
      "\n",
      "üéØ Data is ready for Sinhala ASR training with 80/20 split!\n",
      "\n",
      "üí° Training Tips:\n",
      "   ‚Ä¢ Use the full training set (80%) for model training\n",
      "   ‚Ä¢ Reserve test set (20%) for final evaluation only\n",
      "   ‚Ä¢ Consider using cross-validation or a subset of training data for validation during development\n",
      "   ‚Ä¢ Many ASR frameworks can automatically split training data into train/val during training\n",
      "‚úÖ Export completed! Files created:\n",
      "   üìÑ CSV Files:\n",
      "      ‚Ä¢ train_data.csv (72,155 samples - 80%)\n",
      "      ‚Ä¢ test_data.csv (18,039 samples - 20%)\n",
      "      ‚Ä¢ vocabulary.csv (101 characters)\n",
      "   üìÑ Manifest Files (JSONL format):\n",
      "      ‚Ä¢ train_manifest.jsonl\n",
      "      ‚Ä¢ test_manifest.jsonl\n",
      "   üìÑ Metadata:\n",
      "      ‚Ä¢ metadata.json\n",
      "      üìÅ metadata.json: 0.00 MB\n",
      "      üìÅ test_data.csv: 4.92 MB\n",
      "      üìÅ test_manifest.jsonl: 3.07 MB\n",
      "      üìÅ train_data.csv: 19.67 MB\n",
      "      üìÅ train_manifest.jsonl: 12.27 MB\n",
      "      üìÅ vocabulary.csv: 0.00 MB\n",
      "\n",
      "üìä Total exported data size: 39.94 MB\n",
      "\n",
      "üéØ Data is ready for Sinhala ASR training with 80/20 split!\n",
      "\n",
      "üí° Training Tips:\n",
      "   ‚Ä¢ Use the full training set (80%) for model training\n",
      "   ‚Ä¢ Reserve test set (20%) for final evaluation only\n",
      "   ‚Ä¢ Consider using cross-validation or a subset of training data for validation during development\n",
      "   ‚Ä¢ Many ASR frameworks can automatically split training data into train/val during training\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Create output directory for processed data\n",
    "output_dir = \"processed_asr_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"üíæ Exporting preprocessed data to '{output_dir}' directory...\")\n",
    "\n",
    "# Export splits as CSV files\n",
    "train_data.to_csv(f\"{output_dir}/train_data.csv\", index=False)\n",
    "test_data.to_csv(f\"{output_dir}/test_data.csv\", index=False)\n",
    "\n",
    "# Export vocabulary\n",
    "vocab_df.to_csv(f\"{output_dir}/vocabulary.csv\", index=False)\n",
    "\n",
    "# Create metadata file\n",
    "metadata = {\n",
    "    \"dataset_info\": {\n",
    "        \"total_samples\": len(filtered_df),\n",
    "        \"train_samples\": len(train_data),\n",
    "        \"test_samples\": len(test_data),\n",
    "        \"train_percentage\": round(len(train_data)/len(filtered_df)*100, 1),\n",
    "        \"test_percentage\": round(len(test_data)/len(filtered_df)*100, 1),\n",
    "        \"vocabulary_size\": len(vocab),\n",
    "        \"avg_sentence_length\": float(filtered_df['sentence_cleaned'].str.len().mean()),\n",
    "        \"max_sentence_length\": int(filtered_df['sentence_cleaned'].str.len().max()),\n",
    "        \"min_sentence_length\": int(filtered_df['sentence_cleaned'].str.len().min())\n",
    "    },\n",
    "    \"split_strategy\": \"80% training, 20% testing\",\n",
    "    \"preprocessing_steps\": [\n",
    "        \"Text normalization and cleaning\",\n",
    "        \"Unicode normalization\",\n",
    "        \"Whitespace normalization\",\n",
    "        \"Punctuation normalization\",\n",
    "        \"Empty sentence removal\",\n",
    "        \"Length filtering (2-500 characters)\",\n",
    "        \"Sinhala character ratio filtering (>=50%)\",\n",
    "        \"Duplicate removal\"\n",
    "    ],\n",
    "    \"file_info\": {\n",
    "        \"audio_format\": \"flac\",\n",
    "        \"text_encoding\": \"utf-8\",\n",
    "        \"language\": \"Sinhala (si)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "with open(f\"{output_dir}/metadata.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Create manifest files for ASR training (common format)\n",
    "def create_manifest(data, filename):\n",
    "    \"\"\"Create manifest file in JSON Lines format for ASR training\"\"\"\n",
    "    with open(f\"{output_dir}/{filename}\", 'w', encoding='utf-8') as f:\n",
    "        for _, row in data.iterrows():\n",
    "            manifest_entry = {\n",
    "                \"audio_filepath\": row['file'],\n",
    "                \"text\": row['sentence_cleaned'],\n",
    "                \"duration\": -1,  # To be filled by audio processing\n",
    "                \"source\": row['source']\n",
    "            }\n",
    "            f.write(json.dumps(manifest_entry, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# Create manifest files\n",
    "create_manifest(train_data, \"train_manifest.jsonl\")\n",
    "create_manifest(test_data, \"test_manifest.jsonl\")\n",
    "\n",
    "print(f\"‚úÖ Export completed! Files created:\")\n",
    "print(f\"   üìÑ CSV Files:\")\n",
    "print(f\"      ‚Ä¢ train_data.csv ({len(train_data):,} samples - 80%)\")\n",
    "print(f\"      ‚Ä¢ test_data.csv ({len(test_data):,} samples - 20%)\")\n",
    "print(f\"      ‚Ä¢ vocabulary.csv ({len(vocab)} characters)\")\n",
    "print(f\"   üìÑ Manifest Files (JSONL format):\")\n",
    "print(f\"      ‚Ä¢ train_manifest.jsonl\")\n",
    "print(f\"      ‚Ä¢ test_manifest.jsonl\")\n",
    "print(f\"   üìÑ Metadata:\")\n",
    "print(f\"      ‚Ä¢ metadata.json\")\n",
    "\n",
    "# Show file sizes\n",
    "total_size = 0\n",
    "for filename in os.listdir(output_dir):\n",
    "    file_path = os.path.join(output_dir, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "        total_size += size_mb\n",
    "        print(f\"      üìÅ {filename}: {size_mb:.2f} MB\")\n",
    "\n",
    "print(f\"\\nüìä Total exported data size: {total_size:.2f} MB\")\n",
    "print(f\"\\nüéØ Data is ready for Sinhala ASR training with 80/20 split!\")\n",
    "\n",
    "# Provide guidance on validation\n",
    "print(f\"\\nüí° Training Tips:\")\n",
    "print(f\"   ‚Ä¢ Use the full training set (80%) for model training\")\n",
    "print(f\"   ‚Ä¢ Reserve test set (20%) for final evaluation only\")\n",
    "print(f\"   ‚Ä¢ Consider using cross-validation or a subset of training data for validation during development\")\n",
    "print(f\"   ‚Ä¢ Many ASR frameworks can automatically split training data into train/val during training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ddbb62",
   "metadata": {},
   "source": [
    "## 15. Create Clean Files with Essential Columns Only\n",
    "\n",
    "Extract only the essential columns (`file` and `sentence_cleaned`) from train and test files for ASR training efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "17144173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Creating clean versions of train and test files...\n",
      "üìä Clean Data Summary:\n",
      "   ‚Ä¢ Training data: 72,155 samples\n",
      "   ‚Ä¢ Test data: 18,039 samples\n",
      "üìä Clean Data Summary:\n",
      "   ‚Ä¢ Training data: 72,155 samples\n",
      "   ‚Ä¢ Test data: 18,039 samples\n",
      "\n",
      "üíæ Clean files saved:\n",
      "   ‚Ä¢ train_data_clean.csv\n",
      "   ‚Ä¢ test_data_clean.csv\n",
      "\n",
      "üìù Sample from clean training data:\n",
      "                                       file  \\\n",
      "9583    asr_sinhala/data/98/983e3c6613.flac   \n",
      "83995   asr_sinhala/data/29/29ab15c6d4.flac   \n",
      "110010  asr_sinhala/data/b0/b0072f9ac0.flac   \n",
      "\n",
      "                                     sentence_cleaned  \n",
      "9583     ‡∂ë‡∂∫ ‡∂∏‡∑í‡∂Ω‡∑í‡∂ß‡∂ª‡∑í‡∂∫ ‡∂≠‡∑î‡∑Ö ‡∂≠‡∑í‚Äç‡∂∫‡∂± ‡∂¥‡∑ä‚Äç‡∂ª‡∂∞‡∑è‡∂±‡∂∏ ‡∂¥‡∑ä‚Äç‡∂ª‡∂≠‡∑í‡∂∏‡∑è‡∂±‡∂∫‡∂ö‡∑ä.  \n",
      "83995   ‡∑É‡∑è‡∑Ñ‡∑í‡∂≠‡∑ä‚Äç‡∂∫‡∂ö‡∂ª‡∑î‡∑Ä‡∑è‡∂ß ‡∂ä‡∂ß ‡∑Ä‡∑ê‡∂©‡∑í‡∂∫ ‡∂Ω‡∑ú‡∂ö‡∑î ‡∑Ä‡∂ú‡∂ö‡∑ì‡∂∏‡∂ö‡∑ä ‡∂≠‡∑í‡∂∫‡∑ô‡∂±‡∑Ä‡∑è.  \n",
      "110010                        ‡∂ï‡∂ú‡∑ú‡∂Ω‡∑ä‡∂Ω‡∂±‡∑ä‡∂ß ‡∂Ø‡∂ö‡∑í‡∂±‡∑ä‡∂± ‡∂Ω‡∑ê‡∂∂‡∑ô‡∂∫‡∑í  \n",
      "\n",
      "üìù Sample from clean test data:\n",
      "                                       file  \\\n",
      "116168  asr_sinhala/data/b6/b63bfb7a34.flac   \n",
      "48318   asr_sinhala/data/f1/f134378295.flac   \n",
      "71784   asr_sinhala/data/d3/d3bf1e10b0.flac   \n",
      "\n",
      "                                       sentence_cleaned  \n",
      "116168                                ‡∂∏‡∑ô‡∂ö‡∑ì ‡∑É‡∑í‡∑Ä‡∑ä ‡∂∏‡∑Ñ ‡∂∞‡∑è‡∂≠‡∑î  \n",
      "48318                            ‡∂ë‡∂Ø‡∑í‡∂±‡∑ô‡∂Ø‡∑è ‡∂¢‡∑ì‡∑Ä‡∑í‡∂≠‡∂∫‡∑ö ‡∑É‡∑í‡∂Ø‡∑î‡∑Ä‡∂±  \n",
      "71784   ‡∂Ö‡∂∞‡∑í‡∑Ä‡∑ö‡∂ú‡∑ì ‡∂∏‡∑è‡∂ª‡∑ä‡∂ú‡∂∫‡∑ö ‡∂∏‡∑ê‡∂Ø ‡∑É‡∑ö‡∑Ä‡∑è ‡∑É‡∑ä‡∂Æ‡∑è‡∂±‡∂∫‡∂ö‡∑ä ‡∂â‡∂Ø‡∑í ‡∂ö‡∑Ö ‡∂¥‡∂∏‡∂´‡∑í‡∂±‡∑ä  \n",
      "\n",
      "üìä Clean File Sizes:\n",
      "   ‚Ä¢ train_data_clean.csv: 7.60 MB\n",
      "   ‚Ä¢ test_data_clean.csv: 1.90 MB\n",
      "   ‚Ä¢ Total clean size: 9.50 MB\n",
      "\n",
      "üíæ Space Optimization:\n",
      "   ‚Ä¢ Original total size: 24.59 MB\n",
      "   ‚Ä¢ Clean total size: 9.50 MB\n",
      "   ‚Ä¢ Space saved: 15.09 MB (61.4%)\n",
      "\n",
      "‚úÖ Clean files ready for ASR training!\n",
      "\n",
      "üíæ Clean files saved:\n",
      "   ‚Ä¢ train_data_clean.csv\n",
      "   ‚Ä¢ test_data_clean.csv\n",
      "\n",
      "üìù Sample from clean training data:\n",
      "                                       file  \\\n",
      "9583    asr_sinhala/data/98/983e3c6613.flac   \n",
      "83995   asr_sinhala/data/29/29ab15c6d4.flac   \n",
      "110010  asr_sinhala/data/b0/b0072f9ac0.flac   \n",
      "\n",
      "                                     sentence_cleaned  \n",
      "9583     ‡∂ë‡∂∫ ‡∂∏‡∑í‡∂Ω‡∑í‡∂ß‡∂ª‡∑í‡∂∫ ‡∂≠‡∑î‡∑Ö ‡∂≠‡∑í‚Äç‡∂∫‡∂± ‡∂¥‡∑ä‚Äç‡∂ª‡∂∞‡∑è‡∂±‡∂∏ ‡∂¥‡∑ä‚Äç‡∂ª‡∂≠‡∑í‡∂∏‡∑è‡∂±‡∂∫‡∂ö‡∑ä.  \n",
      "83995   ‡∑É‡∑è‡∑Ñ‡∑í‡∂≠‡∑ä‚Äç‡∂∫‡∂ö‡∂ª‡∑î‡∑Ä‡∑è‡∂ß ‡∂ä‡∂ß ‡∑Ä‡∑ê‡∂©‡∑í‡∂∫ ‡∂Ω‡∑ú‡∂ö‡∑î ‡∑Ä‡∂ú‡∂ö‡∑ì‡∂∏‡∂ö‡∑ä ‡∂≠‡∑í‡∂∫‡∑ô‡∂±‡∑Ä‡∑è.  \n",
      "110010                        ‡∂ï‡∂ú‡∑ú‡∂Ω‡∑ä‡∂Ω‡∂±‡∑ä‡∂ß ‡∂Ø‡∂ö‡∑í‡∂±‡∑ä‡∂± ‡∂Ω‡∑ê‡∂∂‡∑ô‡∂∫‡∑í  \n",
      "\n",
      "üìù Sample from clean test data:\n",
      "                                       file  \\\n",
      "116168  asr_sinhala/data/b6/b63bfb7a34.flac   \n",
      "48318   asr_sinhala/data/f1/f134378295.flac   \n",
      "71784   asr_sinhala/data/d3/d3bf1e10b0.flac   \n",
      "\n",
      "                                       sentence_cleaned  \n",
      "116168                                ‡∂∏‡∑ô‡∂ö‡∑ì ‡∑É‡∑í‡∑Ä‡∑ä ‡∂∏‡∑Ñ ‡∂∞‡∑è‡∂≠‡∑î  \n",
      "48318                            ‡∂ë‡∂Ø‡∑í‡∂±‡∑ô‡∂Ø‡∑è ‡∂¢‡∑ì‡∑Ä‡∑í‡∂≠‡∂∫‡∑ö ‡∑É‡∑í‡∂Ø‡∑î‡∑Ä‡∂±  \n",
      "71784   ‡∂Ö‡∂∞‡∑í‡∑Ä‡∑ö‡∂ú‡∑ì ‡∂∏‡∑è‡∂ª‡∑ä‡∂ú‡∂∫‡∑ö ‡∂∏‡∑ê‡∂Ø ‡∑É‡∑ö‡∑Ä‡∑è ‡∑É‡∑ä‡∂Æ‡∑è‡∂±‡∂∫‡∂ö‡∑ä ‡∂â‡∂Ø‡∑í ‡∂ö‡∑Ö ‡∂¥‡∂∏‡∂´‡∑í‡∂±‡∑ä  \n",
      "\n",
      "üìä Clean File Sizes:\n",
      "   ‚Ä¢ train_data_clean.csv: 7.60 MB\n",
      "   ‚Ä¢ test_data_clean.csv: 1.90 MB\n",
      "   ‚Ä¢ Total clean size: 9.50 MB\n",
      "\n",
      "üíæ Space Optimization:\n",
      "   ‚Ä¢ Original total size: 24.59 MB\n",
      "   ‚Ä¢ Clean total size: 9.50 MB\n",
      "   ‚Ä¢ Space saved: 15.09 MB (61.4%)\n",
      "\n",
      "‚úÖ Clean files ready for ASR training!\n"
     ]
    }
   ],
   "source": [
    "# Create clean versions with only essential columns\n",
    "print(\"üßπ Creating clean versions of train and test files...\")\n",
    "\n",
    "# Extract only file and sentence_cleaned columns from both datasets\n",
    "train_clean = train_data[['file', 'sentence_cleaned']].copy()\n",
    "test_clean = test_data[['file', 'sentence_cleaned']].copy()\n",
    "\n",
    "print(f\"üìä Clean Data Summary:\")\n",
    "print(f\"   ‚Ä¢ Training data: {len(train_clean):,} samples\")\n",
    "print(f\"   ‚Ä¢ Test data: {len(test_clean):,} samples\")\n",
    "\n",
    "# Save clean versions\n",
    "train_clean.to_csv(f\"{output_dir}/train_data_clean.csv\", index=False)\n",
    "test_clean.to_csv(f\"{output_dir}/test_data_clean.csv\", index=False)\n",
    "\n",
    "print(f\"\\nüíæ Clean files saved:\")\n",
    "print(f\"   ‚Ä¢ train_data_clean.csv\")\n",
    "print(f\"   ‚Ä¢ test_data_clean.csv\")\n",
    "\n",
    "# Display first few rows of clean data\n",
    "print(f\"\\nüìù Sample from clean training data:\")\n",
    "print(train_clean.head(3))\n",
    "\n",
    "print(f\"\\nüìù Sample from clean test data:\")\n",
    "print(test_clean.head(3))\n",
    "\n",
    "# Calculate file sizes\n",
    "train_clean_size = os.path.getsize(f\"{output_dir}/train_data_clean.csv\") / (1024 * 1024)\n",
    "test_clean_size = os.path.getsize(f\"{output_dir}/test_data_clean.csv\") / (1024 * 1024)\n",
    "\n",
    "print(f\"\\nüìä Clean File Sizes:\")\n",
    "print(f\"   ‚Ä¢ train_data_clean.csv: {train_clean_size:.2f} MB\")\n",
    "print(f\"   ‚Ä¢ test_data_clean.csv: {test_clean_size:.2f} MB\")\n",
    "print(f\"   ‚Ä¢ Total clean size: {train_clean_size + test_clean_size:.2f} MB\")\n",
    "\n",
    "# Show space savings\n",
    "original_train_size = os.path.getsize(f\"{output_dir}/train_data.csv\") / (1024 * 1024)\n",
    "original_test_size = os.path.getsize(f\"{output_dir}/test_data.csv\") / (1024 * 1024)\n",
    "total_original = original_train_size + original_test_size\n",
    "total_clean = train_clean_size + test_clean_size\n",
    "space_saved = total_original - total_clean\n",
    "\n",
    "print(f\"\\nüíæ Space Optimization:\")\n",
    "print(f\"   ‚Ä¢ Original total size: {total_original:.2f} MB\")\n",
    "print(f\"   ‚Ä¢ Clean total size: {total_clean:.2f} MB\")\n",
    "print(f\"   ‚Ä¢ Space saved: {space_saved:.2f} MB ({space_saved/total_original*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Clean files ready for ASR training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db1ddaf",
   "metadata": {},
   "source": [
    "## 16. Update Manifest Files for Clean Data\n",
    "\n",
    "Create updated manifest files that reference the clean data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8be54d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Creating clean manifest files...\n",
      "‚úÖ Clean manifest files created:\n",
      "   ‚Ä¢ train_manifest_clean.jsonl\n",
      "   ‚Ä¢ test_manifest_clean.jsonl\n",
      "   ‚Ä¢ metadata_clean.json\n",
      "\n",
      "üéØ Summary of Clean Data Files:\n",
      "   üìÑ CSV Files (essential columns only):\n",
      "      ‚Ä¢ train_data_clean.csv (72,155 samples)\n",
      "      ‚Ä¢ test_data_clean.csv (18,039 samples)\n",
      "   üìÑ Manifest Files (JSONL format):\n",
      "      ‚Ä¢ train_manifest_clean.jsonl\n",
      "      ‚Ä¢ test_manifest_clean.jsonl\n",
      "   üìÑ Metadata:\n",
      "      ‚Ä¢ metadata_clean.json\n",
      "\n",
      "‚ú® Ready for efficient ASR training with minimal data footprint!\n",
      "‚úÖ Clean manifest files created:\n",
      "   ‚Ä¢ train_manifest_clean.jsonl\n",
      "   ‚Ä¢ test_manifest_clean.jsonl\n",
      "   ‚Ä¢ metadata_clean.json\n",
      "\n",
      "üéØ Summary of Clean Data Files:\n",
      "   üìÑ CSV Files (essential columns only):\n",
      "      ‚Ä¢ train_data_clean.csv (72,155 samples)\n",
      "      ‚Ä¢ test_data_clean.csv (18,039 samples)\n",
      "   üìÑ Manifest Files (JSONL format):\n",
      "      ‚Ä¢ train_manifest_clean.jsonl\n",
      "      ‚Ä¢ test_manifest_clean.jsonl\n",
      "   üìÑ Metadata:\n",
      "      ‚Ä¢ metadata_clean.json\n",
      "\n",
      "‚ú® Ready for efficient ASR training with minimal data footprint!\n"
     ]
    }
   ],
   "source": [
    "# Create clean manifest files for ASR training\n",
    "def create_clean_manifest(data, filename):\n",
    "    \"\"\"Create manifest file using clean data structure\"\"\"\n",
    "    with open(f\"{output_dir}/{filename}\", 'w', encoding='utf-8') as f:\n",
    "        for _, row in data.iterrows():\n",
    "            manifest_entry = {\n",
    "                \"audio_filepath\": row['file'],\n",
    "                \"text\": row['sentence_cleaned'],\n",
    "                \"duration\": -1  # To be filled by audio processing\n",
    "            }\n",
    "            f.write(json.dumps(manifest_entry, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(\"üìÑ Creating clean manifest files...\")\n",
    "\n",
    "# Create clean manifest files\n",
    "create_clean_manifest(train_clean, \"train_manifest_clean.jsonl\")\n",
    "create_clean_manifest(test_clean, \"test_manifest_clean.jsonl\")\n",
    "\n",
    "print(f\"‚úÖ Clean manifest files created:\")\n",
    "print(f\"   ‚Ä¢ train_manifest_clean.jsonl\")\n",
    "print(f\"   ‚Ä¢ test_manifest_clean.jsonl\")\n",
    "\n",
    "# Update metadata for clean version\n",
    "clean_metadata = {\n",
    "    \"dataset_info\": {\n",
    "        \"total_samples\": len(train_clean) + len(test_clean),\n",
    "        \"train_samples\": len(train_clean),\n",
    "        \"test_samples\": len(test_clean),\n",
    "        \"train_percentage\": 80.0,\n",
    "        \"test_percentage\": 20.0,\n",
    "        \"vocabulary_size\": len(vocab),\n",
    "        \"avg_sentence_length\": float(pd.concat([train_clean['sentence_cleaned'], test_clean['sentence_cleaned']]).str.len().mean()),\n",
    "        \"file_format\": \"clean (file, sentence_cleaned only)\"\n",
    "    },\n",
    "    \"split_strategy\": \"80% training, 20% testing\",\n",
    "    \"data_structure\": {\n",
    "        \"columns\": [\"file\", \"sentence_cleaned\"],\n",
    "        \"description\": \"Minimal structure for efficient ASR training\"\n",
    "    },\n",
    "    \"preprocessing_steps\": [\n",
    "        \"Text normalization and cleaning\",\n",
    "        \"Unicode normalization (NFC)\",\n",
    "        \"Minimal whitespace normalization\",\n",
    "        \"Control character removal\",\n",
    "        \"Empty sentence removal\",\n",
    "        \"Length filtering (2-500 characters)\",\n",
    "        \"Sinhala character ratio filtering (>=50%)\",\n",
    "        \"Duplicate removal\",\n",
    "        \"Column reduction to essentials only\"\n",
    "    ],\n",
    "    \"file_info\": {\n",
    "        \"audio_format\": \"flac\",\n",
    "        \"text_encoding\": \"utf-8\",\n",
    "        \"language\": \"Sinhala (si)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save clean metadata\n",
    "with open(f\"{output_dir}/metadata_clean.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(clean_metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"   ‚Ä¢ metadata_clean.json\")\n",
    "\n",
    "print(f\"\\nüéØ Summary of Clean Data Files:\")\n",
    "print(f\"   üìÑ CSV Files (essential columns only):\")\n",
    "print(f\"      ‚Ä¢ train_data_clean.csv ({len(train_clean):,} samples)\")\n",
    "print(f\"      ‚Ä¢ test_data_clean.csv ({len(test_clean):,} samples)\")\n",
    "print(f\"   üìÑ Manifest Files (JSONL format):\")\n",
    "print(f\"      ‚Ä¢ train_manifest_clean.jsonl\")\n",
    "print(f\"      ‚Ä¢ test_manifest_clean.jsonl\")\n",
    "print(f\"   üìÑ Metadata:\")\n",
    "print(f\"      ‚Ä¢ metadata_clean.json\")\n",
    "print(f\"\\n‚ú® Ready for efficient ASR training with minimal data footprint!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

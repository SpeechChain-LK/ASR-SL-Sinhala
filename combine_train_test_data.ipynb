{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b45c4e6",
   "metadata": {},
   "source": [
    "# Combine Train and Test Data for Sinhala ASR\n",
    "\n",
    "This notebook combines train.csv and test.csv files, extracts file paths and sentences, and saves the combined dataset to a new CSV file for Sinhala Automatic Speech Recognition (ASR) research.\n",
    "\n",
    "## Overview\n",
    "- Load train and test CSV files\n",
    "- Extract file paths and sentences\n",
    "- Combine datasets with source identification\n",
    "- Save to new CSV file\n",
    "- Display dataset statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26709509",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import pandas for data manipulation and processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4dda1876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5634be1f",
   "metadata": {},
   "source": [
    "## 2. Read CSV Files\n",
    "\n",
    "Load train.csv and test.csv files using pandas, display their shapes and basic information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f56202da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train.csv...\n",
      "Train data shape: (132574, 6)\n",
      "Train data columns: ['Unnamed: 0', 'filename', 'x', 'sentence', 'full', 'file']\n",
      "First few rows of train data:\n",
      "Train data shape: (132574, 6)\n",
      "Train data columns: ['Unnamed: 0', 'filename', 'x', 'sentence', 'full', 'file']\n",
      "First few rows of train data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>filename</th>\n",
       "      <th>x</th>\n",
       "      <th>sentence</th>\n",
       "      <th>full</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69861</td>\n",
       "      <td>70d725d61b</td>\n",
       "      <td>6e92f</td>\n",
       "      <td>ශ්‍රාවක චරිත නිදසුන් කොට පැහැදිලි කරන්න.</td>\n",
       "      <td>70d725d61b6e92f</td>\n",
       "      <td>asr_sinhala/data/70/70d725d61b.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77001</td>\n",
       "      <td>7f2e316d14</td>\n",
       "      <td>3b15d</td>\n",
       "      <td>වෙන්න පුළුවනි</td>\n",
       "      <td>7f2e316d143b15d</td>\n",
       "      <td>asr_sinhala/data/7f/7f2e316d14.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>108545</td>\n",
       "      <td>bc470065df</td>\n",
       "      <td>4b5d0</td>\n",
       "      <td>එය තමයි ඔවුන්ට</td>\n",
       "      <td>bc470065df4b5d0</td>\n",
       "      <td>asr_sinhala/data/bc/bc470065df.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47802</td>\n",
       "      <td>4b48bbffc5</td>\n",
       "      <td>8e991</td>\n",
       "      <td>සප්ත ආර්ය ධනයෙහි එක් කොටසකි.</td>\n",
       "      <td>4b48bbffc58e991</td>\n",
       "      <td>asr_sinhala/data/4b/4b48bbffc5.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28441</td>\n",
       "      <td>2f2951d11c</td>\n",
       "      <td>936a6</td>\n",
       "      <td>මුදලාලි නිවසේ දොර විවෘත කිරීමෙන් පසු එය වසා නො...</td>\n",
       "      <td>2f2951d11c936a6</td>\n",
       "      <td>asr_sinhala/data/2f/2f2951d11c.flac</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    filename      x  \\\n",
       "0       69861  70d725d61b  6e92f   \n",
       "1       77001  7f2e316d14  3b15d   \n",
       "2      108545  bc470065df  4b5d0   \n",
       "3       47802  4b48bbffc5  8e991   \n",
       "4       28441  2f2951d11c  936a6   \n",
       "\n",
       "                                            sentence             full  \\\n",
       "0           ශ්‍රාවක චරිත නිදසුන් කොට පැහැදිලි කරන්න.  70d725d61b6e92f   \n",
       "1                                      වෙන්න පුළුවනි  7f2e316d143b15d   \n",
       "2                                     එය තමයි ඔවුන්ට  bc470065df4b5d0   \n",
       "3                       සප්ත ආර්ය ධනයෙහි එක් කොටසකි.  4b48bbffc58e991   \n",
       "4  මුදලාලි නිවසේ දොර විවෘත කිරීමෙන් පසු එය වසා නො...  2f2951d11c936a6   \n",
       "\n",
       "                                  file  \n",
       "0  asr_sinhala/data/70/70d725d61b.flac  \n",
       "1  asr_sinhala/data/7f/7f2e316d14.flac  \n",
       "2  asr_sinhala/data/bc/bc470065df.flac  \n",
       "3  asr_sinhala/data/4b/4b48bbffc5.flac  \n",
       "4  asr_sinhala/data/2f/2f2951d11c.flac  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read train CSV file\n",
    "print(\"Reading train.csv...\")\n",
    "train_df = pd.read_csv('train.csv')\n",
    "print(f\"Train data shape: {train_df.shape}\")\n",
    "print(f\"Train data columns: {list(train_df.columns)}\")\n",
    "print(f\"First few rows of train data:\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac580b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading test.csv...\n",
      "Test data shape: (23396, 6)\n",
      "Test data columns: ['Unnamed: 0', 'filename', 'x', 'sentence', 'full', 'file']\n",
      "First few rows of test data:\n",
      "Test data shape: (23396, 6)\n",
      "Test data columns: ['Unnamed: 0', 'filename', 'x', 'sentence', 'full', 'file']\n",
      "First few rows of test data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>filename</th>\n",
       "      <th>x</th>\n",
       "      <th>sentence</th>\n",
       "      <th>full</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>147490</td>\n",
       "      <td>f44918e4dd</td>\n",
       "      <td>cb1fe</td>\n",
       "      <td>පිරිමින්ට දීල තියෙනවා</td>\n",
       "      <td>f44918e4ddcb1fe</td>\n",
       "      <td>asr_sinhala/data/f4/f44918e4dd.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>149997</td>\n",
       "      <td>f7d50f4b2c</td>\n",
       "      <td>59778</td>\n",
       "      <td>එමෙන්ම මාර්ගය ඉදිකරන ස්ථානයට</td>\n",
       "      <td>f7d50f4b2c59778</td>\n",
       "      <td>asr_sinhala/data/f7/f7d50f4b2c.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12622</td>\n",
       "      <td>12d4081d0c</td>\n",
       "      <td>199c3</td>\n",
       "      <td>එතකොට සිරිගුත්ත හයියෙන් කෑ ගහලා</td>\n",
       "      <td>12d4081d0c199c3</td>\n",
       "      <td>asr_sinhala/data/12/12d4081d0c.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>124199</td>\n",
       "      <td>d2a53f5f0a</td>\n",
       "      <td>e1e01</td>\n",
       "      <td>එතන ගොඩගැහෙන්නේ සුන්බුන් ගොඩක් පමණයි.</td>\n",
       "      <td>d2a53f5f0ae1e01</td>\n",
       "      <td>asr_sinhala/data/d2/d2a53f5f0a.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>139037</td>\n",
       "      <td>e841b4dfe0</td>\n",
       "      <td>8f885</td>\n",
       "      <td>එක විශ්ව ධර්මය</td>\n",
       "      <td>e841b4dfe08f885</td>\n",
       "      <td>asr_sinhala/data/e8/e841b4dfe0.flac</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    filename      x                               sentence  \\\n",
       "0      147490  f44918e4dd  cb1fe                  පිරිමින්ට දීල තියෙනවා   \n",
       "1      149997  f7d50f4b2c  59778           එමෙන්ම මාර්ගය ඉදිකරන ස්ථානයට   \n",
       "2       12622  12d4081d0c  199c3        එතකොට සිරිගුත්ත හයියෙන් කෑ ගහලා   \n",
       "3      124199  d2a53f5f0a  e1e01  එතන ගොඩගැහෙන්නේ සුන්බුන් ගොඩක් පමණයි.   \n",
       "4      139037  e841b4dfe0  8f885                         එක විශ්ව ධර්මය   \n",
       "\n",
       "              full                                 file  \n",
       "0  f44918e4ddcb1fe  asr_sinhala/data/f4/f44918e4dd.flac  \n",
       "1  f7d50f4b2c59778  asr_sinhala/data/f7/f7d50f4b2c.flac  \n",
       "2  12d4081d0c199c3  asr_sinhala/data/12/12d4081d0c.flac  \n",
       "3  d2a53f5f0ae1e01  asr_sinhala/data/d2/d2a53f5f0a.flac  \n",
       "4  e841b4dfe08f885  asr_sinhala/data/e8/e841b4dfe0.flac  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read test CSV file\n",
    "print(\"Reading test.csv...\")\n",
    "test_df = pd.read_csv('test.csv')\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"Test data columns: {list(test_df.columns)}\")\n",
    "print(f\"First few rows of test data:\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f717d426",
   "metadata": {},
   "source": [
    "## 3. Extract Required Columns\n",
    "\n",
    "Extract 'file' and 'sentence' columns from both train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5aab5946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train extracted shape: (132574, 2)\n",
      "Test extracted shape: (23396, 2)\n",
      "\n",
      "Sample from train_extracted:\n",
      "                                  file  \\\n",
      "0  asr_sinhala/data/70/70d725d61b.flac   \n",
      "1  asr_sinhala/data/7f/7f2e316d14.flac   \n",
      "2  asr_sinhala/data/bc/bc470065df.flac   \n",
      "\n",
      "                                   sentence  \n",
      "0  ශ්‍රාවක චරිත නිදසුන් කොට පැහැදිලි කරන්න.  \n",
      "1                             වෙන්න පුළුවනි  \n",
      "2                            එය තමයි ඔවුන්ට  \n",
      "\n",
      "Sample from test_extracted:\n",
      "                                  file                         sentence\n",
      "0  asr_sinhala/data/f4/f44918e4dd.flac            පිරිමින්ට දීල තියෙනවා\n",
      "1  asr_sinhala/data/f7/f7d50f4b2c.flac     එමෙන්ම මාර්ගය ඉදිකරන ස්ථානයට\n",
      "2  asr_sinhala/data/12/12d4081d0c.flac  එතකොට සිරිගුත්ත හයියෙන් කෑ ගහලා\n"
     ]
    }
   ],
   "source": [
    "# Extract file path and sentence columns from both datasets\n",
    "train_extracted = train_df[['file', 'sentence']].copy()\n",
    "test_extracted = test_df[['file', 'sentence']].copy()\n",
    "\n",
    "print(f\"Train extracted shape: {train_extracted.shape}\")\n",
    "print(f\"Test extracted shape: {test_extracted.shape}\")\n",
    "print(f\"\\nSample from train_extracted:\")\n",
    "print(train_extracted.head(3))\n",
    "print(f\"\\nSample from test_extracted:\")\n",
    "print(test_extracted.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c796be3c",
   "metadata": {},
   "source": [
    "## 4. Add Source Identification\n",
    "\n",
    "Add a 'source' column to identify whether each record came from train or test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "198529c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data with source column:\n",
      "                                  file  \\\n",
      "0  asr_sinhala/data/70/70d725d61b.flac   \n",
      "1  asr_sinhala/data/7f/7f2e316d14.flac   \n",
      "2  asr_sinhala/data/bc/bc470065df.flac   \n",
      "\n",
      "                                   sentence source  \n",
      "0  ශ්‍රාවක චරිත නිදසුන් කොට පැහැදිලි කරන්න.  train  \n",
      "1                             වෙන්න පුළුවනි  train  \n",
      "2                            එය තමයි ඔවුන්ට  train  \n",
      "\n",
      "Test data with source column:\n",
      "                                  file                         sentence source\n",
      "0  asr_sinhala/data/f4/f44918e4dd.flac            පිරිමින්ට දීල තියෙනවා   test\n",
      "1  asr_sinhala/data/f7/f7d50f4b2c.flac     එමෙන්ම මාර්ගය ඉදිකරන ස්ථානයට   test\n",
      "2  asr_sinhala/data/12/12d4081d0c.flac  එතකොට සිරිගුත්ත හයියෙන් කෑ ගහලා   test\n",
      "\n",
      "Train records: 132574\n",
      "Test records: 23396\n"
     ]
    }
   ],
   "source": [
    "# Add a source column to identify which dataset each row came from\n",
    "train_extracted['source'] = 'train'\n",
    "test_extracted['source'] = 'test'\n",
    "\n",
    "print(\"Train data with source column:\")\n",
    "print(train_extracted.head(3))\n",
    "print(f\"\\nTest data with source column:\")\n",
    "print(test_extracted.head(3))\n",
    "\n",
    "print(f\"\\nTrain records: {len(train_extracted)}\")\n",
    "print(f\"Test records: {len(test_extracted)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66799b66",
   "metadata": {},
   "source": [
    "## 5. Combine Datasets\n",
    "\n",
    "Use pandas concat to merge the train and test datasets into a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "340e14ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data shape: (155970, 3)\n",
      "Total records: 155970\n",
      "Columns: ['file', 'sentence', 'source']\n",
      "\n",
      "Data distribution by source:\n",
      "source\n",
      "train    132574\n",
      "test      23396\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Combine both datasets\n",
    "combined_df = pd.concat([train_extracted, test_extracted], ignore_index=True)\n",
    "\n",
    "print(f\"Combined data shape: {combined_df.shape}\")\n",
    "print(f\"Total records: {len(combined_df)}\")\n",
    "print(f\"Columns: {list(combined_df.columns)}\")\n",
    "\n",
    "# Verify the combination\n",
    "print(f\"\\nData distribution by source:\")\n",
    "print(combined_df['source'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eea6510",
   "metadata": {},
   "source": [
    "## 6. Save Combined Data\n",
    "\n",
    "Export the combined dataset to a new CSV file named 'combined_file_path_sentence.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbfa2f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data saved to: combined_file_path_sentence.csv\n",
      "File size: 19.25 MB\n",
      "✅ File saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save to new CSV file\n",
    "output_file = 'combined_file_path_sentence.csv'\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "print(f\"Combined data saved to: {output_file}\")\n",
    "\n",
    "# Verify the file was created and check its size\n",
    "import os\n",
    "if os.path.exists(output_file):\n",
    "    file_size = os.path.getsize(output_file) / (1024 * 1024)  # MB\n",
    "    print(f\"File size: {file_size:.2f} MB\")\n",
    "    print(\"✅ File saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2296ae40",
   "metadata": {},
   "source": [
    "## 7. Display Data Overview\n",
    "\n",
    "Show the first few rows of the combined dataset and print basic information about record counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9548df20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of combined data:\n",
      "                                  file  \\\n",
      "0  asr_sinhala/data/70/70d725d61b.flac   \n",
      "1  asr_sinhala/data/7f/7f2e316d14.flac   \n",
      "2  asr_sinhala/data/bc/bc470065df.flac   \n",
      "3  asr_sinhala/data/4b/4b48bbffc5.flac   \n",
      "4  asr_sinhala/data/2f/2f2951d11c.flac   \n",
      "\n",
      "                                            sentence source  \n",
      "0           ශ්‍රාවක චරිත නිදසුන් කොට පැහැදිලි කරන්න.  train  \n",
      "1                                      වෙන්න පුළුවනි  train  \n",
      "2                                     එය තමයි ඔවුන්ට  train  \n",
      "3                       සප්ත ආර්ය ධනයෙහි එක් කොටසකි.  train  \n",
      "4  මුදලාලි නිවසේ දොර විවෘත කිරීමෙන් පසු එය වසා නො...  train  \n",
      "\n",
      "Last 5 rows of combined data:\n",
      "                                       file                    sentence source\n",
      "155965  asr_sinhala/data/7f/7f65556b1e.flac      එදා දවසෙන් වැඩි කොටසක්   test\n",
      "155966  asr_sinhala/data/dc/dcd0c96642.flac             little boy bomb   test\n",
      "155967  asr_sinhala/data/e2/e21b29cf06.flac  අනෙකුත් වැඩිහිටියන්ට පමණයි   test\n",
      "155968  asr_sinhala/data/12/1217bd55fd.flac       ආගමික සභාව කෙරෙහි ඇති   test\n",
      "155969  asr_sinhala/data/f4/f496206b58.flac      තම පුතනුවන්ට වඳින ලදී.   test\n",
      "\n",
      "Dataset info:\n",
      "- Shape: (155970, 3)\n",
      "- Columns: ['file', 'sentence', 'source']\n",
      "- Memory usage: 40.28 MB\n"
     ]
    }
   ],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows of combined data:\")\n",
    "print(combined_df.head())\n",
    "\n",
    "print(f\"\\nLast 5 rows of combined data:\")\n",
    "print(combined_df.tail())\n",
    "\n",
    "print(f\"\\nDataset info:\")\n",
    "print(f\"- Shape: {combined_df.shape}\")\n",
    "print(f\"- Columns: {list(combined_df.columns)}\")\n",
    "print(f\"- Memory usage: {combined_df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368db1fa",
   "metadata": {},
   "source": [
    "## 8. Generate Dataset Statistics\n",
    "\n",
    "Calculate and display dataset summary statistics including total sentences, unique file paths, and average sentence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4be86e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 DATASET SUMMARY STATISTICS\n",
      "==================================================\n",
      "📁 Total sentences: 155,970\n",
      "🎯 Unique file paths: 155,970\n",
      "\n",
      "📝 SENTENCE LENGTH ANALYSIS:\n",
      "   • Average: 34.1 characters\n",
      "   • Median: 24.0 characters\n",
      "   • Min: 2 characters\n",
      "   • Max: 156305 characters\n",
      "   • Std Dev: 799.2 characters\n",
      "\n",
      "📊 DATA SOURCE DISTRIBUTION:\n",
      "   • Train: 132,574 records (85.0%)\n",
      "   • Test: 23,396 records (15.0%)\n",
      "\n",
      "🔍 DATA QUALITY:\n",
      "   • Missing file paths: 0\n",
      "   • Missing sentences: 0\n",
      "\n",
      "📂 FILE PATH ANALYSIS:\n",
      "🎯 Unique file paths: 155,970\n",
      "\n",
      "📝 SENTENCE LENGTH ANALYSIS:\n",
      "   • Average: 34.1 characters\n",
      "   • Median: 24.0 characters\n",
      "   • Min: 2 characters\n",
      "   • Max: 156305 characters\n",
      "   • Std Dev: 799.2 characters\n",
      "\n",
      "📊 DATA SOURCE DISTRIBUTION:\n",
      "   • Train: 132,574 records (85.0%)\n",
      "   • Test: 23,396 records (15.0%)\n",
      "\n",
      "🔍 DATA QUALITY:\n",
      "   • Missing file paths: 0\n",
      "   • Missing sentences: 0\n",
      "\n",
      "📂 FILE PATH ANALYSIS:\n",
      "   • Unique directories: 238\n",
      "\n",
      "✅ Dataset processing completed successfully!\n",
      "   • Unique directories: 238\n",
      "\n",
      "✅ Dataset processing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Calculate comprehensive dataset statistics\n",
    "print(\"📊 DATASET SUMMARY STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Basic counts\n",
    "print(f\"📁 Total sentences: {len(combined_df):,}\")\n",
    "print(f\"🎯 Unique file paths: {combined_df['file'].nunique():,}\")\n",
    "\n",
    "# Text analysis\n",
    "sentence_lengths = combined_df['sentence'].str.len()\n",
    "print(f\"\\n📝 SENTENCE LENGTH ANALYSIS:\")\n",
    "print(f\"   • Average: {sentence_lengths.mean():.1f} characters\")\n",
    "print(f\"   • Median: {sentence_lengths.median():.1f} characters\")\n",
    "print(f\"   • Min: {sentence_lengths.min()} characters\")\n",
    "print(f\"   • Max: {sentence_lengths.max()} characters\")\n",
    "print(f\"   • Std Dev: {sentence_lengths.std():.1f} characters\")\n",
    "\n",
    "# Source distribution\n",
    "print(f\"\\n📊 DATA SOURCE DISTRIBUTION:\")\n",
    "source_counts = combined_df['source'].value_counts()\n",
    "for source, count in source_counts.items():\n",
    "    percentage = (count / len(combined_df)) * 100\n",
    "    print(f\"   • {source.capitalize()}: {count:,} records ({percentage:.1f}%)\")\n",
    "\n",
    "# Missing values check\n",
    "print(f\"\\n🔍 DATA QUALITY:\")\n",
    "missing_files = combined_df['file'].isnull().sum()\n",
    "missing_sentences = combined_df['sentence'].isnull().sum()\n",
    "print(f\"   • Missing file paths: {missing_files}\")\n",
    "print(f\"   • Missing sentences: {missing_sentences}\")\n",
    "\n",
    "# File path analysis\n",
    "print(f\"\\n📂 FILE PATH ANALYSIS:\")\n",
    "unique_directories = combined_df['file'].str.extract(r'data/([a-f0-9]{2})/')[0].nunique()\n",
    "print(f\"   • Unique directories: {unique_directories}\")\n",
    "\n",
    "print(f\"\\n✅ Dataset processing completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b82cdc",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Sinhala ASR Training\n",
    "\n",
    "This section preprocesses the combined dataset to prepare it for Sinhala Automatic Speech Recognition (ASR) model training. The preprocessing includes text normalization, cleaning, tokenization, and audio file validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493869f5",
   "metadata": {},
   "source": [
    "## 9. Text Normalization and Cleaning\n",
    "\n",
    "Clean and normalize the Sinhala text data by removing unwanted characters, normalizing whitespace, and handling special cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5083a763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Applying minimal cleaning to preserve Sinhala text integrity...\n",
      "\n",
      "📝 Text Cleaning Results:\n",
      "Before and after cleaning examples:\n",
      "1. Text preserved: 'ශ්‍රාවක චරිත නිදසුන් කොට පැහැදිලි කරන්න.'\n",
      "\n",
      "2. Text preserved: 'වෙන්න පුළුවනි'\n",
      "\n",
      "3. Text preserved: 'එය තමයි ඔවුන්ට'\n",
      "\n",
      "4. Text preserved: 'සප්ත ආර්ය ධනයෙහි එක් කොටසකි.'\n",
      "\n",
      "5. Text preserved: 'මුදලාලි නිවසේ දොර විවෘත කිරීමෙන් පසු එය වසා නොදමා ගෙතුළට ගොස්‌ තිබේ.'\n",
      "\n",
      "⚠️  Empty sentences after cleaning: 0\n",
      "\n",
      "📊 Cleaning Impact:\n",
      "   • Texts modified: 14 out of 155,970 (0.01%)\n",
      "   • Texts preserved: 155,956 (99.99%)\n",
      "\n",
      "📊 Length Comparison:\n",
      "   • Original avg length: 34.1 characters\n",
      "   • Cleaned avg length: 34.1 characters\n",
      "   • Length difference: -0.0 characters\n",
      "   • Min length: 2 characters\n",
      "   • Max length: 156305 characters\n",
      "\n",
      "📝 Text Cleaning Results:\n",
      "Before and after cleaning examples:\n",
      "1. Text preserved: 'ශ්‍රාවක චරිත නිදසුන් කොට පැහැදිලි කරන්න.'\n",
      "\n",
      "2. Text preserved: 'වෙන්න පුළුවනි'\n",
      "\n",
      "3. Text preserved: 'එය තමයි ඔවුන්ට'\n",
      "\n",
      "4. Text preserved: 'සප්ත ආර්ය ධනයෙහි එක් කොටසකි.'\n",
      "\n",
      "5. Text preserved: 'මුදලාලි නිවසේ දොර විවෘත කිරීමෙන් පසු එය වසා නොදමා ගෙතුළට ගොස්‌ තිබේ.'\n",
      "\n",
      "⚠️  Empty sentences after cleaning: 0\n",
      "\n",
      "📊 Cleaning Impact:\n",
      "   • Texts modified: 14 out of 155,970 (0.01%)\n",
      "   • Texts preserved: 155,956 (99.99%)\n",
      "\n",
      "📊 Length Comparison:\n",
      "   • Original avg length: 34.1 characters\n",
      "   • Cleaned avg length: 34.1 characters\n",
      "   • Length difference: -0.0 characters\n",
      "   • Min length: 2 characters\n",
      "   • Max length: 156305 characters\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def clean_sinhala_text(text):\n",
    "    \"\"\"\n",
    "    Minimal cleaning for Sinhala text to preserve original structure\n",
    "    Only removes truly problematic characters while preserving Sinhala integrity\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string if not already\n",
    "    text = str(text)\n",
    "    \n",
    "    # Only normalize Unicode to NFC (canonical composition) - preserve ZWJ and other important chars\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    \n",
    "    # Only normalize excessive whitespace (3+ spaces to single space)\n",
    "    text = re.sub(r' {3,}', ' ', text)\n",
    "    \n",
    "    # Remove leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Only remove obviously problematic characters (control chars except common ones)\n",
    "    # Preserve all Sinhala chars, ZWJ (\\u200D), ZWNJ (\\u200C), and normal punctuation\n",
    "    # Remove only control characters that are not needed for text rendering\n",
    "    text = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]', '', text)\n",
    "    \n",
    "    # Remove some rare Unicode categories that are clearly not text\n",
    "    # But preserve Sinhala, Latin, punctuation, symbols, joiners, etc.\n",
    "    cleaned_chars = []\n",
    "    for char in text:\n",
    "        category = unicodedata.category(char)\n",
    "        # Remove only format characters that are not joiners\n",
    "        if category == 'Cf' and char not in ['\\u200C', '\\u200D']:  # Preserve ZWNJ and ZWJ\n",
    "            continue\n",
    "        cleaned_chars.append(char)\n",
    "    \n",
    "    text = ''.join(cleaned_chars)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Apply minimal cleaning to the sentences\n",
    "print(\"🧹 Applying minimal cleaning to preserve Sinhala text integrity...\")\n",
    "combined_df['sentence_cleaned'] = combined_df['sentence'].apply(clean_sinhala_text)\n",
    "\n",
    "# Compare before and after cleaning\n",
    "print(\"\\n📝 Text Cleaning Results:\")\n",
    "print(\"Before and after cleaning examples:\")\n",
    "for i in range(5):\n",
    "    if i < len(combined_df):\n",
    "        original = combined_df.iloc[i]['sentence']\n",
    "        cleaned = combined_df.iloc[i]['sentence_cleaned']\n",
    "        if original != cleaned:\n",
    "            print(f\"{i+1}. Original: '{original}'\")\n",
    "            print(f\"   Cleaned:  '{cleaned}'\")\n",
    "            print(f\"   Changed:  {'Yes' if original != cleaned else 'No'}\")\n",
    "        else:\n",
    "            print(f\"{i+1}. Text preserved: '{cleaned}'\")\n",
    "        print()\n",
    "\n",
    "# Check for empty sentences after cleaning\n",
    "empty_after_cleaning = combined_df['sentence_cleaned'].str.len() == 0\n",
    "print(f\"⚠️  Empty sentences after cleaning: {empty_after_cleaning.sum()}\")\n",
    "\n",
    "if empty_after_cleaning.sum() > 0:\n",
    "    print(\"Examples of sentences that became empty:\")\n",
    "    empty_examples = combined_df[empty_after_cleaning]['sentence'].head(5)\n",
    "    for idx, sentence in empty_examples.items():\n",
    "        print(f\"  - '{sentence}'\")\n",
    "\n",
    "# Check how many texts were actually changed\n",
    "texts_changed = (combined_df['sentence'] != combined_df['sentence_cleaned']).sum()\n",
    "print(f\"\\n📊 Cleaning Impact:\")\n",
    "print(f\"   • Texts modified: {texts_changed:,} out of {len(combined_df):,} ({texts_changed/len(combined_df)*100:.2f}%)\")\n",
    "print(f\"   • Texts preserved: {len(combined_df) - texts_changed:,} ({(len(combined_df) - texts_changed)/len(combined_df)*100:.2f}%)\")\n",
    "\n",
    "# Update sentence length statistics after cleaning\n",
    "cleaned_lengths = combined_df['sentence_cleaned'].str.len()\n",
    "original_lengths = combined_df['sentence'].str.len()\n",
    "print(f\"\\n📊 Length Comparison:\")\n",
    "print(f\"   • Original avg length: {original_lengths.mean():.1f} characters\")\n",
    "print(f\"   • Cleaned avg length: {cleaned_lengths.mean():.1f} characters\")\n",
    "print(f\"   • Length difference: {(cleaned_lengths.mean() - original_lengths.mean()):.1f} characters\")\n",
    "print(f\"   • Min length: {cleaned_lengths.min()} characters\")\n",
    "print(f\"   • Max length: {cleaned_lengths.max()} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd96dfbb",
   "metadata": {},
   "source": [
    "## 10. Audio File Validation\n",
    "\n",
    "Validate that audio files exist and can be accessed for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba7a7901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Validating audio files...\n",
      "📁 Audio File Validation Results (sample of 100):\n",
      "   • Files found: 100\n",
      "   • Files missing: 0\n",
      "📁 Audio File Validation Results (sample of 100):\n",
      "   • Files found: 100\n",
      "   • Files missing: 0\n",
      "\n",
      "📊 File Extensions Distribution:\n",
      "   • .flac: 155,970 files\n",
      "\n",
      "📊 File Extensions Distribution:\n",
      "   • .flac: 155,970 files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def validate_audio_files(df, sample_size=1000):\n",
    "    \"\"\"\n",
    "    Validate existence of audio files\n",
    "    \"\"\"\n",
    "    print(\"🔍 Validating audio files...\")\n",
    "    \n",
    "    # Check if files exist (sample for performance)\n",
    "    sample_df = df.sample(n=min(sample_size, len(df)), random_state=42)\n",
    "    \n",
    "    existing_files = []\n",
    "    missing_files = []\n",
    "    \n",
    "    for idx, row in sample_df.iterrows():\n",
    "        file_path = row['file']\n",
    "        if os.path.exists(file_path):\n",
    "            existing_files.append(file_path)\n",
    "        else:\n",
    "            missing_files.append(file_path)\n",
    "    \n",
    "    return existing_files, missing_files\n",
    "\n",
    "# Validate a sample of audio files\n",
    "existing, missing = validate_audio_files(combined_df, sample_size=100)\n",
    "\n",
    "print(f\"📁 Audio File Validation Results (sample of 100):\")\n",
    "print(f\"   • Files found: {len(existing)}\")\n",
    "print(f\"   • Files missing: {len(missing)}\")\n",
    "\n",
    "if missing:\n",
    "    print(f\"\\n⚠️  Missing files examples:\")\n",
    "    for i, file_path in enumerate(missing[:5]):\n",
    "        print(f\"   {i+1}. {file_path}\")\n",
    "\n",
    "# Check unique file extensions\n",
    "file_extensions = combined_df['file'].str.extract(r'\\.([a-zA-Z0-9]+)$')[0].value_counts()\n",
    "print(f\"\\n📊 File Extensions Distribution:\")\n",
    "for ext, count in file_extensions.items():\n",
    "    print(f\"   • .{ext}: {count:,} files\")\n",
    "\n",
    "# Create a column for absolute file paths (if needed)\n",
    "combined_df['file_absolute'] = combined_df['file'].apply(lambda x: os.path.abspath(x) if os.path.exists(x) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704862fa",
   "metadata": {},
   "source": [
    "## 11. Data Filtering and Quality Control\n",
    "\n",
    "Filter out problematic samples and ensure data quality for ASR training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f20009a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Applying quality filters...\n",
      "📊 Filtering Results:\n",
      "   • Initial records: 155,970\n",
      "   • Empty sentences removed: 0\n",
      "   • Length filtered: 33\n",
      "   • Non-Sinhala removed: 5,751\n",
      "   • Duplicates removed: 59,992\n",
      "   • Final records: 90,194\n",
      "   • Retention rate: 57.8%\n",
      "\n",
      "📈 Post-filtering Statistics:\n",
      "   • Average length: 26.7 characters\n",
      "   • Median length: 25.0 characters\n",
      "   • Length range: 2 - 132 characters\n",
      "\n",
      "📊 Source Distribution After Filtering:\n",
      "   • Train: 82,166 records (91.1%)\n",
      "   • Test: 8,028 records (8.9%)\n",
      "📊 Filtering Results:\n",
      "   • Initial records: 155,970\n",
      "   • Empty sentences removed: 0\n",
      "   • Length filtered: 33\n",
      "   • Non-Sinhala removed: 5,751\n",
      "   • Duplicates removed: 59,992\n",
      "   • Final records: 90,194\n",
      "   • Retention rate: 57.8%\n",
      "\n",
      "📈 Post-filtering Statistics:\n",
      "   • Average length: 26.7 characters\n",
      "   • Median length: 25.0 characters\n",
      "   • Length range: 2 - 132 characters\n",
      "\n",
      "📊 Source Distribution After Filtering:\n",
      "   • Train: 82,166 records (91.1%)\n",
      "   • Test: 8,028 records (8.9%)\n"
     ]
    }
   ],
   "source": [
    "def filter_data_for_asr(df, min_length=2, max_length=500):\n",
    "    \"\"\"\n",
    "    Filter data based on quality criteria for ASR training\n",
    "    \"\"\"\n",
    "    print(\"🔄 Applying quality filters...\")\n",
    "    \n",
    "    initial_count = len(df)\n",
    "    \n",
    "    # Filter 1: Remove empty sentences\n",
    "    df_filtered = df[df['sentence_cleaned'].str.len() > 0].copy()\n",
    "    empty_removed = initial_count - len(df_filtered)\n",
    "    \n",
    "    # Filter 2: Remove sentences that are too short or too long\n",
    "    df_filtered = df_filtered[\n",
    "        (df_filtered['sentence_cleaned'].str.len() >= min_length) & \n",
    "        (df_filtered['sentence_cleaned'].str.len() <= max_length)\n",
    "    ].copy()\n",
    "    length_filtered = len(df) - len(df_filtered) - empty_removed\n",
    "    \n",
    "    # Filter 3: Remove sentences with mostly non-Sinhala characters\n",
    "    def is_mostly_sinhala(text):\n",
    "        if not text:\n",
    "            return False\n",
    "        sinhala_chars = len(re.findall(r'[\\u0D80-\\u0DFF]', text))\n",
    "        total_chars = len(re.findall(r'[^\\s]', text))  # Non-whitespace chars\n",
    "        return total_chars > 0 and (sinhala_chars / total_chars) >= 0.5\n",
    "    \n",
    "    df_filtered = df_filtered[df_filtered['sentence_cleaned'].apply(is_mostly_sinhala)].copy()\n",
    "    non_sinhala_removed = len(df) - len(df_filtered) - empty_removed - length_filtered\n",
    "    \n",
    "    # Filter 4: Remove duplicates based on cleaned sentences\n",
    "    df_filtered = df_filtered.drop_duplicates(subset=['sentence_cleaned'], keep='first').copy()\n",
    "    duplicates_removed = len(df) - len(df_filtered) - empty_removed - length_filtered - non_sinhala_removed\n",
    "    \n",
    "    print(f\"📊 Filtering Results:\")\n",
    "    print(f\"   • Initial records: {initial_count:,}\")\n",
    "    print(f\"   • Empty sentences removed: {empty_removed:,}\")\n",
    "    print(f\"   • Length filtered: {length_filtered:,}\")\n",
    "    print(f\"   • Non-Sinhala removed: {non_sinhala_removed:,}\")\n",
    "    print(f\"   • Duplicates removed: {duplicates_removed:,}\")\n",
    "    print(f\"   • Final records: {len(df_filtered):,}\")\n",
    "    print(f\"   • Retention rate: {len(df_filtered)/initial_count*100:.1f}%\")\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "# Apply filtering\n",
    "filtered_df = filter_data_for_asr(combined_df)\n",
    "\n",
    "# Update statistics after filtering\n",
    "print(f\"\\n📈 Post-filtering Statistics:\")\n",
    "filtered_lengths = filtered_df['sentence_cleaned'].str.len()\n",
    "print(f\"   • Average length: {filtered_lengths.mean():.1f} characters\")\n",
    "print(f\"   • Median length: {filtered_lengths.median():.1f} characters\")\n",
    "print(f\"   • Length range: {filtered_lengths.min()} - {filtered_lengths.max()} characters\")\n",
    "\n",
    "# Show distribution by source after filtering\n",
    "print(f\"\\n📊 Source Distribution After Filtering:\")\n",
    "source_dist = filtered_df['source'].value_counts()\n",
    "for source, count in source_dist.items():\n",
    "    percentage = (count / len(filtered_df)) * 100\n",
    "    print(f\"   • {source.capitalize()}: {count:,} records ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a3d514",
   "metadata": {},
   "source": [
    "## 12. Character-Level Analysis\n",
    "\n",
    "Analyze the character distribution for ASR vocabulary preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "939c0094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔤 Analyzing character distribution...\n",
      "📊 Character Distribution Summary:\n",
      "   • Total unique characters: 107\n",
      "   • Sinhala characters: 76\n",
      "   • Latin characters: 0\n",
      "   • Punctuation: 8\n",
      "   • Other characters: 22\n",
      "\n",
      "🇱🇰 Top 20 Sinhala Characters:\n",
      "    1. '්' : 179,293\n",
      "    2. 'න' : 164,429\n",
      "    3. 'ි' : 146,687\n",
      "    4. 'ව' : 116,263\n",
      "    5. 'ක' : 107,634\n",
      "    6. 'ය' : 103,131\n",
      "    7. 'ම' : 98,461\n",
      "    8. 'ත' : 94,662\n",
      "    9. 'ා' : 94,047\n",
      "   10. 'ර' : 79,568\n",
      "   11. 'ු' : 75,894\n",
      "   12. 'ස' : 64,996\n",
      "   13. 'ද' : 61,699\n",
      "   14. 'ෙ' : 55,223\n",
      "   15. 'ප' : 51,924\n",
      "   16. 'ල' : 51,695\n",
      "   17. 'ේ' : 49,952\n",
      "   18. 'හ' : 47,500\n",
      "   19. 'ට' : 45,513\n",
      "   20. 'ග' : 39,015\n",
      "\n",
      "📝 Punctuation Distribution:\n",
      "   • '.' : 23,771\n",
      "   • '?' : 1,882\n",
      "   • ',' : 1,323\n",
      "   • '!' : 263\n",
      "   • ''' : 142\n",
      "   • '\"' : 100\n",
      "   • ';' : 2\n",
      "   • ':' : 1\n",
      "\n",
      "📚 ASR Vocabulary:\n",
      "   • Vocabulary size: 101\n",
      "   • Characters included: ['<pad>', '<unk>', '<sos>', '<eos>', ' ', '්', 'න', 'ි', 'ව', 'ක', 'ය', 'ම', 'ත', 'ා', 'ර', 'ු', 'ස', 'ද', 'ෙ', 'ප']...\n",
      "\n",
      "💾 Vocabulary created with 101 characters\n",
      "📊 Character Distribution Summary:\n",
      "   • Total unique characters: 107\n",
      "   • Sinhala characters: 76\n",
      "   • Latin characters: 0\n",
      "   • Punctuation: 8\n",
      "   • Other characters: 22\n",
      "\n",
      "🇱🇰 Top 20 Sinhala Characters:\n",
      "    1. '්' : 179,293\n",
      "    2. 'න' : 164,429\n",
      "    3. 'ි' : 146,687\n",
      "    4. 'ව' : 116,263\n",
      "    5. 'ක' : 107,634\n",
      "    6. 'ය' : 103,131\n",
      "    7. 'ම' : 98,461\n",
      "    8. 'ත' : 94,662\n",
      "    9. 'ා' : 94,047\n",
      "   10. 'ර' : 79,568\n",
      "   11. 'ු' : 75,894\n",
      "   12. 'ස' : 64,996\n",
      "   13. 'ද' : 61,699\n",
      "   14. 'ෙ' : 55,223\n",
      "   15. 'ප' : 51,924\n",
      "   16. 'ල' : 51,695\n",
      "   17. 'ේ' : 49,952\n",
      "   18. 'හ' : 47,500\n",
      "   19. 'ට' : 45,513\n",
      "   20. 'ග' : 39,015\n",
      "\n",
      "📝 Punctuation Distribution:\n",
      "   • '.' : 23,771\n",
      "   • '?' : 1,882\n",
      "   • ',' : 1,323\n",
      "   • '!' : 263\n",
      "   • ''' : 142\n",
      "   • '\"' : 100\n",
      "   • ';' : 2\n",
      "   • ':' : 1\n",
      "\n",
      "📚 ASR Vocabulary:\n",
      "   • Vocabulary size: 101\n",
      "   • Characters included: ['<pad>', '<unk>', '<sos>', '<eos>', ' ', '්', 'න', 'ි', 'ව', 'ක', 'ය', 'ම', 'ත', 'ා', 'ර', 'ු', 'ස', 'ද', 'ෙ', 'ප']...\n",
      "\n",
      "💾 Vocabulary created with 101 characters\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def analyze_characters(df):\n",
    "    \"\"\"\n",
    "    Analyze character distribution in the dataset\n",
    "    \"\"\"\n",
    "    print(\"🔤 Analyzing character distribution...\")\n",
    "    \n",
    "    # Combine all text\n",
    "    all_text = ' '.join(df['sentence_cleaned'].tolist())\n",
    "    \n",
    "    # Count characters\n",
    "    char_counts = Counter(all_text)\n",
    "    \n",
    "    # Separate different character types\n",
    "    sinhala_chars = {}\n",
    "    punctuation_chars = {}\n",
    "    latin_chars = {}\n",
    "    other_chars = {}\n",
    "    \n",
    "    for char, count in char_counts.items():\n",
    "        if '\\u0D80' <= char <= '\\u0DFF':  # Sinhala Unicode range\n",
    "            sinhala_chars[char] = count\n",
    "        elif char.isalpha() and ord(char) < 128:  # Basic Latin\n",
    "            latin_chars[char] = count\n",
    "        elif char in '.,!?;:\"\\'-()[]{}':  # Common punctuation\n",
    "            punctuation_chars[char] = count\n",
    "        elif char != ' ':  # Skip spaces\n",
    "            other_chars[char] = count\n",
    "    \n",
    "    return sinhala_chars, latin_chars, punctuation_chars, other_chars, char_counts\n",
    "\n",
    "# Analyze character distribution\n",
    "sinhala_chars, latin_chars, punct_chars, other_chars, all_chars = analyze_characters(filtered_df)\n",
    "\n",
    "print(f\"📊 Character Distribution Summary:\")\n",
    "print(f\"   • Total unique characters: {len(all_chars):,}\")\n",
    "print(f\"   • Sinhala characters: {len(sinhala_chars):,}\")\n",
    "print(f\"   • Latin characters: {len(latin_chars):,}\")\n",
    "print(f\"   • Punctuation: {len(punct_chars):,}\")\n",
    "print(f\"   • Other characters: {len(other_chars):,}\")\n",
    "\n",
    "# Show top Sinhala characters\n",
    "print(f\"\\n🇱🇰 Top 20 Sinhala Characters:\")\n",
    "top_sinhala = sorted(sinhala_chars.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "for i, (char, count) in enumerate(top_sinhala, 1):\n",
    "    print(f\"   {i:2d}. '{char}' : {count:,}\")\n",
    "\n",
    "# Show punctuation distribution\n",
    "if punct_chars:\n",
    "    print(f\"\\n📝 Punctuation Distribution:\")\n",
    "    for char, count in sorted(punct_chars.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"   • '{char}' : {count:,}\")\n",
    "\n",
    "# Create vocabulary for ASR\n",
    "def create_vocab(char_counts, min_frequency=10):\n",
    "    \"\"\"Create vocabulary list for ASR training\"\"\"\n",
    "    vocab = []\n",
    "    \n",
    "    # Add special tokens\n",
    "    special_tokens = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "    vocab.extend(special_tokens)\n",
    "    \n",
    "    # Add space\n",
    "    vocab.append(' ')\n",
    "    \n",
    "    # Add characters with frequency >= min_frequency\n",
    "    for char, count in sorted(char_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        if char != ' ' and count >= min_frequency:\n",
    "            vocab.append(char)\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "# Create vocabulary\n",
    "vocab = create_vocab(all_chars, min_frequency=5)\n",
    "print(f\"\\n📚 ASR Vocabulary:\")\n",
    "print(f\"   • Vocabulary size: {len(vocab)}\")\n",
    "print(f\"   • Characters included: {vocab[:20]}...\")  # Show first 20\n",
    "\n",
    "# Save vocabulary\n",
    "vocab_df = pd.DataFrame({'character': vocab, 'index': range(len(vocab))})\n",
    "print(f\"\\n💾 Vocabulary created with {len(vocab)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2f41b4",
   "metadata": {},
   "source": [
    "## 13. Create Training and Test Splits (80/20)\n",
    "\n",
    "Split the preprocessed data into 80% training and 20% testing sets for ASR model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "66ba62d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Creating train/test splits (80/20)...\n",
      "📊 Data Split Summary:\n",
      "   • Training set: 72,155 samples (80.0%)\n",
      "   • Test set: 18,039 samples (20.0%)\n",
      "\n",
      "📈 Source Distribution by Split:\n",
      "\n",
      "Training:\n",
      "   • Train: 65,733 (91.1%)\n",
      "   • Test: 6,422 (8.9%)\n",
      "\n",
      "Test:\n",
      "   • Train: 16,433 (91.1%)\n",
      "   • Test: 1,606 (8.9%)\n",
      "\n",
      "📏 Average Sentence Length by Split:\n",
      "   • Training: 26.7 characters\n",
      "   • Test: 26.7 characters\n",
      "\n",
      "💡 Note: You can create a validation subset from training data during model training if needed.\n",
      "   Recommended: Use 10-15% of training data for validation during training.\n",
      "📊 Data Split Summary:\n",
      "   • Training set: 72,155 samples (80.0%)\n",
      "   • Test set: 18,039 samples (20.0%)\n",
      "\n",
      "📈 Source Distribution by Split:\n",
      "\n",
      "Training:\n",
      "   • Train: 65,733 (91.1%)\n",
      "   • Test: 6,422 (8.9%)\n",
      "\n",
      "Test:\n",
      "   • Train: 16,433 (91.1%)\n",
      "   • Test: 1,606 (8.9%)\n",
      "\n",
      "📏 Average Sentence Length by Split:\n",
      "   • Training: 26.7 characters\n",
      "   • Test: 26.7 characters\n",
      "\n",
      "💡 Note: You can create a validation subset from training data during model training if needed.\n",
      "   Recommended: Use 10-15% of training data for validation during training.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_asr_splits(df, test_size=0.20, random_state=42):\n",
    "    \"\"\"\n",
    "    Create train/test splits for ASR training (80/20)\n",
    "    \"\"\"\n",
    "    print(\"🔄 Creating train/test splits (80/20)...\")\n",
    "    \n",
    "    # Split into train (80%) and test (20%)\n",
    "    train_df, test_df = train_test_split(\n",
    "        df, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state,\n",
    "        stratify=df['source']  # Maintain source distribution\n",
    "    )\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "# Create splits\n",
    "train_data, test_data = create_asr_splits(filtered_df)\n",
    "\n",
    "print(f\"📊 Data Split Summary:\")\n",
    "print(f\"   • Training set: {len(train_data):,} samples ({len(train_data)/len(filtered_df)*100:.1f}%)\")\n",
    "print(f\"   • Test set: {len(test_data):,} samples ({len(test_data)/len(filtered_df)*100:.1f}%)\")\n",
    "\n",
    "# Check source distribution in each split\n",
    "print(f\"\\n📈 Source Distribution by Split:\")\n",
    "for split_name, split_data in [(\"Training\", train_data), (\"Test\", test_data)]:\n",
    "    source_counts = split_data['source'].value_counts()\n",
    "    print(f\"\\n{split_name}:\")\n",
    "    for source, count in source_counts.items():\n",
    "        percentage = (count / len(split_data)) * 100\n",
    "        print(f\"   • {source.capitalize()}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "# Check sentence length distribution across splits\n",
    "print(f\"\\n📏 Average Sentence Length by Split:\")\n",
    "for split_name, split_data in [(\"Training\", train_data), (\"Test\", test_data)]:\n",
    "    avg_length = split_data['sentence_cleaned'].str.len().mean()\n",
    "    print(f\"   • {split_name}: {avg_length:.1f} characters\")\n",
    "\n",
    "# Create a validation subset from training data if needed for model development\n",
    "print(f\"\\n💡 Note: You can create a validation subset from training data during model training if needed.\")\n",
    "print(f\"   Recommended: Use 10-15% of training data for validation during training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7b95d6",
   "metadata": {},
   "source": [
    "## 14. Export Preprocessed Data\n",
    "\n",
    "Save the preprocessed and split data in formats suitable for ASR training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9202a581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Exporting preprocessed data to 'processed_asr_data' directory...\n",
      "✅ Export completed! Files created:\n",
      "   📄 CSV Files:\n",
      "      • train_data.csv (72,155 samples - 80%)\n",
      "      • test_data.csv (18,039 samples - 20%)\n",
      "      • vocabulary.csv (101 characters)\n",
      "   📄 Manifest Files (JSONL format):\n",
      "      • train_manifest.jsonl\n",
      "      • test_manifest.jsonl\n",
      "   📄 Metadata:\n",
      "      • metadata.json\n",
      "      📁 metadata.json: 0.00 MB\n",
      "      📁 test_data.csv: 4.92 MB\n",
      "      📁 test_manifest.jsonl: 3.07 MB\n",
      "      📁 train_data.csv: 19.67 MB\n",
      "      📁 train_manifest.jsonl: 12.27 MB\n",
      "      📁 vocabulary.csv: 0.00 MB\n",
      "\n",
      "📊 Total exported data size: 39.94 MB\n",
      "\n",
      "🎯 Data is ready for Sinhala ASR training with 80/20 split!\n",
      "\n",
      "💡 Training Tips:\n",
      "   • Use the full training set (80%) for model training\n",
      "   • Reserve test set (20%) for final evaluation only\n",
      "   • Consider using cross-validation or a subset of training data for validation during development\n",
      "   • Many ASR frameworks can automatically split training data into train/val during training\n",
      "✅ Export completed! Files created:\n",
      "   📄 CSV Files:\n",
      "      • train_data.csv (72,155 samples - 80%)\n",
      "      • test_data.csv (18,039 samples - 20%)\n",
      "      • vocabulary.csv (101 characters)\n",
      "   📄 Manifest Files (JSONL format):\n",
      "      • train_manifest.jsonl\n",
      "      • test_manifest.jsonl\n",
      "   📄 Metadata:\n",
      "      • metadata.json\n",
      "      📁 metadata.json: 0.00 MB\n",
      "      📁 test_data.csv: 4.92 MB\n",
      "      📁 test_manifest.jsonl: 3.07 MB\n",
      "      📁 train_data.csv: 19.67 MB\n",
      "      📁 train_manifest.jsonl: 12.27 MB\n",
      "      📁 vocabulary.csv: 0.00 MB\n",
      "\n",
      "📊 Total exported data size: 39.94 MB\n",
      "\n",
      "🎯 Data is ready for Sinhala ASR training with 80/20 split!\n",
      "\n",
      "💡 Training Tips:\n",
      "   • Use the full training set (80%) for model training\n",
      "   • Reserve test set (20%) for final evaluation only\n",
      "   • Consider using cross-validation or a subset of training data for validation during development\n",
      "   • Many ASR frameworks can automatically split training data into train/val during training\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Create output directory for processed data\n",
    "output_dir = \"processed_asr_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"💾 Exporting preprocessed data to '{output_dir}' directory...\")\n",
    "\n",
    "# Export splits as CSV files\n",
    "train_data.to_csv(f\"{output_dir}/train_data.csv\", index=False)\n",
    "test_data.to_csv(f\"{output_dir}/test_data.csv\", index=False)\n",
    "\n",
    "# Export vocabulary\n",
    "vocab_df.to_csv(f\"{output_dir}/vocabulary.csv\", index=False)\n",
    "\n",
    "# Create metadata file\n",
    "metadata = {\n",
    "    \"dataset_info\": {\n",
    "        \"total_samples\": len(filtered_df),\n",
    "        \"train_samples\": len(train_data),\n",
    "        \"test_samples\": len(test_data),\n",
    "        \"train_percentage\": round(len(train_data)/len(filtered_df)*100, 1),\n",
    "        \"test_percentage\": round(len(test_data)/len(filtered_df)*100, 1),\n",
    "        \"vocabulary_size\": len(vocab),\n",
    "        \"avg_sentence_length\": float(filtered_df['sentence_cleaned'].str.len().mean()),\n",
    "        \"max_sentence_length\": int(filtered_df['sentence_cleaned'].str.len().max()),\n",
    "        \"min_sentence_length\": int(filtered_df['sentence_cleaned'].str.len().min())\n",
    "    },\n",
    "    \"split_strategy\": \"80% training, 20% testing\",\n",
    "    \"preprocessing_steps\": [\n",
    "        \"Text normalization and cleaning\",\n",
    "        \"Unicode normalization\",\n",
    "        \"Whitespace normalization\",\n",
    "        \"Punctuation normalization\",\n",
    "        \"Empty sentence removal\",\n",
    "        \"Length filtering (2-500 characters)\",\n",
    "        \"Sinhala character ratio filtering (>=50%)\",\n",
    "        \"Duplicate removal\"\n",
    "    ],\n",
    "    \"file_info\": {\n",
    "        \"audio_format\": \"flac\",\n",
    "        \"text_encoding\": \"utf-8\",\n",
    "        \"language\": \"Sinhala (si)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "with open(f\"{output_dir}/metadata.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Create manifest files for ASR training (common format)\n",
    "def create_manifest(data, filename):\n",
    "    \"\"\"Create manifest file in JSON Lines format for ASR training\"\"\"\n",
    "    with open(f\"{output_dir}/{filename}\", 'w', encoding='utf-8') as f:\n",
    "        for _, row in data.iterrows():\n",
    "            manifest_entry = {\n",
    "                \"audio_filepath\": row['file'],\n",
    "                \"text\": row['sentence_cleaned'],\n",
    "                \"duration\": -1,  # To be filled by audio processing\n",
    "                \"source\": row['source']\n",
    "            }\n",
    "            f.write(json.dumps(manifest_entry, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# Create manifest files\n",
    "create_manifest(train_data, \"train_manifest.jsonl\")\n",
    "create_manifest(test_data, \"test_manifest.jsonl\")\n",
    "\n",
    "print(f\"✅ Export completed! Files created:\")\n",
    "print(f\"   📄 CSV Files:\")\n",
    "print(f\"      • train_data.csv ({len(train_data):,} samples - 80%)\")\n",
    "print(f\"      • test_data.csv ({len(test_data):,} samples - 20%)\")\n",
    "print(f\"      • vocabulary.csv ({len(vocab)} characters)\")\n",
    "print(f\"   📄 Manifest Files (JSONL format):\")\n",
    "print(f\"      • train_manifest.jsonl\")\n",
    "print(f\"      • test_manifest.jsonl\")\n",
    "print(f\"   📄 Metadata:\")\n",
    "print(f\"      • metadata.json\")\n",
    "\n",
    "# Show file sizes\n",
    "total_size = 0\n",
    "for filename in os.listdir(output_dir):\n",
    "    file_path = os.path.join(output_dir, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "        total_size += size_mb\n",
    "        print(f\"      📁 {filename}: {size_mb:.2f} MB\")\n",
    "\n",
    "print(f\"\\n📊 Total exported data size: {total_size:.2f} MB\")\n",
    "print(f\"\\n🎯 Data is ready for Sinhala ASR training with 80/20 split!\")\n",
    "\n",
    "# Provide guidance on validation\n",
    "print(f\"\\n💡 Training Tips:\")\n",
    "print(f\"   • Use the full training set (80%) for model training\")\n",
    "print(f\"   • Reserve test set (20%) for final evaluation only\")\n",
    "print(f\"   • Consider using cross-validation or a subset of training data for validation during development\")\n",
    "print(f\"   • Many ASR frameworks can automatically split training data into train/val during training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ddbb62",
   "metadata": {},
   "source": [
    "## 15. Create Clean Files with Essential Columns Only\n",
    "\n",
    "Extract only the essential columns (`file` and `sentence_cleaned`) from train and test files for ASR training efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "17144173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Creating clean versions of train and test files...\n",
      "📊 Clean Data Summary:\n",
      "   • Training data: 72,155 samples\n",
      "   • Test data: 18,039 samples\n",
      "📊 Clean Data Summary:\n",
      "   • Training data: 72,155 samples\n",
      "   • Test data: 18,039 samples\n",
      "\n",
      "💾 Clean files saved:\n",
      "   • train_data_clean.csv\n",
      "   • test_data_clean.csv\n",
      "\n",
      "📝 Sample from clean training data:\n",
      "                                       file  \\\n",
      "9583    asr_sinhala/data/98/983e3c6613.flac   \n",
      "83995   asr_sinhala/data/29/29ab15c6d4.flac   \n",
      "110010  asr_sinhala/data/b0/b0072f9ac0.flac   \n",
      "\n",
      "                                     sentence_cleaned  \n",
      "9583     එය මිලිටරිය තුළ ති‍යන ප්‍රධානම ප්‍රතිමානයක්.  \n",
      "83995   සාහිත්‍යකරුවාට ඊට වැඩිය ලොකු වගකීමක් තියෙනවා.  \n",
      "110010                        ඕගොල්ලන්ට දකින්න ලැබෙයි  \n",
      "\n",
      "📝 Sample from clean test data:\n",
      "                                       file  \\\n",
      "116168  asr_sinhala/data/b6/b63bfb7a34.flac   \n",
      "48318   asr_sinhala/data/f1/f134378295.flac   \n",
      "71784   asr_sinhala/data/d3/d3bf1e10b0.flac   \n",
      "\n",
      "                                       sentence_cleaned  \n",
      "116168                                මෙකී සිව් මහ ධාතු  \n",
      "48318                            එදිනෙදා ජීවිතයේ සිදුවන  \n",
      "71784   අධිවේගී මාර්ගයේ මැද සේවා ස්ථානයක් ඉදි කළ පමණින්  \n",
      "\n",
      "📊 Clean File Sizes:\n",
      "   • train_data_clean.csv: 7.60 MB\n",
      "   • test_data_clean.csv: 1.90 MB\n",
      "   • Total clean size: 9.50 MB\n",
      "\n",
      "💾 Space Optimization:\n",
      "   • Original total size: 24.59 MB\n",
      "   • Clean total size: 9.50 MB\n",
      "   • Space saved: 15.09 MB (61.4%)\n",
      "\n",
      "✅ Clean files ready for ASR training!\n",
      "\n",
      "💾 Clean files saved:\n",
      "   • train_data_clean.csv\n",
      "   • test_data_clean.csv\n",
      "\n",
      "📝 Sample from clean training data:\n",
      "                                       file  \\\n",
      "9583    asr_sinhala/data/98/983e3c6613.flac   \n",
      "83995   asr_sinhala/data/29/29ab15c6d4.flac   \n",
      "110010  asr_sinhala/data/b0/b0072f9ac0.flac   \n",
      "\n",
      "                                     sentence_cleaned  \n",
      "9583     එය මිලිටරිය තුළ ති‍යන ප්‍රධානම ප්‍රතිමානයක්.  \n",
      "83995   සාහිත්‍යකරුවාට ඊට වැඩිය ලොකු වගකීමක් තියෙනවා.  \n",
      "110010                        ඕගොල්ලන්ට දකින්න ලැබෙයි  \n",
      "\n",
      "📝 Sample from clean test data:\n",
      "                                       file  \\\n",
      "116168  asr_sinhala/data/b6/b63bfb7a34.flac   \n",
      "48318   asr_sinhala/data/f1/f134378295.flac   \n",
      "71784   asr_sinhala/data/d3/d3bf1e10b0.flac   \n",
      "\n",
      "                                       sentence_cleaned  \n",
      "116168                                මෙකී සිව් මහ ධාතු  \n",
      "48318                            එදිනෙදා ජීවිතයේ සිදුවන  \n",
      "71784   අධිවේගී මාර්ගයේ මැද සේවා ස්ථානයක් ඉදි කළ පමණින්  \n",
      "\n",
      "📊 Clean File Sizes:\n",
      "   • train_data_clean.csv: 7.60 MB\n",
      "   • test_data_clean.csv: 1.90 MB\n",
      "   • Total clean size: 9.50 MB\n",
      "\n",
      "💾 Space Optimization:\n",
      "   • Original total size: 24.59 MB\n",
      "   • Clean total size: 9.50 MB\n",
      "   • Space saved: 15.09 MB (61.4%)\n",
      "\n",
      "✅ Clean files ready for ASR training!\n"
     ]
    }
   ],
   "source": [
    "# Create clean versions with only essential columns\n",
    "print(\"🧹 Creating clean versions of train and test files...\")\n",
    "\n",
    "# Extract only file and sentence_cleaned columns from both datasets\n",
    "train_clean = train_data[['file', 'sentence_cleaned']].copy()\n",
    "test_clean = test_data[['file', 'sentence_cleaned']].copy()\n",
    "\n",
    "print(f\"📊 Clean Data Summary:\")\n",
    "print(f\"   • Training data: {len(train_clean):,} samples\")\n",
    "print(f\"   • Test data: {len(test_clean):,} samples\")\n",
    "\n",
    "# Save clean versions\n",
    "train_clean.to_csv(f\"{output_dir}/train_data_clean.csv\", index=False)\n",
    "test_clean.to_csv(f\"{output_dir}/test_data_clean.csv\", index=False)\n",
    "\n",
    "print(f\"\\n💾 Clean files saved:\")\n",
    "print(f\"   • train_data_clean.csv\")\n",
    "print(f\"   • test_data_clean.csv\")\n",
    "\n",
    "# Display first few rows of clean data\n",
    "print(f\"\\n📝 Sample from clean training data:\")\n",
    "print(train_clean.head(3))\n",
    "\n",
    "print(f\"\\n📝 Sample from clean test data:\")\n",
    "print(test_clean.head(3))\n",
    "\n",
    "# Calculate file sizes\n",
    "train_clean_size = os.path.getsize(f\"{output_dir}/train_data_clean.csv\") / (1024 * 1024)\n",
    "test_clean_size = os.path.getsize(f\"{output_dir}/test_data_clean.csv\") / (1024 * 1024)\n",
    "\n",
    "print(f\"\\n📊 Clean File Sizes:\")\n",
    "print(f\"   • train_data_clean.csv: {train_clean_size:.2f} MB\")\n",
    "print(f\"   • test_data_clean.csv: {test_clean_size:.2f} MB\")\n",
    "print(f\"   • Total clean size: {train_clean_size + test_clean_size:.2f} MB\")\n",
    "\n",
    "# Show space savings\n",
    "original_train_size = os.path.getsize(f\"{output_dir}/train_data.csv\") / (1024 * 1024)\n",
    "original_test_size = os.path.getsize(f\"{output_dir}/test_data.csv\") / (1024 * 1024)\n",
    "total_original = original_train_size + original_test_size\n",
    "total_clean = train_clean_size + test_clean_size\n",
    "space_saved = total_original - total_clean\n",
    "\n",
    "print(f\"\\n💾 Space Optimization:\")\n",
    "print(f\"   • Original total size: {total_original:.2f} MB\")\n",
    "print(f\"   • Clean total size: {total_clean:.2f} MB\")\n",
    "print(f\"   • Space saved: {space_saved:.2f} MB ({space_saved/total_original*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n✅ Clean files ready for ASR training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db1ddaf",
   "metadata": {},
   "source": [
    "## 16. Update Manifest Files for Clean Data\n",
    "\n",
    "Create updated manifest files that reference the clean data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8be54d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Creating clean manifest files...\n",
      "✅ Clean manifest files created:\n",
      "   • train_manifest_clean.jsonl\n",
      "   • test_manifest_clean.jsonl\n",
      "   • metadata_clean.json\n",
      "\n",
      "🎯 Summary of Clean Data Files:\n",
      "   📄 CSV Files (essential columns only):\n",
      "      • train_data_clean.csv (72,155 samples)\n",
      "      • test_data_clean.csv (18,039 samples)\n",
      "   📄 Manifest Files (JSONL format):\n",
      "      • train_manifest_clean.jsonl\n",
      "      • test_manifest_clean.jsonl\n",
      "   📄 Metadata:\n",
      "      • metadata_clean.json\n",
      "\n",
      "✨ Ready for efficient ASR training with minimal data footprint!\n",
      "✅ Clean manifest files created:\n",
      "   • train_manifest_clean.jsonl\n",
      "   • test_manifest_clean.jsonl\n",
      "   • metadata_clean.json\n",
      "\n",
      "🎯 Summary of Clean Data Files:\n",
      "   📄 CSV Files (essential columns only):\n",
      "      • train_data_clean.csv (72,155 samples)\n",
      "      • test_data_clean.csv (18,039 samples)\n",
      "   📄 Manifest Files (JSONL format):\n",
      "      • train_manifest_clean.jsonl\n",
      "      • test_manifest_clean.jsonl\n",
      "   📄 Metadata:\n",
      "      • metadata_clean.json\n",
      "\n",
      "✨ Ready for efficient ASR training with minimal data footprint!\n"
     ]
    }
   ],
   "source": [
    "# Create clean manifest files for ASR training\n",
    "def create_clean_manifest(data, filename):\n",
    "    \"\"\"Create manifest file using clean data structure\"\"\"\n",
    "    with open(f\"{output_dir}/{filename}\", 'w', encoding='utf-8') as f:\n",
    "        for _, row in data.iterrows():\n",
    "            manifest_entry = {\n",
    "                \"audio_filepath\": row['file'],\n",
    "                \"text\": row['sentence_cleaned'],\n",
    "                \"duration\": -1  # To be filled by audio processing\n",
    "            }\n",
    "            f.write(json.dumps(manifest_entry, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(\"📄 Creating clean manifest files...\")\n",
    "\n",
    "# Create clean manifest files\n",
    "create_clean_manifest(train_clean, \"train_manifest_clean.jsonl\")\n",
    "create_clean_manifest(test_clean, \"test_manifest_clean.jsonl\")\n",
    "\n",
    "print(f\"✅ Clean manifest files created:\")\n",
    "print(f\"   • train_manifest_clean.jsonl\")\n",
    "print(f\"   • test_manifest_clean.jsonl\")\n",
    "\n",
    "# Update metadata for clean version\n",
    "clean_metadata = {\n",
    "    \"dataset_info\": {\n",
    "        \"total_samples\": len(train_clean) + len(test_clean),\n",
    "        \"train_samples\": len(train_clean),\n",
    "        \"test_samples\": len(test_clean),\n",
    "        \"train_percentage\": 80.0,\n",
    "        \"test_percentage\": 20.0,\n",
    "        \"vocabulary_size\": len(vocab),\n",
    "        \"avg_sentence_length\": float(pd.concat([train_clean['sentence_cleaned'], test_clean['sentence_cleaned']]).str.len().mean()),\n",
    "        \"file_format\": \"clean (file, sentence_cleaned only)\"\n",
    "    },\n",
    "    \"split_strategy\": \"80% training, 20% testing\",\n",
    "    \"data_structure\": {\n",
    "        \"columns\": [\"file\", \"sentence_cleaned\"],\n",
    "        \"description\": \"Minimal structure for efficient ASR training\"\n",
    "    },\n",
    "    \"preprocessing_steps\": [\n",
    "        \"Text normalization and cleaning\",\n",
    "        \"Unicode normalization (NFC)\",\n",
    "        \"Minimal whitespace normalization\",\n",
    "        \"Control character removal\",\n",
    "        \"Empty sentence removal\",\n",
    "        \"Length filtering (2-500 characters)\",\n",
    "        \"Sinhala character ratio filtering (>=50%)\",\n",
    "        \"Duplicate removal\",\n",
    "        \"Column reduction to essentials only\"\n",
    "    ],\n",
    "    \"file_info\": {\n",
    "        \"audio_format\": \"flac\",\n",
    "        \"text_encoding\": \"utf-8\",\n",
    "        \"language\": \"Sinhala (si)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save clean metadata\n",
    "with open(f\"{output_dir}/metadata_clean.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(clean_metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"   • metadata_clean.json\")\n",
    "\n",
    "print(f\"\\n🎯 Summary of Clean Data Files:\")\n",
    "print(f\"   📄 CSV Files (essential columns only):\")\n",
    "print(f\"      • train_data_clean.csv ({len(train_clean):,} samples)\")\n",
    "print(f\"      • test_data_clean.csv ({len(test_clean):,} samples)\")\n",
    "print(f\"   📄 Manifest Files (JSONL format):\")\n",
    "print(f\"      • train_manifest_clean.jsonl\")\n",
    "print(f\"      • test_manifest_clean.jsonl\")\n",
    "print(f\"   📄 Metadata:\")\n",
    "print(f\"      • metadata_clean.json\")\n",
    "print(f\"\\n✨ Ready for efficient ASR training with minimal data footprint!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

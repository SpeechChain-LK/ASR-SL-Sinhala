{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc39fd2",
   "metadata": {},
   "source": [
    "# ğŸ¯ Sinhala ASR Model Tester\n",
    "\n",
    "This notebook is designed to load and test your trained Sinhala ASR model with any audio file.\n",
    "\n",
    "## ğŸ“‹ Requirements:\n",
    "- Trained model saved in `./sinhala-whisper-asr-final/` directory\n",
    "- Audio files to test (supports .wav, .flac, .mp3, etc.)\n",
    "- Required libraries: transformers, librosa, torch\n",
    "\n",
    "## ğŸš€ Features:\n",
    "- Load your trained Sinhala ASR model\n",
    "- Test single audio files\n",
    "- Test multiple audio files\n",
    "- Batch testing from dataset\n",
    "- Performance analysis and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6414e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# INSTALL REQUIRED PACKAGES\n",
    "# ================================\n",
    "\n",
    "# Uncomment if packages are not installed\n",
    "# %pip install transformers datasets librosa torch\n",
    "# %pip install soundfile numpy pandas\n",
    "# %pip install jiwer  # For evaluation metrics\n",
    "\n",
    "print(\"ğŸ“¦ All required packages should be installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4ed243c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Evaluation metrics available (WER, CER)\n",
      "âœ… All imports successful!\n",
      "ğŸ”¥ PyTorch version: 2.7.1+cpu\n",
      "ğŸ’» CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# IMPORTS\n",
    "# ================================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from transformers import (\n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperProcessor,\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperTokenizer\n",
    ")\n",
    "\n",
    "# Optional: For evaluation metrics\n",
    "try:\n",
    "    from jiwer import wer, cer\n",
    "    METRICS_AVAILABLE = True\n",
    "    print(\"ğŸ“Š Evaluation metrics available (WER, CER)\")\n",
    "except ImportError:\n",
    "    METRICS_AVAILABLE = False\n",
    "    print(\"âš ï¸ jiwer not available - metrics will be limited\")\n",
    "\n",
    "print(\"âœ… All imports successful!\")\n",
    "print(f\"ğŸ”¥ PyTorch version: {torch.__version__}\")\n",
    "print(f\"ğŸ’» CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ğŸ® GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3244363b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Configuration:\n",
      "   ğŸ“ Model Path: ./sinhala-whisper-asr-final\n",
      "   ğŸ”„ Fallback: openai/whisper-base\n",
      "   ğŸ“ˆ Sample Rate: 16000 Hz\n",
      "   â±ï¸ Max Length: 30 seconds\n",
      "   ğŸ’» Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# MODEL CONFIGURATION\n",
    "# ================================\n",
    "\n",
    "# Model paths - Update these if your model is saved elsewhere\n",
    "MODEL_PATH = \"./sinhala-whisper-asr-final\"  # Your trained model\n",
    "FALLBACK_MODEL = \"openai/whisper-base\"     # Fallback to base model if trained model not found\n",
    "\n",
    "# Audio settings\n",
    "SAMPLE_RATE = 16000\n",
    "MAX_AUDIO_LENGTH = 30  # seconds\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"ğŸ¯ Configuration:\")\n",
    "print(f\"   ğŸ“ Model Path: {MODEL_PATH}\")\n",
    "print(f\"   ğŸ”„ Fallback: {FALLBACK_MODEL}\")\n",
    "print(f\"   ğŸ“ˆ Sample Rate: {SAMPLE_RATE} Hz\")\n",
    "print(f\"   â±ï¸ Max Length: {MAX_AUDIO_LENGTH} seconds\")\n",
    "print(f\"   ğŸ’» Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d2fa49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Sinhala ASR Tester initialized!\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# MODEL LOADING CLASS\n",
    "# ================================\n",
    "\n",
    "class SinhalaASRTester:\n",
    "    def __init__(self, model_path=MODEL_PATH, fallback_model=FALLBACK_MODEL):\n",
    "        self.model_path = model_path\n",
    "        self.fallback_model = fallback_model\n",
    "        self.device = DEVICE\n",
    "        self.model = None\n",
    "        self.processor = None\n",
    "        self.model_type = None\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the trained model or fallback to base model\"\"\"\n",
    "        \n",
    "        print(f\"ğŸ”„ Loading Sinhala ASR model...\")\n",
    "        \n",
    "        # Try to load trained model first\n",
    "        if os.path.exists(self.model_path):\n",
    "            try:\n",
    "                print(f\"ğŸ“ Loading trained model from: {self.model_path}\")\n",
    "                self.model = WhisperForConditionalGeneration.from_pretrained(self.model_path)\n",
    "                self.processor = WhisperProcessor.from_pretrained(self.model_path)\n",
    "                self.model_type = \"trained\"\n",
    "                print(f\"âœ… Trained model loaded successfully!\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Error loading trained model: {e}\")\n",
    "                print(f\"ğŸ”„ Falling back to base model...\")\n",
    "                self._load_base_model()\n",
    "        else:\n",
    "            print(f\"ğŸ“ Trained model not found at: {self.model_path}\")\n",
    "            print(f\"ğŸ”„ Loading base model...\")\n",
    "            self._load_base_model()\n",
    "        \n",
    "        # Configure model\n",
    "        self.model.config.forced_decoder_ids = None\n",
    "        self.model.config.suppress_tokens = []\n",
    "        \n",
    "        # Move to device\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        print(f\"ğŸ¯ Model Configuration:\")\n",
    "        print(f\"   ğŸ¤– Model Type: {self.model_type}\")\n",
    "        print(f\"   ğŸ—ï¸ Architecture: {self.model.config.model_type}\")\n",
    "        print(f\"   ğŸŒ Language: Sinhala (si)\")\n",
    "        print(f\"   ğŸ’» Device: {self.device}\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _load_base_model(self):\n",
    "        \"\"\"Load the base Whisper model\"\"\"\n",
    "        try:\n",
    "            self.model = WhisperForConditionalGeneration.from_pretrained(self.fallback_model)\n",
    "            self.processor = WhisperProcessor.from_pretrained(\n",
    "                self.fallback_model, \n",
    "                language=\"si\", \n",
    "                task=\"transcribe\"\n",
    "            )\n",
    "            self.model_type = \"base\"\n",
    "            print(f\"âœ… Base model loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading base model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def preprocess_audio(self, audio_path):\n",
    "        \"\"\"Load and preprocess audio file\"\"\"\n",
    "        try:\n",
    "            # Load audio\n",
    "            audio_array, sr = librosa.load(audio_path, sr=SAMPLE_RATE)\n",
    "            \n",
    "            # Get duration\n",
    "            duration = len(audio_array) / SAMPLE_RATE\n",
    "            \n",
    "            # Trim if too long\n",
    "            max_samples = MAX_AUDIO_LENGTH * SAMPLE_RATE\n",
    "            if len(audio_array) > max_samples:\n",
    "                audio_array = audio_array[:max_samples]\n",
    "                print(f\"âš ï¸ Audio trimmed to {MAX_AUDIO_LENGTH} seconds\")\n",
    "            \n",
    "            # Normalize\n",
    "            max_val = np.max(np.abs(audio_array))\n",
    "            if max_val > 0:\n",
    "                audio_array = audio_array / max_val\n",
    "            \n",
    "            return audio_array, duration\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error preprocessing audio: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    def transcribe(self, audio_path, show_details=True):\n",
    "        \"\"\"Transcribe a single audio file\"\"\"\n",
    "        \n",
    "        if self.model is None:\n",
    "            print(f\"âŒ Model not loaded. Call load_model() first.\")\n",
    "            return None\n",
    "        \n",
    "        if not os.path.exists(audio_path):\n",
    "            print(f\"âŒ Audio file not found: {audio_path}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            if show_details:\n",
    "                print(f\"\\nğŸµ Processing: {os.path.basename(audio_path)}\")\n",
    "            \n",
    "            # Preprocess audio\n",
    "            audio_array, duration = self.preprocess_audio(audio_path)\n",
    "            if audio_array is None:\n",
    "                return None\n",
    "            \n",
    "            if show_details:\n",
    "                print(f\"ğŸ“Š Audio info: {duration:.2f}s, {len(audio_array):,} samples\")\n",
    "            \n",
    "            # Process with model\n",
    "            inputs = self.processor(audio_array, sampling_rate=SAMPLE_RATE, return_tensors=\"pt\")\n",
    "            input_features = inputs.input_features.to(self.device)\n",
    "            \n",
    "            # Generate transcription\n",
    "            with torch.no_grad():\n",
    "                # Clear forced decoder configuration before generation\n",
    "                original_forced_decoder_ids = self.model.config.forced_decoder_ids\n",
    "                self.model.config.forced_decoder_ids = None\n",
    "                \n",
    "                predicted_ids = self.model.generate(\n",
    "                    input_features,\n",
    "                    language=\"si\",\n",
    "                    task=\"transcribe\",\n",
    "                    max_length=448,\n",
    "                    num_beams=1,\n",
    "                    do_sample=False\n",
    "                )\n",
    "                \n",
    "                # Restore original configuration\n",
    "                self.model.config.forced_decoder_ids = original_forced_decoder_ids\n",
    "            \n",
    "            # Decode\n",
    "            transcription = self.processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "            \n",
    "            result = {\n",
    "                'file': os.path.basename(audio_path),\n",
    "                'path': audio_path,\n",
    "                'duration': duration,\n",
    "                'transcription': transcription,\n",
    "                'word_count': len(transcription.split()) if transcription.strip() else 0,\n",
    "                'char_count': len(transcription),\n",
    "                'model_type': self.model_type\n",
    "            }\n",
    "            \n",
    "            if show_details:\n",
    "                self._display_result(result)\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error during transcription: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _display_result(self, result):\n",
    "        \"\"\"Display transcription result\"\"\"\n",
    "        print(f\"\\n\" + \"=\" * 60)\n",
    "        print(f\"ğŸ¯ TRANSCRIPTION RESULT\")\n",
    "        print(f\"=\" * 60)\n",
    "        print(f\"ğŸµ File: {result['file']}\")\n",
    "        print(f\"â±ï¸ Duration: {result['duration']:.2f} seconds\")\n",
    "        print(f\"ğŸ¤– Model: {result['model_type']} Whisper\")\n",
    "        print(f\"ğŸ“ Transcription: '{result['transcription']}'\")\n",
    "        \n",
    "        if result['transcription'].strip():\n",
    "            speaking_rate = result['word_count'] / (result['duration'] / 60) if result['duration'] > 0 else 0\n",
    "            print(f\"ğŸ“Š Analysis:\")\n",
    "            print(f\"   ğŸ”¤ Words: {result['word_count']}\")\n",
    "            print(f\"   ğŸ“ Characters: {result['char_count']}\")\n",
    "            print(f\"   â±ï¸ Speaking rate: ~{speaking_rate:.1f} words/min\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ Empty transcription\")\n",
    "        \n",
    "        print(f\"=\" * 60)\n",
    "\n",
    "# Initialize the tester\n",
    "print(\"ğŸ¯ Sinhala ASR Tester initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed1934dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Loading Sinhala ASR model...\n",
      "ğŸ“ Loading trained model from: ./sinhala-whisper-asr-final\n",
      "âœ… Trained model loaded successfully!\n",
      "ğŸ¯ Model Configuration:\n",
      "   ğŸ¤– Model Type: trained\n",
      "   ğŸ—ï¸ Architecture: whisper\n",
      "   ğŸŒ Language: Sinhala (si)\n",
      "   ğŸ’» Device: cpu\n",
      "\n",
      "ğŸ‰ Ready to test audio files!\n",
      "ğŸ’¡ Use asr_tester.transcribe('your_audio_file.wav') to test\n",
      "âœ… Trained model loaded successfully!\n",
      "ğŸ¯ Model Configuration:\n",
      "   ğŸ¤– Model Type: trained\n",
      "   ğŸ—ï¸ Architecture: whisper\n",
      "   ğŸŒ Language: Sinhala (si)\n",
      "   ğŸ’» Device: cpu\n",
      "\n",
      "ğŸ‰ Ready to test audio files!\n",
      "ğŸ’¡ Use asr_tester.transcribe('your_audio_file.wav') to test\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# LOAD THE MODEL\n",
    "# ================================\n",
    "\n",
    "# Create tester instance\n",
    "asr_tester = SinhalaASRTester()\n",
    "\n",
    "# Load the model\n",
    "success = asr_tester.load_model()\n",
    "\n",
    "if success:\n",
    "    print(f\"\\nğŸ‰ Ready to test audio files!\")\n",
    "    print(f\"ğŸ’¡ Use asr_tester.transcribe('your_audio_file.wav') to test\")\n",
    "else:\n",
    "    print(f\"\\nâŒ Failed to load model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64718bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Testing audio file: test_audio.wav\n",
      "==================================================\n",
      "\n",
      "ğŸµ Processing: test_audio.wav\n",
      "ğŸ“Š Audio info: 2.78s, 44,446 samples\n",
      "âŒ Error during transcription: You have explicitly specified `forced_decoder_ids`. Please remove the `forced_decoder_ids` argument in favour of `input_ids` or `decoder_input_ids` respectively.\n",
      "\n",
      "âŒ Transcription failed\n",
      "ğŸ’¡ Make sure the audio file exists and is in a supported format\n",
      "âŒ Error during transcription: You have explicitly specified `forced_decoder_ids`. Please remove the `forced_decoder_ids` argument in favour of `input_ids` or `decoder_input_ids` respectively.\n",
      "\n",
      "âŒ Transcription failed\n",
      "ğŸ’¡ Make sure the audio file exists and is in a supported format\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# TEST SINGLE AUDIO FILE\n",
    "# ================================\n",
    "\n",
    "# Test your specific audio file\n",
    "audio_file = \"test_audio.wav\"  # Change this to your audio file path\n",
    "\n",
    "print(f\"ğŸ¯ Testing audio file: {audio_file}\")\n",
    "print(f\"=\" * 50)\n",
    "\n",
    "result = asr_tester.transcribe(audio_file)\n",
    "\n",
    "if result:\n",
    "    print(f\"\\nâœ… Transcription completed!\")\n",
    "else:\n",
    "    print(f\"\\nâŒ Transcription failed\")\n",
    "    print(f\"ğŸ’¡ Make sure the audio file exists and is in a supported format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf430b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Simple Audio Test\n",
      "==================================================\n",
      "ğŸµ Loading audio: test_audio.wav\n",
      "ğŸ“Š Audio: 2.78s, 44,446 samples\n",
      "ğŸ”„ Generating transcription...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'suppress_tokens': [], 'begin_suppress_tokens': [220, 50257]}. If this is not desired, please set these values explicitly.\n",
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> to see related `.generate()` flags.\n",
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> to see related `.generate()` flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ¯ TRANSCRIPTION RESULT\n",
      "============================================================\n",
      "ğŸµ File: test_audio.wav\n",
      "â±ï¸ Duration: 2.78 seconds\n",
      "ğŸ¤– Model: Trained Sinhala Whisper\n",
      "ğŸ“ Transcription: 'à¶…à¶¸à·Šï¿½, à¶¶à¶œà·Š'\n",
      "============================================================\n",
      "ğŸ“Š Analysis:\n",
      "   ğŸ”¤ Words: 2\n",
      "   ğŸ“ Characters: 9\n",
      "   â±ï¸ Speaking rate: ~43.2 words/min\n",
      "\n",
      "âœ… Transcription successful!\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# SIMPLE AUDIO TEST (Fixed Version)\n",
    "# ================================\n",
    "\n",
    "def simple_transcribe_test(audio_file):\n",
    "    \"\"\"Simple transcription test without forced_decoder_ids issues\"\"\"\n",
    "    \n",
    "    if not os.path.exists(audio_file):\n",
    "        print(f\"âŒ Audio file not found: {audio_file}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        print(f\"ğŸµ Loading audio: {audio_file}\")\n",
    "        \n",
    "        # Load and preprocess audio\n",
    "        audio_array, sr = librosa.load(audio_file, sr=16000)\n",
    "        duration = len(audio_array) / 16000\n",
    "        \n",
    "        print(f\"ğŸ“Š Audio: {duration:.2f}s, {len(audio_array):,} samples\")\n",
    "        \n",
    "        # Normalize audio\n",
    "        max_val = np.max(np.abs(audio_array))\n",
    "        if max_val > 0:\n",
    "            audio_array = audio_array / max_val\n",
    "        \n",
    "        # Process with model\n",
    "        inputs = asr_tester.processor(audio_array, sampling_rate=16000, return_tensors=\"pt\")\n",
    "        input_features = inputs.input_features.to(asr_tester.device)\n",
    "        \n",
    "        print(f\"ğŸ”„ Generating transcription...\")\n",
    "        \n",
    "        # Generate without problematic parameters\n",
    "        with torch.no_grad():\n",
    "            # Clear configuration completely\n",
    "            asr_tester.model.generation_config.forced_decoder_ids = None\n",
    "            asr_tester.model.config.forced_decoder_ids = None\n",
    "            \n",
    "            predicted_ids = asr_tester.model.generate(\n",
    "                input_features,\n",
    "                max_length=448,\n",
    "                num_beams=1,\n",
    "                do_sample=False\n",
    "            )\n",
    "        \n",
    "        # Decode transcription\n",
    "        transcription = asr_tester.processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\n\" + \"=\" * 60)\n",
    "        print(f\"ğŸ¯ TRANSCRIPTION RESULT\")\n",
    "        print(f\"=\" * 60)\n",
    "        print(f\"ğŸµ File: {audio_file}\")\n",
    "        print(f\"â±ï¸ Duration: {duration:.2f} seconds\")\n",
    "        print(f\"ğŸ¤– Model: Trained Sinhala Whisper\")\n",
    "        print(f\"ğŸ“ Transcription: '{transcription}'\")\n",
    "        print(f\"=\" * 60)\n",
    "        \n",
    "        if transcription.strip():\n",
    "            words = len(transcription.split())\n",
    "            chars = len(transcription)\n",
    "            speaking_rate = words / (duration / 60) if duration > 0 else 0\n",
    "            print(f\"ğŸ“Š Analysis:\")\n",
    "            print(f\"   ğŸ”¤ Words: {words}\")\n",
    "            print(f\"   ğŸ“ Characters: {chars}\")\n",
    "            print(f\"   â±ï¸ Speaking rate: ~{speaking_rate:.1f} words/min\")\n",
    "        \n",
    "        return {\n",
    "            'file': audio_file,\n",
    "            'duration': duration,\n",
    "            'transcription': transcription,\n",
    "            'word_count': len(transcription.split()),\n",
    "            'char_count': len(transcription)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Test the audio file\n",
    "print(f\"ğŸ¯ Simple Audio Test\")\n",
    "print(f\"=\" * 50)\n",
    "result = simple_transcribe_test(\"test_audio.wav\")\n",
    "\n",
    "if result:\n",
    "    print(f\"\\nâœ… Transcription successful!\")\n",
    "else:\n",
    "    print(f\"\\nâŒ Transcription failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b4cd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# TEST MULTIPLE AUDIO FILES\n",
    "# ================================\n",
    "\n",
    "def test_multiple_files(file_list, show_individual=True):\n",
    "    \"\"\"Test multiple audio files\"\"\"\n",
    "    \n",
    "    print(f\"ğŸ¯ Testing {len(file_list)} audio files...\")\n",
    "    print(f\"=\" * 50)\n",
    "    \n",
    "    results = []\n",
    "    successful = 0\n",
    "    \n",
    "    for i, audio_file in enumerate(file_list, 1):\n",
    "        print(f\"\\nğŸ“ [{i}/{len(file_list)}] Processing: {os.path.basename(audio_file)}\")\n",
    "        \n",
    "        result = asr_tester.transcribe(audio_file, show_details=show_individual)\n",
    "        \n",
    "        if result:\n",
    "            results.append(result)\n",
    "            successful += 1\n",
    "            if not show_individual:\n",
    "                print(f\"   âœ… Success: '{result['transcription'][:50]}...'\")\n",
    "        else:\n",
    "            print(f\"   âŒ Failed\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n\" + \"=\" * 60)\n",
    "    print(f\"ğŸ“Š BATCH TESTING SUMMARY\")\n",
    "    print(f\"=\" * 60)\n",
    "    print(f\"ğŸ“ Total files: {len(file_list)}\")\n",
    "    print(f\"âœ… Successful: {successful}\")\n",
    "    print(f\"âŒ Failed: {len(file_list) - successful}\")\n",
    "    print(f\"ğŸ“ˆ Success rate: {successful/len(file_list)*100:.1f}%\")\n",
    "    \n",
    "    if results:\n",
    "        total_duration = sum(r['duration'] for r in results)\n",
    "        total_words = sum(r['word_count'] for r in results)\n",
    "        avg_speaking_rate = total_words / (total_duration / 60) if total_duration > 0 else 0\n",
    "        \n",
    "        print(f\"â±ï¸ Total audio: {total_duration:.1f} seconds\")\n",
    "        print(f\"ğŸ”¤ Total words: {total_words}\")\n",
    "        print(f\"â±ï¸ Avg speaking rate: {avg_speaking_rate:.1f} words/min\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example: Test multiple files from dataset\n",
    "# Uncomment and modify paths as needed\n",
    "\n",
    "# Sample FLAC files from your dataset\n",
    "sample_files = [\n",
    "    \"asr_sinhala/data/00/0000f47c22.flac\",\n",
    "    \"asr_sinhala/data/00/000101700f.flac\",\n",
    "    \"asr_sinhala/data/00/000107b539.flac\"\n",
    "]\n",
    "\n",
    "# Filter existing files\n",
    "existing_files = [f for f in sample_files if os.path.exists(f)]\n",
    "\n",
    "if existing_files:\n",
    "    print(f\"Found {len(existing_files)} sample files to test\")\n",
    "    batch_results = test_multiple_files(existing_files, show_individual=False)\n",
    "else:\n",
    "    print(f\"No sample audio files found. Update the paths above to test multiple files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb9af8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# TEST FROM CSV DATASET\n",
    "# ================================\n",
    "\n",
    "def test_from_csv(csv_path, audio_base_path=\"asr_sinhala/data\", max_samples=5):\n",
    "    \"\"\"Test audio files from CSV dataset with ground truth comparison\"\"\"\n",
    "    \n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"âŒ CSV file not found: {csv_path}\")\n",
    "        return\n",
    "    \n",
    "    # Load CSV\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"ğŸ“Š Loaded CSV with {len(df)} samples\")\n",
    "        \n",
    "        # Take sample\n",
    "        sample_df = df.head(max_samples)\n",
    "        \n",
    "        print(f\"ğŸ¯ Testing {len(sample_df)} samples from dataset...\")\n",
    "        print(f\"=\" * 60)\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for idx, row in sample_df.iterrows():\n",
    "            audio_path = os.path.join(audio_base_path, row.iloc[0])  # First column is audio path\n",
    "            ground_truth = row.iloc[1] if len(row) > 1 else \"N/A\"    # Second column is text\n",
    "            \n",
    "            print(f\"\\nğŸ“ [{idx+1}/{len(sample_df)}] {os.path.basename(audio_path)}\")\n",
    "            print(f\"ğŸ“ Ground truth: '{ground_truth}'\")\n",
    "            \n",
    "            if os.path.exists(audio_path):\n",
    "                result = asr_tester.transcribe(audio_path, show_details=False)\n",
    "                \n",
    "                if result:\n",
    "                    result['ground_truth'] = ground_truth\n",
    "                    results.append(result)\n",
    "                    \n",
    "                    print(f\"ğŸ¤– Prediction: '{result['transcription']}'\")\n",
    "                    \n",
    "                    # Calculate metrics if available\n",
    "                    if METRICS_AVAILABLE and ground_truth != \"N/A\":\n",
    "                        try:\n",
    "                            wer_score = wer(ground_truth, result['transcription']) * 100\n",
    "                            cer_score = cer(ground_truth, result['transcription']) * 100\n",
    "                            print(f\"ğŸ“Š WER: {wer_score:.1f}%, CER: {cer_score:.1f}%\")\n",
    "                            result['wer'] = wer_score\n",
    "                            result['cer'] = cer_score\n",
    "                        except:\n",
    "                            print(f\"ğŸ“Š Metrics calculation failed\")\n",
    "                    \n",
    "                    print(f\"âœ… Success\")\n",
    "                else:\n",
    "                    print(f\"âŒ Transcription failed\")\n",
    "            else:\n",
    "                print(f\"âŒ Audio file not found: {audio_path}\")\n",
    "        \n",
    "        # Summary\n",
    "        if results:\n",
    "            print(f\"\\n\" + \"=\" * 60)\n",
    "            print(f\"ğŸ“Š DATASET TESTING SUMMARY\")\n",
    "            print(f\"=\" * 60)\n",
    "            print(f\"âœ… Successful transcriptions: {len(results)}/{len(sample_df)}\")\n",
    "            \n",
    "            if METRICS_AVAILABLE and any('wer' in r for r in results):\n",
    "                avg_wer = np.mean([r['wer'] for r in results if 'wer' in r])\n",
    "                avg_cer = np.mean([r['cer'] for r in results if 'cer' in r])\n",
    "                print(f\"ğŸ“ˆ Average WER: {avg_wer:.1f}%\")\n",
    "                print(f\"ğŸ“ˆ Average CER: {avg_cer:.1f}%\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing CSV: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test with your CSV files\n",
    "csv_files = [\"test.csv\", \"train.csv\"]\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    if os.path.exists(csv_file):\n",
    "        print(f\"\\nğŸ¯ Testing from {csv_file}...\")\n",
    "        csv_results = test_from_csv(csv_file, max_samples=3)\n",
    "        break\n",
    "else:\n",
    "    print(f\"No CSV files found for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1127f41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# CUSTOM AUDIO TEST\n",
    "# ================================\n",
    "\n",
    "def test_custom_audio():\n",
    "    \"\"\"Interactive function to test any audio file\"\"\"\n",
    "    \n",
    "    print(f\"ğŸ¯ Custom Audio Testing\")\n",
    "    print(f\"=\" * 40)\n",
    "    print(f\"Enter the path to your audio file, or 'quit' to exit\")\n",
    "    print(f\"Supported formats: .wav, .flac, .mp3, .m4a, .ogg\")\n",
    "    \n",
    "    while True:\n",
    "        audio_path = input(f\"\\nğŸµ Audio file path: \").strip()\n",
    "        \n",
    "        if audio_path.lower() in ['quit', 'exit', 'q']:\n",
    "            print(f\"ğŸ‘‹ Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if not audio_path:\n",
    "            continue\n",
    "        \n",
    "        # Remove quotes if present\n",
    "        audio_path = audio_path.strip('\"\\'')\n",
    "        \n",
    "        result = asr_tester.transcribe(audio_path)\n",
    "        \n",
    "        if result:\n",
    "            print(f\"\\nâœ… Transcription successful!\")\n",
    "        else:\n",
    "            print(f\"\\nâŒ Transcription failed\")\n",
    "\n",
    "# Uncomment to run interactive testing\n",
    "# test_custom_audio()\n",
    "\n",
    "print(f\"ğŸ’¡ Uncomment the line above to run interactive audio testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd50cc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# SAVE RESULTS TO FILE\n",
    "# ================================\n",
    "\n",
    "def save_results_to_csv(results, filename=\"transcription_results.csv\"):\n",
    "    \"\"\"Save transcription results to CSV file\"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        print(f\"âŒ No results to save\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Convert results to DataFrame\n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        # Save to CSV\n",
    "        df.to_csv(filename, index=False)\n",
    "        \n",
    "        print(f\"ğŸ’¾ Results saved to: {filename}\")\n",
    "        print(f\"ğŸ“Š Saved {len(results)} transcription results\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error saving results: {e}\")\n",
    "\n",
    "# Example: Save results if you have any\n",
    "# save_results_to_csv(batch_results, \"my_transcription_results.csv\")\n",
    "\n",
    "print(f\"ğŸ’¡ Use save_results_to_csv(your_results, 'filename.csv') to save results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f669b0",
   "metadata": {},
   "source": [
    "## ğŸ¯ Quick Usage Guide\n",
    "\n",
    "### Single File Testing:\n",
    "```python\n",
    "result = asr_tester.transcribe(\"your_audio.wav\")\n",
    "```\n",
    "\n",
    "### Multiple Files Testing:\n",
    "```python\n",
    "files = [\"audio1.wav\", \"audio2.wav\", \"audio3.wav\"]\n",
    "results = test_multiple_files(files)\n",
    "```\n",
    "\n",
    "### Dataset Testing:\n",
    "```python\n",
    "results = test_from_csv(\"your_dataset.csv\", \"path/to/audio/files\")\n",
    "```\n",
    "\n",
    "### Save Results:\n",
    "```python\n",
    "save_results_to_csv(results, \"transcription_results.csv\")\n",
    "```\n",
    "\n",
    "## ğŸ“ Notes:\n",
    "- Make sure your trained model is in `./sinhala-whisper-asr-final/`\n",
    "- If trained model is not found, it will use the base Whisper model\n",
    "- Supports various audio formats: WAV, FLAC, MP3, M4A, OGG\n",
    "- Audio longer than 30 seconds will be automatically trimmed\n",
    "- Install `jiwer` package for WER/CER evaluation metrics\n",
    "\n",
    "## ğŸš€ Happy Testing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb5df9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 ASR Data Combination Tool\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# COMBINE TRAIN AND TEST DATA CLEAN FILES\n",
    "# Final Data Combination for Sinhala ASR Dataset\n",
    "# ================================\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"🔗 ASR Data Combination Tool\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "070ef68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Configuration:\n",
      "   📊 Train file: processed_asr_data/train_data_clean.csv\n",
      "   📊 Test file: processed_asr_data/test_data_clean.csv\n",
      "   📁 Output directory: processed_asr_data\n",
      "   💾 Output file: processed_asr_data\\combined_asr_data.csv\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# FILE PATHS CONFIGURATION\n",
    "# ================================\n",
    "\n",
    "# Define input and output paths\n",
    "train_file = \"processed_asr_data/train_data_clean.csv\"\n",
    "test_file = \"processed_asr_data/test_data_clean.csv\"\n",
    "output_dir = \"processed_asr_data\"\n",
    "output_file = os.path.join(output_dir, \"combined_asr_data.csv\")\n",
    "\n",
    "print(f\"📁 Configuration:\")\n",
    "print(f\"   📊 Train file: {train_file}\")\n",
    "print(f\"   📊 Test file: {test_file}\")\n",
    "print(f\"   📁 Output directory: {output_dir}\")\n",
    "print(f\"   💾 Output file: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c464a858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Loading data files...\n",
      "✅ Train data loaded: 72,155 samples\n",
      "✅ Test data loaded: 18,039 samples\n",
      "✅ Train data loaded: 72,155 samples\n",
      "✅ Test data loaded: 18,039 samples\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# DATA LOADING AND VALIDATION\n",
    "# ================================\n",
    "\n",
    "def load_and_validate_files(train_path, test_path):\n",
    "    \"\"\"Load and validate input files\"\"\"\n",
    "    \n",
    "    print(\"\\n📊 Loading data files...\")\n",
    "    \n",
    "    # Check if files exist\n",
    "    if not os.path.exists(train_path):\n",
    "        raise FileNotFoundError(f\"❌ Train file not found: {train_path}\")\n",
    "    if not os.path.exists(test_path):\n",
    "        raise FileNotFoundError(f\"❌ Test file not found: {test_path}\")\n",
    "    \n",
    "    # Load the files\n",
    "    try:\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "        \n",
    "        print(f\"✅ Train data loaded: {len(train_df):,} samples\")\n",
    "        print(f\"✅ Test data loaded: {len(test_df):,} samples\")\n",
    "        \n",
    "        return train_df, test_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"❌ Error loading files: {e}\")\n",
    "\n",
    "# Load the data\n",
    "train_df, test_df = load_and_validate_files(train_file, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1a57313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Data Structure Analysis:\n",
      "========================================\n",
      "📋 Train Data Structure:\n",
      "   Shape: (72155, 2)\n",
      "   Columns: ['file', 'sentence_cleaned']\n",
      "   Data types: {'file': dtype('O'), 'sentence_cleaned': dtype('O')}\n",
      "\n",
      "📋 Test Data Structure:\n",
      "   Shape: (18039, 2)\n",
      "   Columns: ['file', 'sentence_cleaned']\n",
      "   Data types: {'file': dtype('O'), 'sentence_cleaned': dtype('O')}\n",
      "\n",
      "🔍 Column Compatibility Check:\n",
      "✅ Column names match perfectly\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# DATA STRUCTURE ANALYSIS\n",
    "# ================================\n",
    "\n",
    "print(f\"\\n🔍 Data Structure Analysis:\")\n",
    "print(f\"=\" * 40)\n",
    "\n",
    "print(f\"📋 Train Data Structure:\")\n",
    "print(f\"   Shape: {train_df.shape}\")\n",
    "print(f\"   Columns: {list(train_df.columns)}\")\n",
    "print(f\"   Data types: {train_df.dtypes.to_dict()}\")\n",
    "\n",
    "print(f\"\\n📋 Test Data Structure:\")\n",
    "print(f\"   Shape: {test_df.shape}\")\n",
    "print(f\"   Columns: {list(test_df.columns)}\")\n",
    "print(f\"   Data types: {test_df.dtypes.to_dict()}\")\n",
    "\n",
    "# Check column compatibility\n",
    "print(f\"\\n🔍 Column Compatibility Check:\")\n",
    "train_cols = set(train_df.columns)\n",
    "test_cols = set(test_df.columns)\n",
    "\n",
    "if train_cols == test_cols:\n",
    "    print(\"✅ Column names match perfectly\")\n",
    "    common_cols = list(train_df.columns)\n",
    "else:\n",
    "    print(\"⚠️ Column mismatch detected\")\n",
    "    common_cols = list(train_cols & test_cols)\n",
    "    train_only = train_cols - test_cols\n",
    "    test_only = test_cols - train_cols\n",
    "    \n",
    "    print(f\"   🔗 Common columns: {common_cols}\")\n",
    "    if train_only:\n",
    "        print(f\"   🏋️ Train-only columns: {list(train_only)}\")\n",
    "    if test_only:\n",
    "        print(f\"   🧪 Test-only columns: {list(test_only)}\")\n",
    "    \n",
    "    print(f\"   💡 Will use common columns only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "695bc67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Sample Data Preview:\n",
      "==============================\n",
      "\n",
      "🏋️ Train Data Sample:\n",
      "                                  file  \\\n",
      "0  asr_sinhala/data/98/983e3c6613.flac   \n",
      "1  asr_sinhala/data/29/29ab15c6d4.flac   \n",
      "2  asr_sinhala/data/b0/b0072f9ac0.flac   \n",
      "\n",
      "                                sentence_cleaned  \n",
      "0   එය මිලිටරිය තුළ ති‍යන ප්‍රධානම ප්‍රතිමානයක්.  \n",
      "1  සාහිත්‍යකරුවාට ඊට වැඩිය ලොකු වගකීමක් තියෙනවා.  \n",
      "2                        ඕගොල්ලන්ට දකින්න ලැබෙයි  \n",
      "\n",
      "🧪 Test Data Sample:\n",
      "                                  file  \\\n",
      "0  asr_sinhala/data/b6/b63bfb7a34.flac   \n",
      "1  asr_sinhala/data/f1/f134378295.flac   \n",
      "2  asr_sinhala/data/d3/d3bf1e10b0.flac   \n",
      "\n",
      "                                  sentence_cleaned  \n",
      "0                                මෙකී සිව් මහ ධාතු  \n",
      "1                           එදිනෙදා ජීවිතයේ සිදුවන  \n",
      "2  අධිවේගී මාර්ගයේ මැද සේවා ස්ථානයක් ඉදි කළ පමණින්  \n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# SAMPLE DATA PREVIEW\n",
    "# ================================\n",
    "\n",
    "print(f\"\\n📝 Sample Data Preview:\")\n",
    "print(f\"=\" * 30)\n",
    "\n",
    "print(f\"\\n🏋️ Train Data Sample:\")\n",
    "print(train_df.head(3))\n",
    "\n",
    "print(f\"\\n🧪 Test Data Sample:\")\n",
    "print(test_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32a455ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔗 Preparing data for combination...\n",
      "ℹ️ Skipping 'data_split' column - combining without split indicator\n",
      "🔗 Combining datasets...\n",
      "📊 Combination Results:\n",
      "   🏋️ Train samples: 72,155\n",
      "   🧪 Test samples: 18,039\n",
      "   📈 Combined total: 90,194\n",
      "   📋 Final columns: ['sentence_cleaned', 'file']\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# DATA PREPARATION AND COMBINATION\n",
    "# ================================\n",
    "\n",
    "def prepare_and_combine_data(train_data, test_data, include_split_column=False):\n",
    "    \"\"\"Prepare and combine train and test data\"\"\"\n",
    "    \n",
    "    print(f\"\\n🔗 Preparing data for combination...\")\n",
    "    \n",
    "    # Use common columns only\n",
    "    train_cols = set(train_data.columns)\n",
    "    test_cols = set(test_data.columns)\n",
    "    common_cols = list(train_cols & test_cols)\n",
    "    \n",
    "    train_prepared = train_data[common_cols].copy()\n",
    "    test_prepared = test_data[common_cols].copy()\n",
    "    \n",
    "    # Add split indicator column (disabled by default)\n",
    "    if include_split_column:\n",
    "        train_prepared['data_split'] = 'train'\n",
    "        test_prepared['data_split'] = 'test'\n",
    "        print(\"✅ Added 'data_split' column\")\n",
    "    else:\n",
    "        print(\"ℹ️ Skipping 'data_split' column - combining without split indicator\")\n",
    "    \n",
    "    # Combine datasets\n",
    "    print(\"🔗 Combining datasets...\")\n",
    "    combined_df = pd.concat([train_prepared, test_prepared], ignore_index=True)\n",
    "    \n",
    "    print(f\"📊 Combination Results:\")\n",
    "    print(f\"   🏋️ Train samples: {len(train_prepared):,}\")\n",
    "    print(f\"   🧪 Test samples: {len(test_prepared):,}\")\n",
    "    print(f\"   📈 Combined total: {len(combined_df):,}\")\n",
    "    print(f\"   📋 Final columns: {list(combined_df.columns)}\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Combine the data WITHOUT split column\n",
    "combined_df = prepare_and_combine_data(train_df, test_df, include_split_column=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16501cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Data Quality Assessment:\n",
      "===================================\n",
      "🔄 Duplicate Analysis:\n",
      "   Total rows: 90,194\n",
      "   Duplicate rows: 0\n",
      "   ✅ No duplicates found\n",
      "\n",
      "🔍 Missing Value Analysis:\n",
      "   ✅ No missing values found\n",
      "\n",
      "📊 Combined Dataset:\n",
      "   🔗 Total samples: 90,194 (train + test combined)\n",
      "   ℹ️ No split indicator column included\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# DATA QUALITY CHECKS\n",
    "# ================================\n",
    "\n",
    "print(f\"\\n🔍 Data Quality Assessment:\")\n",
    "print(f\"=\" * 35)\n",
    "\n",
    "# Check for duplicates\n",
    "initial_count = len(combined_df)\n",
    "duplicates = combined_df.duplicated().sum()\n",
    "print(f\"🔄 Duplicate Analysis:\")\n",
    "print(f\"   Total rows: {initial_count:,}\")\n",
    "print(f\"   Duplicate rows: {duplicates:,}\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    print(f\"   🧹 Removing {duplicates:,} duplicates...\")\n",
    "    combined_df = combined_df.drop_duplicates()\n",
    "    final_count = len(combined_df)\n",
    "    print(f\"   ✅ After deduplication: {final_count:,} samples\")\n",
    "    print(f\"   📉 Removed: {initial_count - final_count:,} duplicates\")\n",
    "else:\n",
    "    print(f\"   ✅ No duplicates found\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\n🔍 Missing Value Analysis:\")\n",
    "missing_data = combined_df.isnull().sum()\n",
    "total_missing = missing_data.sum()\n",
    "\n",
    "if total_missing > 0:\n",
    "    print(f\"   ⚠️ Missing values detected:\")\n",
    "    for col, count in missing_data.items():\n",
    "        if count > 0:\n",
    "            percentage = (count / len(combined_df)) * 100\n",
    "            print(f\"      📊 {col}: {count:,} missing ({percentage:.1f}%)\")\n",
    "else:\n",
    "    print(f\"   ✅ No missing values found\")\n",
    "\n",
    "# Data split distribution (only if column exists)\n",
    "if 'data_split' in combined_df.columns:\n",
    "    print(f\"\\n📊 Data Split Distribution:\")\n",
    "    split_counts = combined_df['data_split'].value_counts()\n",
    "    for split, count in split_counts.items():\n",
    "        percentage = (count / len(combined_df)) * 100\n",
    "        print(f\"   {split.title()}: {count:,} samples ({percentage:.1f}%)\")\n",
    "else:\n",
    "    print(f\"\\n📊 Combined Dataset:\")\n",
    "    print(f\"   🔗 Total samples: {len(combined_df):,} (train + test combined)\")\n",
    "    print(f\"   ℹ️ No split indicator column included\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9442e3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Final Combined Dataset Preview:\n",
      "========================================\n",
      "📊 Dataset Shape: (90194, 2)\n",
      "📋 Columns: ['sentence_cleaned', 'file']\n",
      "\n",
      "🔍 Sample Combined Data:\n",
      "                                sentence_cleaned  \\\n",
      "0   එය මිලිටරිය තුළ ති‍යන ප්‍රධානම ප්‍රතිමානයක්.   \n",
      "1  සාහිත්‍යකරුවාට ඊට වැඩිය ලොකු වගකීමක් තියෙනවා.   \n",
      "2                        ඕගොල්ලන්ට දකින්න ලැබෙයි   \n",
      "3                     බොදු බල සේනා හිමිවරුන් අතර   \n",
      "4                            ඔබතුමා දන්නා විදියට   \n",
      "\n",
      "                                  file  \n",
      "0  asr_sinhala/data/98/983e3c6613.flac  \n",
      "1  asr_sinhala/data/29/29ab15c6d4.flac  \n",
      "2  asr_sinhala/data/b0/b0072f9ac0.flac  \n",
      "3  asr_sinhala/data/85/85fc577f7b.flac  \n",
      "4  asr_sinhala/data/7e/7e6bd7795f.flac  \n",
      "\n",
      "📈 Dataset Statistics:\n",
      "   📊 Total samples: 90,194\n",
      "   📋 Features: 2\n",
      "   💾 Memory usage: 23.81 MB\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# FINAL COMBINED DATA PREVIEW\n",
    "# ================================\n",
    "\n",
    "print(f\"\\n📝 Final Combined Dataset Preview:\")\n",
    "print(f\"=\" * 40)\n",
    "\n",
    "print(f\"📊 Dataset Shape: {combined_df.shape}\")\n",
    "print(f\"📋 Columns: {list(combined_df.columns)}\")\n",
    "\n",
    "print(f\"\\n🔍 Sample Combined Data:\")\n",
    "print(combined_df.head(5))\n",
    "\n",
    "print(f\"\\n📈 Dataset Statistics:\")\n",
    "print(f\"   📊 Total samples: {len(combined_df):,}\")\n",
    "print(f\"   📋 Features: {len(combined_df.columns)}\")\n",
    "print(f\"   💾 Memory usage: {combined_df.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd572135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 Saving combined data...\n",
      "✅ Data saved successfully!\n",
      "📁 File location: processed_asr_data\\combined_asr_data.csv\n",
      "📊 File size: 9.50 MB\n",
      "📈 Rows saved: 90,194\n",
      "✅ Data saved successfully!\n",
      "📁 File location: processed_asr_data\\combined_asr_data.csv\n",
      "📊 File size: 9.50 MB\n",
      "📈 Rows saved: 90,194\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# SAVE COMBINED DATA\n",
    "# ================================\n",
    "\n",
    "def save_combined_data(data, output_path):\n",
    "    \"\"\"Save combined data to file\"\"\"\n",
    "    \n",
    "    print(f\"\\n💾 Saving combined data...\")\n",
    "    \n",
    "    # Create output directory if needed\n",
    "    output_directory = os.path.dirname(output_path)\n",
    "    if output_directory and not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        print(f\"📁 Created output directory: {output_directory}\")\n",
    "    \n",
    "    try:\n",
    "        # Save to CSV\n",
    "        data.to_csv(output_path, index=False)\n",
    "        \n",
    "        # Verify saved file\n",
    "        file_size = os.path.getsize(output_path) / (1024**2)  # MB\n",
    "        \n",
    "        print(f\"✅ Data saved successfully!\")\n",
    "        print(f\"📁 File location: {output_path}\")\n",
    "        print(f\"📊 File size: {file_size:.2f} MB\")\n",
    "        print(f\"📈 Rows saved: {len(data):,}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving file: {e}\")\n",
    "        return False\n",
    "\n",
    "# Save the combined data\n",
    "save_success = save_combined_data(combined_df, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a218d1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📄 Creating metadata file...\n",
      "✅ Metadata file created: processed_asr_data\\dataset_metadata.txt\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# CREATE METADATA FILE\n",
    "# ================================\n",
    "\n",
    "def create_metadata_file(data, output_directory):\n",
    "    \"\"\"Create detailed metadata file\"\"\"\n",
    "    \n",
    "    metadata_path = os.path.join(output_directory, \"dataset_metadata.txt\")\n",
    "    \n",
    "    print(f\"\\n📄 Creating metadata file...\")\n",
    "    \n",
    "    try:\n",
    "        with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"COMBINED SINHALA ASR DATASET METADATA\\n\")\n",
    "            f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "            \n",
    "            # Basic information\n",
    "            f.write(f\"Creation Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"Total Samples: {len(data):,}\\n\")\n",
    "            f.write(f\"Total Columns: {len(data.columns)}\\n\")\n",
    "            f.write(f\"Dataset Type: Combined (train + test without split indicator)\\n\\n\")\n",
    "            \n",
    "            # Column information\n",
    "            f.write(\"COLUMN INFORMATION:\\n\")\n",
    "            f.write(\"-\" * 30 + \"\\n\")\n",
    "            for i, col in enumerate(data.columns, 1):\n",
    "                dtype = str(data[col].dtype)\n",
    "                non_null = data[col].count()\n",
    "                f.write(f\"{i:2d}. {col:<20} | Type: {dtype:<10} | Non-null: {non_null:,}\\n\")\n",
    "            \n",
    "            # Data split information (only if column exists)\n",
    "            if 'data_split' in data.columns:\n",
    "                f.write(f\"\\nDATA SPLIT DISTRIBUTION:\\n\")\n",
    "                f.write(\"-\" * 30 + \"\\n\")\n",
    "                split_counts = data['data_split'].value_counts()\n",
    "                for split, count in split_counts.items():\n",
    "                    percentage = (count / len(data)) * 100\n",
    "                    f.write(f\"{split.title():<10}: {count:>8,} samples ({percentage:5.1f}%)\\n\")\n",
    "            else:\n",
    "                f.write(f\"\\nDATASET STRUCTURE:\\n\")\n",
    "                f.write(\"-\" * 30 + \"\\n\")\n",
    "                f.write(f\"Combined Dataset: {len(data):,} total samples\\n\")\n",
    "                f.write(f\"Split Indicator: Not included (unified dataset)\\n\")\n",
    "                f.write(f\"Source: train_data_clean.csv + test_data_clean.csv\\n\")\n",
    "            \n",
    "            # Missing values\n",
    "            f.write(f\"\\nMISSING VALUES SUMMARY:\\n\")\n",
    "            f.write(\"-\" * 30 + \"\\n\")\n",
    "            missing_summary = data.isnull().sum()\n",
    "            for col, missing_count in missing_summary.items():\n",
    "                if missing_count > 0:\n",
    "                    percentage = (missing_count / len(data)) * 100\n",
    "                    f.write(f\"{col:<20}: {missing_count:>6,} missing ({percentage:5.1f}%)\\n\")\n",
    "            if missing_summary.sum() == 0:\n",
    "                f.write(\"No missing values found.\\n\")\n",
    "            \n",
    "            # Sample data\n",
    "            f.write(f\"\\nSAMPLE DATA (First 5 rows):\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "            f.write(data.head(5).to_string())\n",
    "            \n",
    "            f.write(f\"\\n\\nEND OF METADATA\\n\")\n",
    "        \n",
    "        print(f\"✅ Metadata file created: {metadata_path}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating metadata: {e}\")\n",
    "        return False\n",
    "\n",
    "# Create metadata file\n",
    "if save_success:\n",
    "    create_metadata_file(combined_df, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f19226a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Validating output file...\n",
      "✅ File validation successful!\n",
      "   📊 Loaded shape: (90194, 2)\n",
      "   📋 Columns match: True\n",
      "   🔢 Row count match: True\n",
      "✅ File validation successful!\n",
      "   📊 Loaded shape: (90194, 2)\n",
      "   📋 Columns match: True\n",
      "   🔢 Row count match: True\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# VALIDATION AND SUMMARY\n",
    "# ================================\n",
    "\n",
    "def validate_output_file(file_path):\n",
    "    \"\"\"Validate the saved output file\"\"\"\n",
    "    \n",
    "    print(f\"\\n🔍 Validating output file...\")\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"❌ Output file not found: {file_path}\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Load and verify\n",
    "        validation_df = pd.read_csv(file_path)\n",
    "        \n",
    "        print(f\"✅ File validation successful!\")\n",
    "        print(f\"   📊 Loaded shape: {validation_df.shape}\")\n",
    "        print(f\"   📋 Columns match: {list(validation_df.columns) == list(combined_df.columns)}\")\n",
    "        print(f\"   🔢 Row count match: {len(validation_df) == len(combined_df)}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Validation failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Validate the output\n",
    "if save_success:\n",
    "    validation_success = validate_output_file(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce67ba3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "🎉 DATA COMBINATION COMPLETED!\n",
      "============================================================\n",
      "📊 PROCESSING SUMMARY:\n",
      "   📁 Input Files: 2 (train + test)\n",
      "   🏋️ Train Samples: 72,155\n",
      "   🧪 Test Samples: 18,039\n",
      "   📈 Combined Total: 90,194\n",
      "   📋 Final Columns: 2\n",
      "\n",
      "💾 OUTPUT FILES:\n",
      "   📊 Main Dataset: processed_asr_data\\combined_asr_data.csv\n",
      "   📄 Metadata: processed_asr_data\\dataset_metadata.txt\n",
      "\n",
      "🔍 DATA QUALITY:\n",
      "   🧹 Duplicates Removed: 0\n",
      "   ✅ Missing Values: None\n",
      "   💯 Data Integrity: ✅ Verified\n",
      "\n",
      "🚀 READY FOR ASR TRAINING!\n",
      "   Use 'processed_asr_data\\combined_asr_data.csv' for your Sinhala ASR model training\n",
      "============================================================\n",
      "\n",
      "📁 Quick Access:\n",
      "   Combined Data: ./processed_asr_data\\combined_asr_data.csv\n",
      "   Metadata: ./processed_asr_data\\dataset_metadata.txt\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# FINAL SUMMARY REPORT\n",
    "# ================================\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"🎉 DATA COMBINATION COMPLETED!\")\n",
    "print(f\"=\" * 60)\n",
    "\n",
    "print(f\"📊 PROCESSING SUMMARY:\")\n",
    "print(f\"   📁 Input Files: 2 (train + test)\")\n",
    "print(f\"   🏋️ Train Samples: {len(train_df):,}\")\n",
    "print(f\"   🧪 Test Samples: {len(test_df):,}\")\n",
    "print(f\"   📈 Combined Total: {len(combined_df):,}\")\n",
    "print(f\"   📋 Final Columns: {len(combined_df.columns)}\")\n",
    "\n",
    "print(f\"\\n💾 OUTPUT FILES:\")\n",
    "print(f\"   📊 Main Dataset: {output_file}\")\n",
    "print(f\"   📄 Metadata: {os.path.join(output_dir, 'dataset_metadata.txt')}\")\n",
    "\n",
    "print(f\"\\n🔍 DATA QUALITY:\")\n",
    "duplicates_removed = initial_count - len(combined_df) if 'initial_count' in locals() else 0\n",
    "print(f\"   🧹 Duplicates Removed: {duplicates_removed:,}\")\n",
    "print(f\"   ✅ Missing Values: {'None' if combined_df.isnull().sum().sum() == 0 else 'Present'}\")\n",
    "print(f\"   💯 Data Integrity: {'✅ Verified' if save_success else '❌ Issues Found'}\")\n",
    "\n",
    "print(f\"\\n🚀 READY FOR ASR TRAINING!\")\n",
    "print(f\"   Use '{output_file}' for your Sinhala ASR model training\")\n",
    "print(f\"=\" * 60)\n",
    "\n",
    "# Display file locations for easy access\n",
    "print(f\"\\n📁 Quick Access:\")\n",
    "print(f\"   Combined Data: ./{output_file}\")\n",
    "print(f\"   Metadata: ./{os.path.join(output_dir, 'dataset_metadata.txt')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ca1fe3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "📊 EXTRACTING 10,000 SAMPLES\n",
      "============================================================\n",
      "📈 Dataset Info:\n",
      "   📊 Total available samples: 90,194\n",
      "   🎯 Target samples: 15,000\n",
      "   ✅ Sufficient samples available\n",
      "\n",
      "🔀 Sampling Strategy:\n",
      "   📊 Using random sampling for diversity\n",
      "   ✅ Extracted 15,000 samples\n",
      "\n",
      "💾 Saving sampled data...\n",
      "   📁 Output file: processed_asr_data\\15000-sinhala-asr-data.csv\n",
      "✅ Sampled data saved successfully!\n",
      "📊 File size: 1.58 MB\n",
      "📈 Rows saved: 15,000\n",
      "📋 Columns: ['sentence_cleaned', 'file']\n",
      "\n",
      "📝 Sample Preview:\n",
      "         sentence_cleaned                                 file\n",
      "0       හැමෝම අනිවාර්යෙන්  asr_sinhala/data/a2/a22f6ae045.flac\n",
      "1  ඇති හැකි ඇත්තෝ පෙරට ආහ  asr_sinhala/data/b9/b9666683a1.flac\n",
      "2         බෞද්ධයන් වන අප,  asr_sinhala/data/66/66160f5aa8.flac\n",
      "\n",
      "🎉 SUCCESS!\n",
      "   📁 File saved: processed_asr_data\\15000-sinhala-asr-data.csv\n",
      "   🚀 Ready for ASR training with 15,000 samples!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# EXTRACT 10,000 SAMPLES FOR SINHALA ASR\n",
    "# ================================\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"📊 EXTRACTING 10,000 SAMPLES\")\n",
    "print(f\"=\" * 60)\n",
    "\n",
    "# Check available samples\n",
    "total_samples = len(combined_df)\n",
    "target_samples = 15000\n",
    "\n",
    "print(f\"📈 Dataset Info:\")\n",
    "print(f\"   📊 Total available samples: {total_samples:,}\")\n",
    "print(f\"   🎯 Target samples: {target_samples:,}\")\n",
    "\n",
    "if total_samples < target_samples:\n",
    "    print(f\"⚠️ Warning: Only {total_samples:,} samples available, less than requested {target_samples:,}\")\n",
    "    sample_count = total_samples\n",
    "    print(f\"   💡 Will extract all {total_samples:,} samples\")\n",
    "else:\n",
    "    sample_count = target_samples\n",
    "    print(f\"   ✅ Sufficient samples available\")\n",
    "\n",
    "# Extract samples (random sampling for diversity)\n",
    "print(f\"\\n🔀 Sampling Strategy:\")\n",
    "print(f\"   📊 Using random sampling for diversity\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Sample the data\n",
    "sampled_df = combined_df.sample(n=sample_count, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"   ✅ Extracted {len(sampled_df):,} samples\")\n",
    "\n",
    "# Define output file path\n",
    "sample_output_file = os.path.join(output_dir, \"15000-sinhala-asr-data.csv\")\n",
    "\n",
    "print(f\"\\n💾 Saving sampled data...\")\n",
    "print(f\"   📁 Output file: {sample_output_file}\")\n",
    "\n",
    "# Save the sampled data\n",
    "try:\n",
    "    sampled_df.to_csv(sample_output_file, index=False)\n",
    "    \n",
    "    # Verify saved file\n",
    "    file_size = os.path.getsize(sample_output_file) / (1024**2)  # MB\n",
    "    \n",
    "    print(f\"✅ Sampled data saved successfully!\")\n",
    "    print(f\"📊 File size: {file_size:.2f} MB\")\n",
    "    print(f\"📈 Rows saved: {len(sampled_df):,}\")\n",
    "    print(f\"📋 Columns: {list(sampled_df.columns)}\")\n",
    "    \n",
    "    # Show sample preview\n",
    "    print(f\"\\n📝 Sample Preview:\")\n",
    "    print(sampled_df.head(3))\n",
    "    \n",
    "    print(f\"\\n🎉 SUCCESS!\")\n",
    "    print(f\"   📁 File saved: {sample_output_file}\")\n",
    "    print(f\"   🚀 Ready for ASR training with {len(sampled_df):,} samples!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error saving sampled data: {e}\")\n",
    "\n",
    "print(f\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72484f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "📊 SPLITTING 15,000 SAMPLES INTO TRAIN/TEST\n",
      "============================================================\n",
      "📈 Split Configuration:\n",
      "   📊 Total samples: 15,000\n",
      "   🏋️ Train split: 80%\n",
      "   🧪 Test split: 20%\n",
      "\n",
      "📊 Split Sizes:\n",
      "   🏋️ Train samples: 12,000\n",
      "   🧪 Test samples: 3,000\n",
      "\n",
      "🔀 Data Split:\n",
      "   ✅ Train data: 12,000 samples\n",
      "   ✅ Test data: 3,000 samples\n",
      "\n",
      "💾 Saving split files...\n",
      "   📁 Train file: processed_asr_data\\15-train.csv\n",
      "   📁 Test file: processed_asr_data\\15-test.csv\n",
      "   ✅ Train file saved: 1.27 MB\n",
      "   ✅ Test file saved: 0.32 MB\n",
      "\n",
      "📝 Sample Previews:\n",
      "\n",
      "🏋️ Train Data Sample:\n",
      "               sentence_cleaned                                 file\n",
      "0         නමුත් කතන්දර කියන ඒවා  asr_sinhala/data/db/db176e4b7c.flac\n",
      "1         අපට තිබුණේ පැළ සිටුවා  asr_sinhala/data/30/3070cc3c17.flac\n",
      "2  නීති විරෝධී ලෙස ඔට්ටු ඇල්ලීම  asr_sinhala/data/1c/1cb59c2460.flac\n",
      "\n",
      "🧪 Test Data Sample:\n",
      "           sentence_cleaned                                 file\n",
      "0        කවුරුවත් හොයා ගන්න  asr_sinhala/data/ab/ab01f6eea4.flac\n",
      "1      එතුමාට දැන් තියෙන්නේ  asr_sinhala/data/24/2492478f22.flac\n",
      "2  අපට පහසුවෙන් හඳුනාගත හැක  asr_sinhala/data/98/98efc1acab.flac\n",
      "\n",
      "🎉 SPLIT COMPLETED SUCCESSFULLY!\n",
      "   📁 Train file: processed_asr_data\\15-train.csv\n",
      "   📁 Test file: processed_asr_data\\15-test.csv\n",
      "   🏋️ Training samples: 12,000 (80%)\n",
      "   🧪 Testing samples: 3,000 (20%)\n",
      "   🚀 Ready for ASR model training and evaluation!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# SPLIT 15,000 SAMPLES INTO TRAIN/TEST (80/20)\n",
    "# ================================\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"📊 SPLITTING 15,000 SAMPLES INTO TRAIN/TEST\")\n",
    "print(f\"=\" * 60)\n",
    "\n",
    "# Check if sampled_df exists\n",
    "if 'sampled_df' not in locals():\n",
    "    print(f\"❌ Error: sampled_df not found. Please run the sampling cell first.\")\n",
    "else:\n",
    "    total_samples = len(sampled_df)\n",
    "    \n",
    "    print(f\"📈 Split Configuration:\")\n",
    "    print(f\"   📊 Total samples: {total_samples:,}\")\n",
    "    print(f\"   🏋️ Train split: 80%\")\n",
    "    print(f\"   🧪 Test split: 20%\")\n",
    "    \n",
    "    # Calculate split sizes\n",
    "    train_size = int(0.8 * total_samples)\n",
    "    test_size = total_samples - train_size\n",
    "    \n",
    "    print(f\"\\n📊 Split Sizes:\")\n",
    "    print(f\"   🏋️ Train samples: {train_size:,}\")\n",
    "    print(f\"   🧪 Test samples: {test_size:,}\")\n",
    "    \n",
    "    # Shuffle the data before splitting for better randomization\n",
    "    shuffled_df = sampled_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Split the data\n",
    "    train_10k_df = shuffled_df[:train_size].reset_index(drop=True)\n",
    "    test_10k_df = shuffled_df[train_size:].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\n🔀 Data Split:\")\n",
    "    print(f\"   ✅ Train data: {len(train_10k_df):,} samples\")\n",
    "    print(f\"   ✅ Test data: {len(test_10k_df):,} samples\")\n",
    "    \n",
    "    # Define output file paths\n",
    "    train_10k_file = os.path.join(output_dir, \"15-train.csv\")\n",
    "    test_10k_file = os.path.join(output_dir, \"15-test.csv\")\n",
    "    \n",
    "    print(f\"\\n💾 Saving split files...\")\n",
    "    print(f\"   📁 Train file: {train_10k_file}\")\n",
    "    print(f\"   📁 Test file: {test_10k_file}\")\n",
    "    \n",
    "    # Save train file\n",
    "    try:\n",
    "        train_10k_df.to_csv(train_10k_file, index=False)\n",
    "        train_file_size = os.path.getsize(train_10k_file) / (1024**2)  # MB\n",
    "        print(f\"   ✅ Train file saved: {train_file_size:.2f} MB\")\n",
    "        train_success = True\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error saving train file: {e}\")\n",
    "        train_success = False\n",
    "    \n",
    "    # Save test file\n",
    "    try:\n",
    "        test_10k_df.to_csv(test_10k_file, index=False)\n",
    "        test_file_size = os.path.getsize(test_10k_file) / (1024**2)  # MB\n",
    "        print(f\"   ✅ Test file saved: {test_file_size:.2f} MB\")\n",
    "        test_success = True\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error saving test file: {e}\")\n",
    "        test_success = False\n",
    "    \n",
    "    if train_success and test_success:\n",
    "        print(f\"\\n📝 Sample Previews:\")\n",
    "        print(f\"\\n🏋️ Train Data Sample:\")\n",
    "        print(train_10k_df.head(3))\n",
    "        \n",
    "        print(f\"\\n🧪 Test Data Sample:\")\n",
    "        print(test_10k_df.head(3))\n",
    "        \n",
    "        print(f\"\\n🎉 SPLIT COMPLETED SUCCESSFULLY!\")\n",
    "        print(f\"   📁 Train file: {train_10k_file}\")\n",
    "        print(f\"   📁 Test file: {test_10k_file}\")\n",
    "        print(f\"   🏋️ Training samples: {len(train_10k_df):,} (80%)\")\n",
    "        print(f\"   🧪 Testing samples: {len(test_10k_df):,} (20%)\")\n",
    "        print(f\"   🚀 Ready for ASR model training and evaluation!\")\n",
    "    else:\n",
    "        print(f\"\\n❌ Some files failed to save. Please check the errors above.\")\n",
    "\n",
    "print(f\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dcb941",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

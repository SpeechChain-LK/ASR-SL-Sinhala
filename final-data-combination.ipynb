{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb5df9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— ASR Data Combination Tool\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# COMBINE TRAIN AND TEST DATA CLEAN FILES\n",
    "# Final Data Combination for Sinhala ASR Dataset\n",
    "# ================================\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"ğŸ”— ASR Data Combination Tool\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "070ef68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Configuration:\n",
      "   ğŸ“Š Train file: processed_asr_data/train_data_clean.csv\n",
      "   ğŸ“Š Test file: processed_asr_data/test_data_clean.csv\n",
      "   ğŸ“ Output directory: processed_asr_data\n",
      "   ğŸ’¾ Output file: processed_asr_data\\combined_asr_data.csv\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# FILE PATHS CONFIGURATION\n",
    "# ================================\n",
    "\n",
    "# Define input and output paths\n",
    "train_file = \"processed_asr_data/train_data_clean.csv\"\n",
    "test_file = \"processed_asr_data/test_data_clean.csv\"\n",
    "output_dir = \"processed_asr_data\"\n",
    "output_file = os.path.join(output_dir, \"combined_asr_data.csv\")\n",
    "\n",
    "print(f\"ğŸ“ Configuration:\")\n",
    "print(f\"   ğŸ“Š Train file: {train_file}\")\n",
    "print(f\"   ğŸ“Š Test file: {test_file}\")\n",
    "print(f\"   ğŸ“ Output directory: {output_dir}\")\n",
    "print(f\"   ğŸ’¾ Output file: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c464a858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Loading data files...\n",
      "âœ… Train data loaded: 72,155 samples\n",
      "âœ… Test data loaded: 18,039 samples\n",
      "âœ… Train data loaded: 72,155 samples\n",
      "âœ… Test data loaded: 18,039 samples\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# DATA LOADING AND VALIDATION\n",
    "# ================================\n",
    "\n",
    "def load_and_validate_files(train_path, test_path):\n",
    "    \"\"\"Load and validate input files\"\"\"\n",
    "    \n",
    "    print(\"\\nğŸ“Š Loading data files...\")\n",
    "    \n",
    "    # Check if files exist\n",
    "    if not os.path.exists(train_path):\n",
    "        raise FileNotFoundError(f\"âŒ Train file not found: {train_path}\")\n",
    "    if not os.path.exists(test_path):\n",
    "        raise FileNotFoundError(f\"âŒ Test file not found: {test_path}\")\n",
    "    \n",
    "    # Load the files\n",
    "    try:\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "        \n",
    "        print(f\"âœ… Train data loaded: {len(train_df):,} samples\")\n",
    "        print(f\"âœ… Test data loaded: {len(test_df):,} samples\")\n",
    "        \n",
    "        return train_df, test_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"âŒ Error loading files: {e}\")\n",
    "\n",
    "# Load the data\n",
    "train_df, test_df = load_and_validate_files(train_file, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1a57313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Data Structure Analysis:\n",
      "========================================\n",
      "ğŸ“‹ Train Data Structure:\n",
      "   Shape: (72155, 2)\n",
      "   Columns: ['file', 'sentence_cleaned']\n",
      "   Data types: {'file': dtype('O'), 'sentence_cleaned': dtype('O')}\n",
      "\n",
      "ğŸ“‹ Test Data Structure:\n",
      "   Shape: (18039, 2)\n",
      "   Columns: ['file', 'sentence_cleaned']\n",
      "   Data types: {'file': dtype('O'), 'sentence_cleaned': dtype('O')}\n",
      "\n",
      "ğŸ” Column Compatibility Check:\n",
      "âœ… Column names match perfectly\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# DATA STRUCTURE ANALYSIS\n",
    "# ================================\n",
    "\n",
    "print(f\"\\nğŸ” Data Structure Analysis:\")\n",
    "print(f\"=\" * 40)\n",
    "\n",
    "print(f\"ğŸ“‹ Train Data Structure:\")\n",
    "print(f\"   Shape: {train_df.shape}\")\n",
    "print(f\"   Columns: {list(train_df.columns)}\")\n",
    "print(f\"   Data types: {train_df.dtypes.to_dict()}\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ Test Data Structure:\")\n",
    "print(f\"   Shape: {test_df.shape}\")\n",
    "print(f\"   Columns: {list(test_df.columns)}\")\n",
    "print(f\"   Data types: {test_df.dtypes.to_dict()}\")\n",
    "\n",
    "# Check column compatibility\n",
    "print(f\"\\nğŸ” Column Compatibility Check:\")\n",
    "train_cols = set(train_df.columns)\n",
    "test_cols = set(test_df.columns)\n",
    "\n",
    "if train_cols == test_cols:\n",
    "    print(\"âœ… Column names match perfectly\")\n",
    "    common_cols = list(train_df.columns)\n",
    "else:\n",
    "    print(\"âš ï¸ Column mismatch detected\")\n",
    "    common_cols = list(train_cols & test_cols)\n",
    "    train_only = train_cols - test_cols\n",
    "    test_only = test_cols - train_cols\n",
    "    \n",
    "    print(f\"   ğŸ”— Common columns: {common_cols}\")\n",
    "    if train_only:\n",
    "        print(f\"   ğŸ‹ï¸ Train-only columns: {list(train_only)}\")\n",
    "    if test_only:\n",
    "        print(f\"   ğŸ§ª Test-only columns: {list(test_only)}\")\n",
    "    \n",
    "    print(f\"   ğŸ’¡ Will use common columns only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "695bc67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Sample Data Preview:\n",
      "==============================\n",
      "\n",
      "ğŸ‹ï¸ Train Data Sample:\n",
      "                                  file  \\\n",
      "0  asr_sinhala/data/98/983e3c6613.flac   \n",
      "1  asr_sinhala/data/29/29ab15c6d4.flac   \n",
      "2  asr_sinhala/data/b0/b0072f9ac0.flac   \n",
      "\n",
      "                                sentence_cleaned  \n",
      "0   à¶‘à¶º à¶¸à·’à¶½à·’à¶§à¶»à·’à¶º à¶­à·”à·… à¶­à·’â€à¶ºà¶± à¶´à·Šâ€à¶»à¶°à·à¶±à¶¸ à¶´à·Šâ€à¶»à¶­à·’à¶¸à·à¶±à¶ºà¶šà·Š.  \n",
      "1  à·ƒà·à·„à·’à¶­à·Šâ€à¶ºà¶šà¶»à·”à·€à·à¶§ à¶Šà¶§ à·€à·à¶©à·’à¶º à¶½à·œà¶šà·” à·€à¶œà¶šà·“à¶¸à¶šà·Š à¶­à·’à¶ºà·™à¶±à·€à·.  \n",
      "2                        à¶•à¶œà·œà¶½à·Šà¶½à¶±à·Šà¶§ à¶¯à¶šà·’à¶±à·Šà¶± à¶½à·à¶¶à·™à¶ºà·’  \n",
      "\n",
      "ğŸ§ª Test Data Sample:\n",
      "                                  file  \\\n",
      "0  asr_sinhala/data/b6/b63bfb7a34.flac   \n",
      "1  asr_sinhala/data/f1/f134378295.flac   \n",
      "2  asr_sinhala/data/d3/d3bf1e10b0.flac   \n",
      "\n",
      "                                  sentence_cleaned  \n",
      "0                                à¶¸à·™à¶šà·“ à·ƒà·’à·€à·Š à¶¸à·„ à¶°à·à¶­à·”  \n",
      "1                           à¶‘à¶¯à·’à¶±à·™à¶¯à· à¶¢à·“à·€à·’à¶­à¶ºà·š à·ƒà·’à¶¯à·”à·€à¶±  \n",
      "2  à¶…à¶°à·’à·€à·šà¶œà·“ à¶¸à·à¶»à·Šà¶œà¶ºà·š à¶¸à·à¶¯ à·ƒà·šà·€à· à·ƒà·Šà¶®à·à¶±à¶ºà¶šà·Š à¶‰à¶¯à·’ à¶šà·… à¶´à¶¸à¶«à·’à¶±à·Š  \n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# SAMPLE DATA PREVIEW\n",
    "# ================================\n",
    "\n",
    "print(f\"\\nğŸ“ Sample Data Preview:\")\n",
    "print(f\"=\" * 30)\n",
    "\n",
    "print(f\"\\nğŸ‹ï¸ Train Data Sample:\")\n",
    "print(train_df.head(3))\n",
    "\n",
    "print(f\"\\nğŸ§ª Test Data Sample:\")\n",
    "print(test_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32a455ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”— Preparing data for combination...\n",
      "â„¹ï¸ Skipping 'data_split' column - combining without split indicator\n",
      "ğŸ”— Combining datasets...\n",
      "ğŸ“Š Combination Results:\n",
      "   ğŸ‹ï¸ Train samples: 72,155\n",
      "   ğŸ§ª Test samples: 18,039\n",
      "   ğŸ“ˆ Combined total: 90,194\n",
      "   ğŸ“‹ Final columns: ['sentence_cleaned', 'file']\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# DATA PREPARATION AND COMBINATION\n",
    "# ================================\n",
    "\n",
    "def prepare_and_combine_data(train_data, test_data, include_split_column=False):\n",
    "    \"\"\"Prepare and combine train and test data\"\"\"\n",
    "    \n",
    "    print(f\"\\nğŸ”— Preparing data for combination...\")\n",
    "    \n",
    "    # Use common columns only\n",
    "    train_cols = set(train_data.columns)\n",
    "    test_cols = set(test_data.columns)\n",
    "    common_cols = list(train_cols & test_cols)\n",
    "    \n",
    "    train_prepared = train_data[common_cols].copy()\n",
    "    test_prepared = test_data[common_cols].copy()\n",
    "    \n",
    "    # Add split indicator column (disabled by default)\n",
    "    if include_split_column:\n",
    "        train_prepared['data_split'] = 'train'\n",
    "        test_prepared['data_split'] = 'test'\n",
    "        print(\"âœ… Added 'data_split' column\")\n",
    "    else:\n",
    "        print(\"â„¹ï¸ Skipping 'data_split' column - combining without split indicator\")\n",
    "    \n",
    "    # Combine datasets\n",
    "    print(\"ğŸ”— Combining datasets...\")\n",
    "    combined_df = pd.concat([train_prepared, test_prepared], ignore_index=True)\n",
    "    \n",
    "    print(f\"ğŸ“Š Combination Results:\")\n",
    "    print(f\"   ğŸ‹ï¸ Train samples: {len(train_prepared):,}\")\n",
    "    print(f\"   ğŸ§ª Test samples: {len(test_prepared):,}\")\n",
    "    print(f\"   ğŸ“ˆ Combined total: {len(combined_df):,}\")\n",
    "    print(f\"   ğŸ“‹ Final columns: {list(combined_df.columns)}\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Combine the data WITHOUT split column\n",
    "combined_df = prepare_and_combine_data(train_df, test_df, include_split_column=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16501cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Data Quality Assessment:\n",
      "===================================\n",
      "ğŸ”„ Duplicate Analysis:\n",
      "   Total rows: 90,194\n",
      "   Duplicate rows: 0\n",
      "   âœ… No duplicates found\n",
      "\n",
      "ğŸ” Missing Value Analysis:\n",
      "   âœ… No missing values found\n",
      "\n",
      "ğŸ“Š Combined Dataset:\n",
      "   ğŸ”— Total samples: 90,194 (train + test combined)\n",
      "   â„¹ï¸ No split indicator column included\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# DATA QUALITY CHECKS\n",
    "# ================================\n",
    "\n",
    "print(f\"\\nğŸ” Data Quality Assessment:\")\n",
    "print(f\"=\" * 35)\n",
    "\n",
    "# Check for duplicates\n",
    "initial_count = len(combined_df)\n",
    "duplicates = combined_df.duplicated().sum()\n",
    "print(f\"ğŸ”„ Duplicate Analysis:\")\n",
    "print(f\"   Total rows: {initial_count:,}\")\n",
    "print(f\"   Duplicate rows: {duplicates:,}\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    print(f\"   ğŸ§¹ Removing {duplicates:,} duplicates...\")\n",
    "    combined_df = combined_df.drop_duplicates()\n",
    "    final_count = len(combined_df)\n",
    "    print(f\"   âœ… After deduplication: {final_count:,} samples\")\n",
    "    print(f\"   ğŸ“‰ Removed: {initial_count - final_count:,} duplicates\")\n",
    "else:\n",
    "    print(f\"   âœ… No duplicates found\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nğŸ” Missing Value Analysis:\")\n",
    "missing_data = combined_df.isnull().sum()\n",
    "total_missing = missing_data.sum()\n",
    "\n",
    "if total_missing > 0:\n",
    "    print(f\"   âš ï¸ Missing values detected:\")\n",
    "    for col, count in missing_data.items():\n",
    "        if count > 0:\n",
    "            percentage = (count / len(combined_df)) * 100\n",
    "            print(f\"      ğŸ“Š {col}: {count:,} missing ({percentage:.1f}%)\")\n",
    "else:\n",
    "    print(f\"   âœ… No missing values found\")\n",
    "\n",
    "# Data split distribution (only if column exists)\n",
    "if 'data_split' in combined_df.columns:\n",
    "    print(f\"\\nğŸ“Š Data Split Distribution:\")\n",
    "    split_counts = combined_df['data_split'].value_counts()\n",
    "    for split, count in split_counts.items():\n",
    "        percentage = (count / len(combined_df)) * 100\n",
    "        print(f\"   {split.title()}: {count:,} samples ({percentage:.1f}%)\")\n",
    "else:\n",
    "    print(f\"\\nğŸ“Š Combined Dataset:\")\n",
    "    print(f\"   ğŸ”— Total samples: {len(combined_df):,} (train + test combined)\")\n",
    "    print(f\"   â„¹ï¸ No split indicator column included\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9442e3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Final Combined Dataset Preview:\n",
      "========================================\n",
      "ğŸ“Š Dataset Shape: (90194, 2)\n",
      "ğŸ“‹ Columns: ['sentence_cleaned', 'file']\n",
      "\n",
      "ğŸ” Sample Combined Data:\n",
      "                                sentence_cleaned  \\\n",
      "0   à¶‘à¶º à¶¸à·’à¶½à·’à¶§à¶»à·’à¶º à¶­à·”à·… à¶­à·’â€à¶ºà¶± à¶´à·Šâ€à¶»à¶°à·à¶±à¶¸ à¶´à·Šâ€à¶»à¶­à·’à¶¸à·à¶±à¶ºà¶šà·Š.   \n",
      "1  à·ƒà·à·„à·’à¶­à·Šâ€à¶ºà¶šà¶»à·”à·€à·à¶§ à¶Šà¶§ à·€à·à¶©à·’à¶º à¶½à·œà¶šà·” à·€à¶œà¶šà·“à¶¸à¶šà·Š à¶­à·’à¶ºà·™à¶±à·€à·.   \n",
      "2                        à¶•à¶œà·œà¶½à·Šà¶½à¶±à·Šà¶§ à¶¯à¶šà·’à¶±à·Šà¶± à¶½à·à¶¶à·™à¶ºà·’   \n",
      "3                     à¶¶à·œà¶¯à·” à¶¶à¶½ à·ƒà·šà¶±à· à·„à·’à¶¸à·’à·€à¶»à·”à¶±à·Š à¶…à¶­à¶»   \n",
      "4                            à¶”à¶¶à¶­à·”à¶¸à· à¶¯à¶±à·Šà¶±à· à·€à·’à¶¯à·’à¶ºà¶§   \n",
      "\n",
      "                                  file  \n",
      "0  asr_sinhala/data/98/983e3c6613.flac  \n",
      "1  asr_sinhala/data/29/29ab15c6d4.flac  \n",
      "2  asr_sinhala/data/b0/b0072f9ac0.flac  \n",
      "3  asr_sinhala/data/85/85fc577f7b.flac  \n",
      "4  asr_sinhala/data/7e/7e6bd7795f.flac  \n",
      "\n",
      "ğŸ“ˆ Dataset Statistics:\n",
      "   ğŸ“Š Total samples: 90,194\n",
      "   ğŸ“‹ Features: 2\n",
      "   ğŸ’¾ Memory usage: 23.81 MB\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# FINAL COMBINED DATA PREVIEW\n",
    "# ================================\n",
    "\n",
    "print(f\"\\nğŸ“ Final Combined Dataset Preview:\")\n",
    "print(f\"=\" * 40)\n",
    "\n",
    "print(f\"ğŸ“Š Dataset Shape: {combined_df.shape}\")\n",
    "print(f\"ğŸ“‹ Columns: {list(combined_df.columns)}\")\n",
    "\n",
    "print(f\"\\nğŸ” Sample Combined Data:\")\n",
    "print(combined_df.head(5))\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Dataset Statistics:\")\n",
    "print(f\"   ğŸ“Š Total samples: {len(combined_df):,}\")\n",
    "print(f\"   ğŸ“‹ Features: {len(combined_df.columns)}\")\n",
    "print(f\"   ğŸ’¾ Memory usage: {combined_df.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd572135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ Saving combined data...\n",
      "âœ… Data saved successfully!\n",
      "ğŸ“ File location: processed_asr_data\\combined_asr_data.csv\n",
      "ğŸ“Š File size: 9.50 MB\n",
      "ğŸ“ˆ Rows saved: 90,194\n",
      "âœ… Data saved successfully!\n",
      "ğŸ“ File location: processed_asr_data\\combined_asr_data.csv\n",
      "ğŸ“Š File size: 9.50 MB\n",
      "ğŸ“ˆ Rows saved: 90,194\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# SAVE COMBINED DATA\n",
    "# ================================\n",
    "\n",
    "def save_combined_data(data, output_path):\n",
    "    \"\"\"Save combined data to file\"\"\"\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ Saving combined data...\")\n",
    "    \n",
    "    # Create output directory if needed\n",
    "    output_directory = os.path.dirname(output_path)\n",
    "    if output_directory and not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        print(f\"ğŸ“ Created output directory: {output_directory}\")\n",
    "    \n",
    "    try:\n",
    "        # Save to CSV\n",
    "        data.to_csv(output_path, index=False)\n",
    "        \n",
    "        # Verify saved file\n",
    "        file_size = os.path.getsize(output_path) / (1024**2)  # MB\n",
    "        \n",
    "        print(f\"âœ… Data saved successfully!\")\n",
    "        print(f\"ğŸ“ File location: {output_path}\")\n",
    "        print(f\"ğŸ“Š File size: {file_size:.2f} MB\")\n",
    "        print(f\"ğŸ“ˆ Rows saved: {len(data):,}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error saving file: {e}\")\n",
    "        return False\n",
    "\n",
    "# Save the combined data\n",
    "save_success = save_combined_data(combined_df, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a218d1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“„ Creating metadata file...\n",
      "âœ… Metadata file created: processed_asr_data\\dataset_metadata.txt\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# CREATE METADATA FILE\n",
    "# ================================\n",
    "\n",
    "def create_metadata_file(data, output_directory):\n",
    "    \"\"\"Create detailed metadata file\"\"\"\n",
    "    \n",
    "    metadata_path = os.path.join(output_directory, \"dataset_metadata.txt\")\n",
    "    \n",
    "    print(f\"\\nğŸ“„ Creating metadata file...\")\n",
    "    \n",
    "    try:\n",
    "        with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"COMBINED SINHALA ASR DATASET METADATA\\n\")\n",
    "            f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "            \n",
    "            # Basic information\n",
    "            f.write(f\"Creation Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"Total Samples: {len(data):,}\\n\")\n",
    "            f.write(f\"Total Columns: {len(data.columns)}\\n\")\n",
    "            f.write(f\"Dataset Type: Combined (train + test without split indicator)\\n\\n\")\n",
    "            \n",
    "            # Column information\n",
    "            f.write(\"COLUMN INFORMATION:\\n\")\n",
    "            f.write(\"-\" * 30 + \"\\n\")\n",
    "            for i, col in enumerate(data.columns, 1):\n",
    "                dtype = str(data[col].dtype)\n",
    "                non_null = data[col].count()\n",
    "                f.write(f\"{i:2d}. {col:<20} | Type: {dtype:<10} | Non-null: {non_null:,}\\n\")\n",
    "            \n",
    "            # Data split information (only if column exists)\n",
    "            if 'data_split' in data.columns:\n",
    "                f.write(f\"\\nDATA SPLIT DISTRIBUTION:\\n\")\n",
    "                f.write(\"-\" * 30 + \"\\n\")\n",
    "                split_counts = data['data_split'].value_counts()\n",
    "                for split, count in split_counts.items():\n",
    "                    percentage = (count / len(data)) * 100\n",
    "                    f.write(f\"{split.title():<10}: {count:>8,} samples ({percentage:5.1f}%)\\n\")\n",
    "            else:\n",
    "                f.write(f\"\\nDATASET STRUCTURE:\\n\")\n",
    "                f.write(\"-\" * 30 + \"\\n\")\n",
    "                f.write(f\"Combined Dataset: {len(data):,} total samples\\n\")\n",
    "                f.write(f\"Split Indicator: Not included (unified dataset)\\n\")\n",
    "                f.write(f\"Source: train_data_clean.csv + test_data_clean.csv\\n\")\n",
    "            \n",
    "            # Missing values\n",
    "            f.write(f\"\\nMISSING VALUES SUMMARY:\\n\")\n",
    "            f.write(\"-\" * 30 + \"\\n\")\n",
    "            missing_summary = data.isnull().sum()\n",
    "            for col, missing_count in missing_summary.items():\n",
    "                if missing_count > 0:\n",
    "                    percentage = (missing_count / len(data)) * 100\n",
    "                    f.write(f\"{col:<20}: {missing_count:>6,} missing ({percentage:5.1f}%)\\n\")\n",
    "            if missing_summary.sum() == 0:\n",
    "                f.write(\"No missing values found.\\n\")\n",
    "            \n",
    "            # Sample data\n",
    "            f.write(f\"\\nSAMPLE DATA (First 5 rows):\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "            f.write(data.head(5).to_string())\n",
    "            \n",
    "            f.write(f\"\\n\\nEND OF METADATA\\n\")\n",
    "        \n",
    "        print(f\"âœ… Metadata file created: {metadata_path}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error creating metadata: {e}\")\n",
    "        return False\n",
    "\n",
    "# Create metadata file\n",
    "if save_success:\n",
    "    create_metadata_file(combined_df, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f19226a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Validating output file...\n",
      "âœ… File validation successful!\n",
      "   ğŸ“Š Loaded shape: (90194, 2)\n",
      "   ğŸ“‹ Columns match: True\n",
      "   ğŸ”¢ Row count match: True\n",
      "âœ… File validation successful!\n",
      "   ğŸ“Š Loaded shape: (90194, 2)\n",
      "   ğŸ“‹ Columns match: True\n",
      "   ğŸ”¢ Row count match: True\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# VALIDATION AND SUMMARY\n",
    "# ================================\n",
    "\n",
    "def validate_output_file(file_path):\n",
    "    \"\"\"Validate the saved output file\"\"\"\n",
    "    \n",
    "    print(f\"\\nğŸ” Validating output file...\")\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"âŒ Output file not found: {file_path}\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Load and verify\n",
    "        validation_df = pd.read_csv(file_path)\n",
    "        \n",
    "        print(f\"âœ… File validation successful!\")\n",
    "        print(f\"   ğŸ“Š Loaded shape: {validation_df.shape}\")\n",
    "        print(f\"   ğŸ“‹ Columns match: {list(validation_df.columns) == list(combined_df.columns)}\")\n",
    "        print(f\"   ğŸ”¢ Row count match: {len(validation_df) == len(combined_df)}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Validation failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Validate the output\n",
    "if save_success:\n",
    "    validation_success = validate_output_file(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce67ba3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ‰ DATA COMBINATION COMPLETED!\n",
      "============================================================\n",
      "ğŸ“Š PROCESSING SUMMARY:\n",
      "   ğŸ“ Input Files: 2 (train + test)\n",
      "   ğŸ‹ï¸ Train Samples: 72,155\n",
      "   ğŸ§ª Test Samples: 18,039\n",
      "   ğŸ“ˆ Combined Total: 90,194\n",
      "   ğŸ“‹ Final Columns: 2\n",
      "\n",
      "ğŸ’¾ OUTPUT FILES:\n",
      "   ğŸ“Š Main Dataset: processed_asr_data\\combined_asr_data.csv\n",
      "   ğŸ“„ Metadata: processed_asr_data\\dataset_metadata.txt\n",
      "\n",
      "ğŸ” DATA QUALITY:\n",
      "   ğŸ§¹ Duplicates Removed: 0\n",
      "   âœ… Missing Values: None\n",
      "   ğŸ’¯ Data Integrity: âœ… Verified\n",
      "\n",
      "ğŸš€ READY FOR ASR TRAINING!\n",
      "   Use 'processed_asr_data\\combined_asr_data.csv' for your Sinhala ASR model training\n",
      "============================================================\n",
      "\n",
      "ğŸ“ Quick Access:\n",
      "   Combined Data: ./processed_asr_data\\combined_asr_data.csv\n",
      "   Metadata: ./processed_asr_data\\dataset_metadata.txt\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# FINAL SUMMARY REPORT\n",
    "# ================================\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"ğŸ‰ DATA COMBINATION COMPLETED!\")\n",
    "print(f\"=\" * 60)\n",
    "\n",
    "print(f\"ğŸ“Š PROCESSING SUMMARY:\")\n",
    "print(f\"   ğŸ“ Input Files: 2 (train + test)\")\n",
    "print(f\"   ğŸ‹ï¸ Train Samples: {len(train_df):,}\")\n",
    "print(f\"   ğŸ§ª Test Samples: {len(test_df):,}\")\n",
    "print(f\"   ğŸ“ˆ Combined Total: {len(combined_df):,}\")\n",
    "print(f\"   ğŸ“‹ Final Columns: {len(combined_df.columns)}\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ OUTPUT FILES:\")\n",
    "print(f\"   ğŸ“Š Main Dataset: {output_file}\")\n",
    "print(f\"   ğŸ“„ Metadata: {os.path.join(output_dir, 'dataset_metadata.txt')}\")\n",
    "\n",
    "print(f\"\\nğŸ” DATA QUALITY:\")\n",
    "duplicates_removed = initial_count - len(combined_df) if 'initial_count' in locals() else 0\n",
    "print(f\"   ğŸ§¹ Duplicates Removed: {duplicates_removed:,}\")\n",
    "print(f\"   âœ… Missing Values: {'None' if combined_df.isnull().sum().sum() == 0 else 'Present'}\")\n",
    "print(f\"   ğŸ’¯ Data Integrity: {'âœ… Verified' if save_success else 'âŒ Issues Found'}\")\n",
    "\n",
    "print(f\"\\nğŸš€ READY FOR ASR TRAINING!\")\n",
    "print(f\"   Use '{output_file}' for your Sinhala ASR model training\")\n",
    "print(f\"=\" * 60)\n",
    "\n",
    "# Display file locations for easy access\n",
    "print(f\"\\nğŸ“ Quick Access:\")\n",
    "print(f\"   Combined Data: ./{output_file}\")\n",
    "print(f\"   Metadata: ./{os.path.join(output_dir, 'dataset_metadata.txt')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ca1fe3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“Š EXTRACTING 10,000 SAMPLES\n",
      "============================================================\n",
      "ğŸ“ˆ Dataset Info:\n",
      "   ğŸ“Š Total available samples: 90,194\n",
      "   ğŸ¯ Target samples: 15,000\n",
      "   âœ… Sufficient samples available\n",
      "\n",
      "ğŸ”€ Sampling Strategy:\n",
      "   ğŸ“Š Using random sampling for diversity\n",
      "   âœ… Extracted 15,000 samples\n",
      "\n",
      "ğŸ’¾ Saving sampled data...\n",
      "   ğŸ“ Output file: processed_asr_data\\15000-sinhala-asr-data.csv\n",
      "âœ… Sampled data saved successfully!\n",
      "ğŸ“Š File size: 1.58 MB\n",
      "ğŸ“ˆ Rows saved: 15,000\n",
      "ğŸ“‹ Columns: ['sentence_cleaned', 'file']\n",
      "\n",
      "ğŸ“ Sample Preview:\n",
      "         sentence_cleaned                                 file\n",
      "0       à·„à·à¶¸à·à¶¸ à¶…à¶±à·’à·€à·à¶»à·Šà¶ºà·™à¶±à·Š  asr_sinhala/data/a2/a22f6ae045.flac\n",
      "1  à¶‡à¶­à·’ à·„à·à¶šà·’ à¶‡à¶­à·Šà¶­à· à¶´à·™à¶»à¶§ à¶†à·„  asr_sinhala/data/b9/b9666683a1.flac\n",
      "2         à¶¶à·à¶¯à·Šà¶°à¶ºà¶±à·Š à·€à¶± à¶…à¶´,  asr_sinhala/data/66/66160f5aa8.flac\n",
      "\n",
      "ğŸ‰ SUCCESS!\n",
      "   ğŸ“ File saved: processed_asr_data\\15000-sinhala-asr-data.csv\n",
      "   ğŸš€ Ready for ASR training with 15,000 samples!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# EXTRACT 10,000 SAMPLES FOR SINHALA ASR\n",
    "# ================================\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"ğŸ“Š EXTRACTING 10,000 SAMPLES\")\n",
    "print(f\"=\" * 60)\n",
    "\n",
    "# Check available samples\n",
    "total_samples = len(combined_df)\n",
    "target_samples = 15000\n",
    "\n",
    "print(f\"ğŸ“ˆ Dataset Info:\")\n",
    "print(f\"   ğŸ“Š Total available samples: {total_samples:,}\")\n",
    "print(f\"   ğŸ¯ Target samples: {target_samples:,}\")\n",
    "\n",
    "if total_samples < target_samples:\n",
    "    print(f\"âš ï¸ Warning: Only {total_samples:,} samples available, less than requested {target_samples:,}\")\n",
    "    sample_count = total_samples\n",
    "    print(f\"   ğŸ’¡ Will extract all {total_samples:,} samples\")\n",
    "else:\n",
    "    sample_count = target_samples\n",
    "    print(f\"   âœ… Sufficient samples available\")\n",
    "\n",
    "# Extract samples (random sampling for diversity)\n",
    "print(f\"\\nğŸ”€ Sampling Strategy:\")\n",
    "print(f\"   ğŸ“Š Using random sampling for diversity\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Sample the data\n",
    "sampled_df = combined_df.sample(n=sample_count, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"   âœ… Extracted {len(sampled_df):,} samples\")\n",
    "\n",
    "# Define output file path\n",
    "sample_output_file = os.path.join(output_dir, \"15000-sinhala-asr-data.csv\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ Saving sampled data...\")\n",
    "print(f\"   ğŸ“ Output file: {sample_output_file}\")\n",
    "\n",
    "# Save the sampled data\n",
    "try:\n",
    "    sampled_df.to_csv(sample_output_file, index=False)\n",
    "    \n",
    "    # Verify saved file\n",
    "    file_size = os.path.getsize(sample_output_file) / (1024**2)  # MB\n",
    "    \n",
    "    print(f\"âœ… Sampled data saved successfully!\")\n",
    "    print(f\"ğŸ“Š File size: {file_size:.2f} MB\")\n",
    "    print(f\"ğŸ“ˆ Rows saved: {len(sampled_df):,}\")\n",
    "    print(f\"ğŸ“‹ Columns: {list(sampled_df.columns)}\")\n",
    "    \n",
    "    # Show sample preview\n",
    "    print(f\"\\nğŸ“ Sample Preview:\")\n",
    "    print(sampled_df.head(3))\n",
    "    \n",
    "    print(f\"\\nğŸ‰ SUCCESS!\")\n",
    "    print(f\"   ğŸ“ File saved: {sample_output_file}\")\n",
    "    print(f\"   ğŸš€ Ready for ASR training with {len(sampled_df):,} samples!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error saving sampled data: {e}\")\n",
    "\n",
    "print(f\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72484f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“Š SPLITTING 15,000 SAMPLES INTO TRAIN/TEST\n",
      "============================================================\n",
      "ğŸ“ˆ Split Configuration:\n",
      "   ğŸ“Š Total samples: 15,000\n",
      "   ğŸ‹ï¸ Train split: 80%\n",
      "   ğŸ§ª Test split: 20%\n",
      "\n",
      "ğŸ“Š Split Sizes:\n",
      "   ğŸ‹ï¸ Train samples: 12,000\n",
      "   ğŸ§ª Test samples: 3,000\n",
      "\n",
      "ğŸ”€ Data Split:\n",
      "   âœ… Train data: 12,000 samples\n",
      "   âœ… Test data: 3,000 samples\n",
      "\n",
      "ğŸ’¾ Saving split files...\n",
      "   ğŸ“ Train file: processed_asr_data\\15-train.csv\n",
      "   ğŸ“ Test file: processed_asr_data\\15-test.csv\n",
      "   âœ… Train file saved: 1.27 MB\n",
      "   âœ… Test file saved: 0.32 MB\n",
      "\n",
      "ğŸ“ Sample Previews:\n",
      "\n",
      "ğŸ‹ï¸ Train Data Sample:\n",
      "               sentence_cleaned                                 file\n",
      "0         à¶±à¶¸à·”à¶­à·Š à¶šà¶­à¶±à·Šà¶¯à¶» à¶šà·’à¶ºà¶± à¶’à·€à·  asr_sinhala/data/db/db176e4b7c.flac\n",
      "1         à¶…à¶´à¶§ à¶­à·’à¶¶à·”à¶«à·š à¶´à·à·… à·ƒà·’à¶§à·”à·€à·  asr_sinhala/data/30/3070cc3c17.flac\n",
      "2  à¶±à·“à¶­à·’ à·€à·’à¶»à·à¶°à·“ à¶½à·™à·ƒ à¶”à¶§à·Šà¶§à·” à¶‡à¶½à·Šà¶½à·“à¶¸  asr_sinhala/data/1c/1cb59c2460.flac\n",
      "\n",
      "ğŸ§ª Test Data Sample:\n",
      "           sentence_cleaned                                 file\n",
      "0        à¶šà·€à·”à¶»à·”à·€à¶­à·Š à·„à·œà¶ºà· à¶œà¶±à·Šà¶±  asr_sinhala/data/ab/ab01f6eea4.flac\n",
      "1      à¶‘à¶­à·”à¶¸à·à¶§ à¶¯à·à¶±à·Š à¶­à·’à¶ºà·™à¶±à·Šà¶±à·š  asr_sinhala/data/24/2492478f22.flac\n",
      "2  à¶…à¶´à¶§ à¶´à·„à·ƒà·”à·€à·™à¶±à·Š à·„à¶³à·”à¶±à·à¶œà¶­ à·„à·à¶š  asr_sinhala/data/98/98efc1acab.flac\n",
      "\n",
      "ğŸ‰ SPLIT COMPLETED SUCCESSFULLY!\n",
      "   ğŸ“ Train file: processed_asr_data\\15-train.csv\n",
      "   ğŸ“ Test file: processed_asr_data\\15-test.csv\n",
      "   ğŸ‹ï¸ Training samples: 12,000 (80%)\n",
      "   ğŸ§ª Testing samples: 3,000 (20%)\n",
      "   ğŸš€ Ready for ASR model training and evaluation!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# SPLIT 15,000 SAMPLES INTO TRAIN/TEST (80/20)\n",
    "# ================================\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"ğŸ“Š SPLITTING 15,000 SAMPLES INTO TRAIN/TEST\")\n",
    "print(f\"=\" * 60)\n",
    "\n",
    "# Check if sampled_df exists\n",
    "if 'sampled_df' not in locals():\n",
    "    print(f\"âŒ Error: sampled_df not found. Please run the sampling cell first.\")\n",
    "else:\n",
    "    total_samples = len(sampled_df)\n",
    "    \n",
    "    print(f\"ğŸ“ˆ Split Configuration:\")\n",
    "    print(f\"   ğŸ“Š Total samples: {total_samples:,}\")\n",
    "    print(f\"   ğŸ‹ï¸ Train split: 80%\")\n",
    "    print(f\"   ğŸ§ª Test split: 20%\")\n",
    "    \n",
    "    # Calculate split sizes\n",
    "    train_size = int(0.8 * total_samples)\n",
    "    test_size = total_samples - train_size\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Split Sizes:\")\n",
    "    print(f\"   ğŸ‹ï¸ Train samples: {train_size:,}\")\n",
    "    print(f\"   ğŸ§ª Test samples: {test_size:,}\")\n",
    "    \n",
    "    # Shuffle the data before splitting for better randomization\n",
    "    shuffled_df = sampled_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Split the data\n",
    "    train_10k_df = shuffled_df[:train_size].reset_index(drop=True)\n",
    "    test_10k_df = shuffled_df[train_size:].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nğŸ”€ Data Split:\")\n",
    "    print(f\"   âœ… Train data: {len(train_10k_df):,} samples\")\n",
    "    print(f\"   âœ… Test data: {len(test_10k_df):,} samples\")\n",
    "    \n",
    "    # Define output file paths\n",
    "    train_10k_file = os.path.join(output_dir, \"15-train.csv\")\n",
    "    test_10k_file = os.path.join(output_dir, \"15-test.csv\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ Saving split files...\")\n",
    "    print(f\"   ğŸ“ Train file: {train_10k_file}\")\n",
    "    print(f\"   ğŸ“ Test file: {test_10k_file}\")\n",
    "    \n",
    "    # Save train file\n",
    "    try:\n",
    "        train_10k_df.to_csv(train_10k_file, index=False)\n",
    "        train_file_size = os.path.getsize(train_10k_file) / (1024**2)  # MB\n",
    "        print(f\"   âœ… Train file saved: {train_file_size:.2f} MB\")\n",
    "        train_success = True\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error saving train file: {e}\")\n",
    "        train_success = False\n",
    "    \n",
    "    # Save test file\n",
    "    try:\n",
    "        test_10k_df.to_csv(test_10k_file, index=False)\n",
    "        test_file_size = os.path.getsize(test_10k_file) / (1024**2)  # MB\n",
    "        print(f\"   âœ… Test file saved: {test_file_size:.2f} MB\")\n",
    "        test_success = True\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error saving test file: {e}\")\n",
    "        test_success = False\n",
    "    \n",
    "    if train_success and test_success:\n",
    "        print(f\"\\nğŸ“ Sample Previews:\")\n",
    "        print(f\"\\nğŸ‹ï¸ Train Data Sample:\")\n",
    "        print(train_10k_df.head(3))\n",
    "        \n",
    "        print(f\"\\nğŸ§ª Test Data Sample:\")\n",
    "        print(test_10k_df.head(3))\n",
    "        \n",
    "        print(f\"\\nğŸ‰ SPLIT COMPLETED SUCCESSFULLY!\")\n",
    "        print(f\"   ğŸ“ Train file: {train_10k_file}\")\n",
    "        print(f\"   ğŸ“ Test file: {test_10k_file}\")\n",
    "        print(f\"   ğŸ‹ï¸ Training samples: {len(train_10k_df):,} (80%)\")\n",
    "        print(f\"   ğŸ§ª Testing samples: {len(test_10k_df):,} (20%)\")\n",
    "        print(f\"   ğŸš€ Ready for ASR model training and evaluation!\")\n",
    "    else:\n",
    "        print(f\"\\nâŒ Some files failed to save. Please check the errors above.\")\n",
    "\n",
    "print(f\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dcb941",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
